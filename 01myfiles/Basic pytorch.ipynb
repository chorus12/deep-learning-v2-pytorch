{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic pytorch stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jupytext] Reading Basic pytorch.ipynb\n",
      "[jupytext] Writing Basic pytorch.md (destination file replaced)\n"
     ]
    }
   ],
   "source": [
    "!jupytext --to markdown \"Basic pytorch.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a 3d tensor \n",
    "pay attention that torch.int = int32 and simple int = int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6]],\n",
      "\n",
      "        [[1, 2, 3],\n",
      "         [4, 5, 6]],\n",
      "\n",
      "        [[1, 2, 3],\n",
      "         [4, 5, 6]]], dtype=torch.int32) \n",
      "\n",
      " torch.Size([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([\n",
    "     [[1, 2, 3],\n",
    "      [4, 5, 6]],\n",
    "     [[1, 2, 3],\n",
    "      [4, 5, 6]],\n",
    "     [[1, 2, 3],\n",
    "      [4, 5, 6]]], \n",
    "    dtype = torch.int)\n",
    "print(y, \"\\n\\n\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making summation over different dimentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  6,  9],\n",
       "        [12, 15, 18]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 7, 9],\n",
       "        [5, 7, 9],\n",
       "        [5, 7, 9]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6, 15],\n",
       "        [ 6, 15],\n",
       "        [ 6, 15]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Float tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.6011e-38, 4.5866e-41, 1.3215e-12],\n",
      "        [3.0663e-41, 1.4013e-45, 4.5866e-41]]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor(2,3)\n",
    "print(x, x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68070255, 0.61317935, 0.75916742],\n",
       "       [0.14941783, 0.34229245, 0.91174874]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.random.random((2,3)).astype(float)\n",
    "np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6807, 0.6132, 0.7592],\n",
      "        [0.1494, 0.3423, 0.9117]]) torch.float32 \n",
      "\n",
      " tensor([[-0.9780,  0.0616, -0.0514],\n",
      "        [ 1.3100,  0.6309,  0.2642]]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.FloatTensor(np_array)\n",
    "x2 = torch.randn(2,3)\n",
    "print(x1, x1.dtype, \"\\n\\n\",x2, x2.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integer tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3], dtype=torch.int32), torch.int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_tensor = torch.arange(4, dtype=torch.int)\n",
    "int_tensor, int_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3]], dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_tensor.view(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(63, dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(y, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  2.7183,  7.3891, 20.0855])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = torch.exp(int_tensor.float())\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  2.7183, 14.7781, 60.2566])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_tensor*e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matmul and MM plus devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6670,  1.4792],\n",
       "        [-0.1720,  0.6526]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x1, x2.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6670,  1.4792],\n",
       "        [-0.1720,  0.6526]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x1, x2.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.66702986,  1.4791664 ],\n",
       "       [-0.1719538 ,  0.6526077 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(x1.numpy(), x2.t().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6670,  1.4792],\n",
       "        [-0.1720,  0.6526]], dtype=torch.float64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x1.to(torch.float64), x2.to(torch.float64).t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device('cuda')\n",
    "x1 = x1.to(device)\n",
    "x1.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3395,  0.7248],\n",
       "        [-1.6600, -0.0285],\n",
       "        [-0.3693, -1.8319]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    x.to(device).numpy()\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-9868ff2c1551>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "torch.mm(torch.ones(2,3), torch.ones(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.ones(2,3), torch.ones(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit of autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(0,4).float().requires_grad_(True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Linear(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4, out_features=2, bias=True)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = torch.arange(0,4).float()\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = net(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6486, -0.9467], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0062, -0.0700,  0.2314, -0.3623],\n",
      "        [ 0.2912, -0.1247, -0.1449, -0.1122]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0457, -0.1956], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Моя первая нейронная сетка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = torch.nn.Linear(hidden_size,2)\n",
    "        self.layer3 = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_val):\n",
    "        h = input_val\n",
    "        h = self.layer1(h)\n",
    "        h = self.layer2(h)\n",
    "        h = self.layer3(h)\n",
    "        return h\n",
    "    \n",
    "    def print_params(self):\n",
    "        for item in self.parameters():\n",
    "            print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyNet(4,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1714,  0.0054,  0.0135,  0.2375],\n",
      "        [-0.2734, -0.2523,  0.1203, -0.0256],\n",
      "        [-0.0172, -0.2048, -0.4937, -0.2385],\n",
      "        [ 0.2607,  0.0676, -0.1736, -0.2190],\n",
      "        [ 0.2596,  0.0793,  0.3028, -0.4382],\n",
      "        [-0.0682, -0.4884, -0.2896,  0.1911],\n",
      "        [-0.2250, -0.1496,  0.0766,  0.4946],\n",
      "        [-0.2225, -0.4926, -0.1673, -0.3859],\n",
      "        [ 0.4157,  0.4976, -0.1205,  0.3753],\n",
      "        [-0.4672, -0.2079, -0.4549,  0.0241],\n",
      "        [ 0.4097,  0.2128, -0.2531,  0.2228],\n",
      "        [-0.3411, -0.2235,  0.2279, -0.2119],\n",
      "        [-0.2136, -0.2691, -0.2938,  0.4035],\n",
      "        [-0.1405, -0.0119,  0.4893, -0.2947],\n",
      "        [-0.3604,  0.1100,  0.1278, -0.1234],\n",
      "        [-0.3834, -0.2455,  0.1176, -0.1726]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 2.6076e-01,  1.3226e-01,  1.7381e-04,  4.2966e-01, -1.0452e-01,\n",
      "        -5.2752e-02, -2.0049e-01, -9.8614e-02, -2.1173e-01,  4.5403e-01,\n",
      "         8.6372e-02, -3.6805e-01,  6.7845e-02,  1.4513e-01, -2.1347e-01,\n",
      "         1.2120e-01], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0698,  0.1378, -0.1112,  0.0681,  0.2014,  0.0924,  0.1708,  0.0132,\n",
      "         -0.0053, -0.0263, -0.2182, -0.0755, -0.1306, -0.0442, -0.0158, -0.0665],\n",
      "        [ 0.0692, -0.2108, -0.2092, -0.1434,  0.2437,  0.0794,  0.1311, -0.0286,\n",
      "         -0.1457, -0.2063, -0.0570,  0.1250, -0.0279,  0.0685, -0.1966, -0.1228]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0400, 0.0397], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "net.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5246, 0.5869], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward(torch.rand(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet2(torch.nn.Sequential):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__(nn.Linear(input_size, hidden_size), \n",
    "                         nn.Linear(hidden_size, output_size), \n",
    "                         nn.Softmax())\n",
    "    def print_params(self):\n",
    "        for item in self.parameters():\n",
    "            print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_net2(input_size, hidden_size, output_size):\n",
    "    return nn.Sequential(nn.Linear(input_size, hidden_size), \n",
    "                             nn.Linear(hidden_size, output_size), \n",
    "                             nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = MyNet2(4,16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = make_net2(4,16,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6007, 0.7096, 0.7543, 0.5361, 0.4564, 0.5808, 0.4171, 0.4801, 0.3513,\n",
       "        0.5427], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2.forward(torch.ones(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.1123, 0.2141, 0.0880, 0.0594, 0.0878, 0.0930, 0.0576, 0.0825, 0.1199,\n",
       "        0.0856], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt.forward(torch.ones(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1373,  0.2929,  0.2541, -0.0555],\n",
      "        [-0.1513,  0.3345, -0.1255, -0.2897],\n",
      "        [-0.1781, -0.4404, -0.4254, -0.0877],\n",
      "        [-0.4260,  0.4384,  0.1961, -0.4451],\n",
      "        [ 0.4116,  0.2819, -0.1327, -0.0184],\n",
      "        [-0.0227, -0.1085, -0.1410,  0.0528],\n",
      "        [ 0.1697,  0.2691, -0.0950, -0.4855],\n",
      "        [-0.4631, -0.1972, -0.0513,  0.3360],\n",
      "        [ 0.1719, -0.4612, -0.1836, -0.4056],\n",
      "        [ 0.0106,  0.0285,  0.1324, -0.2354],\n",
      "        [-0.0161,  0.3861, -0.1371,  0.0623],\n",
      "        [ 0.1441,  0.3036, -0.0753,  0.0216],\n",
      "        [ 0.4255, -0.1737,  0.2636,  0.4485],\n",
      "        [-0.0294, -0.4045,  0.3832,  0.0756],\n",
      "        [ 0.1896,  0.4565,  0.0278, -0.4594],\n",
      "        [-0.1792,  0.3235, -0.1605, -0.1490]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0562,  0.2413,  0.3477,  0.1714,  0.3858, -0.0527,  0.3031, -0.3139,\n",
      "        -0.0870, -0.0347,  0.3929, -0.3392, -0.4987, -0.0069,  0.2188, -0.2879],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0272, -0.0542, -0.1275,  0.0889, -0.1771,  0.1152,  0.1019, -0.1685,\n",
      "         -0.1410,  0.0065,  0.1737,  0.0778, -0.2350, -0.2006, -0.0544,  0.1500],\n",
      "        [ 0.1078, -0.2162, -0.0996,  0.0863,  0.1278, -0.1641, -0.1638, -0.1063,\n",
      "         -0.2154,  0.1021,  0.1727, -0.0736,  0.2288,  0.1930,  0.2205,  0.2470],\n",
      "        [-0.1365, -0.0196, -0.0081,  0.1301, -0.1026,  0.0830, -0.1279, -0.2222,\n",
      "         -0.0686, -0.0784,  0.2465, -0.1896,  0.0577, -0.1717, -0.0562,  0.0645],\n",
      "        [-0.1139,  0.1641, -0.1150, -0.0627,  0.0859,  0.1929, -0.1994,  0.0526,\n",
      "          0.2218,  0.1745, -0.2288,  0.0967,  0.0821,  0.2371, -0.0385,  0.0978],\n",
      "        [-0.1850,  0.2255, -0.1326,  0.2450, -0.2330, -0.0861,  0.1912,  0.2321,\n",
      "         -0.2495,  0.0832, -0.0253, -0.1806,  0.1318,  0.2147,  0.1903, -0.1170],\n",
      "        [ 0.1953,  0.1164,  0.1417,  0.2284, -0.0496, -0.0645, -0.0224,  0.2141,\n",
      "         -0.1005,  0.1376, -0.0345, -0.1693,  0.1580, -0.1072, -0.0893, -0.0504],\n",
      "        [-0.2013, -0.1026,  0.1983, -0.0443, -0.0919,  0.2095, -0.1068,  0.1696,\n",
      "         -0.0934, -0.0458, -0.1320, -0.2314, -0.2325,  0.1356,  0.1486,  0.0065],\n",
      "        [-0.1578, -0.1901, -0.1793,  0.0781, -0.0480,  0.0428,  0.0704, -0.0360,\n",
      "          0.2498,  0.0852, -0.1074,  0.1306, -0.2350,  0.1536,  0.1978, -0.2424],\n",
      "        [-0.0745, -0.0746,  0.1652, -0.1427, -0.1216,  0.0557, -0.1440, -0.0435,\n",
      "         -0.1871,  0.1380,  0.1734, -0.2239,  0.0229, -0.2391,  0.1509,  0.2067],\n",
      "        [ 0.0046,  0.2031, -0.2394, -0.0066, -0.2042, -0.2079,  0.0156,  0.1889,\n",
      "          0.0344, -0.0137, -0.1741,  0.1765, -0.0913, -0.0346, -0.1205, -0.1922]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0690,  0.0206, -0.2303, -0.0754, -0.1580,  0.0122,  0.0892,  0.0488,\n",
      "         0.2422,  0.0820], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "ttt.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-100,100,0.5)\n",
    "above_zero = (x >=0).astype(int)\n",
    "below_zero = (x<0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =  x*above_zero + 0.5*below_zero*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.axis('equal')\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = nn.Linear(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3135)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.full([10, 64], 0.999)  # A prediction (logit)\n",
    "pos_weight = torch.ones([64])  # All weights are equal to 1\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "criterion(output, target)  # -log(sigmoid(0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990],\n",
       "        [0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990],\n",
       "        [0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990],\n",
       "        [0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990],\n",
       "        [0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990],\n",
       "        [0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990],\n",
       "        [0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990],\n",
       "        [0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990],\n",
       "        [0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990],\n",
       "        [0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
