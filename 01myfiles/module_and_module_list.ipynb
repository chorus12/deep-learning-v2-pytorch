{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch: how and when to use Module, Sequential, ModuleList and ModuleDict\n",
    "### Effective way to share, reuse and break down the complexity of your models\n",
    "\n",
    "Updated at Pytorch 4.1\n",
    "\n",
    "You can find the code [here](https://github.com/FrancescoSaverioZuppichini/Pytorch-how-and-when-to-use-Module-Sequential-ModuleList-and-ModuleDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pytorch](https://pytorch.org/) is an open source deep learning frameworks that provide a smart way to create ML models. Even if the documentation is well made, I still find that most people still are able to write bad and not organized PyTorch code.\n",
    "\n",
    "Today, we are going to see how to use the three main building blocks of PyTorch: `Module, Sequential and ModuleList`. We are going to start with an example and iteratively we will make it better.\n",
    "\n",
    "All these four classes are contained into `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "\n",
    "# nn.Module\n",
    "# nn.Sequential\n",
    "# nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module: the main building block\n",
    "\n",
    "The [Module](https://pytorch.org/docs/stable/nn.html?highlight=module) is the main building block, it defines the base class for all neural network and you MUST subclass it. \n",
    "\n",
    "Let's create a classic CNN classifier as example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 1024)\n",
    "        self.fc2 = nn.Linear(1024, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=50176, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyCNNClassifier(1, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn([4,1,28,28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5027,  0.2934,  0.0662, -0.1046, -0.6346,  0.0197, -0.4155,  0.3261,\n",
       "         -0.2198,  0.2511],\n",
       "        [-0.4341,  0.3062,  0.2153, -0.1583, -0.6856, -0.0468, -0.5879,  0.3907,\n",
       "         -0.1177,  0.2106],\n",
       "        [-0.4082,  0.3024,  0.1708, -0.0927, -0.5339,  0.0038, -0.4559,  0.3311,\n",
       "         -0.2118,  0.2325],\n",
       "        [-0.4989,  0.3627,  0.2405, -0.1431, -0.5814,  0.0121, -0.4598,  0.3193,\n",
       "         -0.1608,  0.2813]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple classifier with an encoding part that uses two layers with 3x3 convs + batchnorm + relu and a decoding part with two linear layers. If you are not new to PyTorch you may have seen this type of coding before, but there are two problems.\n",
    "\n",
    "If we want to add a layer we have to again write lots of code in the `__init__` and in the `forward` function. Also, if we have some common block that we want to use in another model, e.g. the 3x3 conv + batchnorm + relu, we have to write it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential: stack and merge layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sequential](https://pytorch.org/docs/stable/nn.html?highlight=sequential#torch.nn.Sequential) is a container of Modules that can be stacked together and run at the same time.\n",
    "\n",
    "You can notice that we have to store into `self` everything. We can use `Sequential` to improve our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_c, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64 * 28 * 28, 1024),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(1024, n_classes), \n",
    "            nn.Softmax(dim=1)\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        print(x.shape)\n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (conv_block1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=50176, out_features=1024, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "    (3): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyCNNClassifier(1, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyCNNClassifier(\n",
       "  (conv_block1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv_block2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=50176, out_features=1024, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
       "    (3): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "model.apply(weight_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "<class 'torch.nn.modules.activation.Sigmoid'>\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "<class 'torch.nn.modules.activation.Softmax'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class '__main__.MyCNNClassifier'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyCNNClassifier(\n",
       "  (conv_block1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv_block2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=50176, out_features=1024, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
       "    (3): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scan_my_net(m):\n",
    "    print(type(m))\n",
    "\n",
    "model.apply(scan_my_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([1024, 50176])\n",
      "torch.Size([1024])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for item in model.parameters():\n",
    "    print(item.data.shape)\n",
    "    item.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_block1.1.running_mean tensor([ 1.5740e-02, -2.0754e-02,  2.3193e-02, -1.3082e-02,  8.6501e-03,\n",
      "         2.6578e-02,  1.5690e-03, -2.0276e-02,  2.7573e-02, -1.5555e-02,\n",
      "         2.8842e-02, -2.4972e-02,  1.7962e-02,  1.1767e-02,  3.0511e-03,\n",
      "         4.3805e-03, -2.6113e-02,  2.8526e-05,  2.3143e-02,  1.8352e-02,\n",
      "        -2.3752e-02, -1.4090e-02,  2.7229e-02,  1.8934e-02, -2.5730e-02,\n",
      "         1.7694e-03,  1.4734e-02,  2.9743e-02, -2.2625e-02,  9.7365e-03,\n",
      "        -2.6535e-02, -2.2983e-02])\n",
      "conv_block1.1.running_var tensor([0.8460, 0.8410, 0.8414, 0.8557, 0.8294, 0.8350, 0.8300, 0.8246, 0.8244,\n",
      "        0.8417, 0.8455, 0.8445, 0.8285, 0.8342, 0.8263, 0.8383, 0.8377, 0.8286,\n",
      "        0.8363, 0.8392, 0.8259, 0.8345, 0.8363, 0.8311, 0.8413, 0.8394, 0.8304,\n",
      "        0.8294, 0.8440, 0.8252, 0.8419, 0.8405])\n",
      "conv_block1.1.num_batches_tracked tensor(2)\n",
      "conv_block2.1.running_mean tensor([ 0.0116,  0.0092,  0.0069,  0.0083,  0.0173, -0.0268,  0.0089,  0.0092,\n",
      "        -0.0134, -0.0084,  0.0157, -0.0232,  0.0034,  0.0190,  0.0083, -0.0203,\n",
      "         0.0047, -0.0082, -0.0126, -0.0097,  0.0313, -0.0015,  0.0036,  0.0004,\n",
      "        -0.0789, -0.0030,  0.0283,  0.0025, -0.0221, -0.0387, -0.0359, -0.0105,\n",
      "         0.0052,  0.0018, -0.0199, -0.0209,  0.0259, -0.0029, -0.0092, -0.0188,\n",
      "        -0.0299,  0.0087,  0.0172,  0.0070, -0.0103,  0.0067,  0.0325, -0.0091,\n",
      "        -0.0263, -0.0213,  0.0298, -0.0041, -0.0041, -0.0082,  0.0004, -0.0226,\n",
      "         0.0018,  0.0194,  0.0091, -0.0147,  0.0303, -0.0159,  0.0318, -0.0199])\n",
      "conv_block2.1.running_var tensor([0.8264, 0.8188, 0.8209, 0.8172, 0.8196, 0.8217, 0.8207, 0.8210, 0.8189,\n",
      "        0.8200, 0.8193, 0.8209, 0.8167, 0.8220, 0.8221, 0.8199, 0.8220, 0.8209,\n",
      "        0.8205, 0.8159, 0.8237, 0.8199, 0.8245, 0.8187, 0.8254, 0.8205, 0.8206,\n",
      "        0.8214, 0.8188, 0.8236, 0.8218, 0.8216, 0.8310, 0.8176, 0.8202, 0.8258,\n",
      "        0.8217, 0.8188, 0.8189, 0.8194, 0.8234, 0.8232, 0.8172, 0.8230, 0.8183,\n",
      "        0.8182, 0.8163, 0.8216, 0.8215, 0.8201, 0.8188, 0.8178, 0.8183, 0.8167,\n",
      "        0.8221, 0.8178, 0.8196, 0.8186, 0.8178, 0.8194, 0.8194, 0.8201, 0.8198,\n",
      "        0.8200])\n",
      "conv_block2.1.num_batches_tracked tensor(2)\n"
     ]
    }
   ],
   "source": [
    "for name, buf in model.named_buffers():\n",
    "    print(name, buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([ 1.5740e-02, -2.0754e-02,  2.3193e-02, -1.3082e-02,  8.6501e-03,\n",
      "         2.6578e-02,  1.5690e-03, -2.0276e-02,  2.7573e-02, -1.5555e-02,\n",
      "         2.8842e-02, -2.4972e-02,  1.7962e-02,  1.1767e-02,  3.0511e-03,\n",
      "         4.3805e-03, -2.6113e-02,  2.8526e-05,  2.3143e-02,  1.8352e-02,\n",
      "        -2.3752e-02, -1.4090e-02,  2.7229e-02,  1.8934e-02, -2.5730e-02,\n",
      "         1.7694e-03,  1.4734e-02,  2.9743e-02, -2.2625e-02,  9.7365e-03,\n",
      "        -2.6535e-02, -2.2983e-02])\n",
      "1 tensor([0.8460, 0.8410, 0.8414, 0.8557, 0.8294, 0.8350, 0.8300, 0.8246, 0.8244,\n",
      "        0.8417, 0.8455, 0.8445, 0.8285, 0.8342, 0.8263, 0.8383, 0.8377, 0.8286,\n",
      "        0.8363, 0.8392, 0.8259, 0.8345, 0.8363, 0.8311, 0.8413, 0.8394, 0.8304,\n",
      "        0.8294, 0.8440, 0.8252, 0.8419, 0.8405])\n",
      "2 tensor(2)\n",
      "3 tensor([ 0.0116,  0.0092,  0.0069,  0.0083,  0.0173, -0.0268,  0.0089,  0.0092,\n",
      "        -0.0134, -0.0084,  0.0157, -0.0232,  0.0034,  0.0190,  0.0083, -0.0203,\n",
      "         0.0047, -0.0082, -0.0126, -0.0097,  0.0313, -0.0015,  0.0036,  0.0004,\n",
      "        -0.0789, -0.0030,  0.0283,  0.0025, -0.0221, -0.0387, -0.0359, -0.0105,\n",
      "         0.0052,  0.0018, -0.0199, -0.0209,  0.0259, -0.0029, -0.0092, -0.0188,\n",
      "        -0.0299,  0.0087,  0.0172,  0.0070, -0.0103,  0.0067,  0.0325, -0.0091,\n",
      "        -0.0263, -0.0213,  0.0298, -0.0041, -0.0041, -0.0082,  0.0004, -0.0226,\n",
      "         0.0018,  0.0194,  0.0091, -0.0147,  0.0303, -0.0159,  0.0318, -0.0199])\n",
      "4 tensor([0.8264, 0.8188, 0.8209, 0.8172, 0.8196, 0.8217, 0.8207, 0.8210, 0.8189,\n",
      "        0.8200, 0.8193, 0.8209, 0.8167, 0.8220, 0.8221, 0.8199, 0.8220, 0.8209,\n",
      "        0.8205, 0.8159, 0.8237, 0.8199, 0.8245, 0.8187, 0.8254, 0.8205, 0.8206,\n",
      "        0.8214, 0.8188, 0.8236, 0.8218, 0.8216, 0.8310, 0.8176, 0.8202, 0.8258,\n",
      "        0.8217, 0.8188, 0.8189, 0.8194, 0.8234, 0.8232, 0.8172, 0.8230, 0.8183,\n",
      "        0.8182, 0.8163, 0.8216, 0.8215, 0.8201, 0.8188, 0.8178, 0.8183, 0.8167,\n",
      "        0.8221, 0.8178, 0.8196, 0.8186, 0.8178, 0.8194, 0.8194, 0.8201, 0.8198,\n",
      "        0.8200])\n",
      "5 tensor(2)\n"
     ]
    }
   ],
   "source": [
    "for i, buf in enumerate(model.buffers()):\n",
    "    print(i, buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MyCNNClassifier(\n",
      "  (conv_block1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=50176, out_features=1024, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "    (3): Softmax(dim=None)\n",
      "  )\n",
      ")\n",
      "1 Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "2 Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "3 BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "4 ReLU()\n",
      "5 Sequential(\n",
      "  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "6 Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "7 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "8 ReLU()\n",
      "9 Sequential(\n",
      "  (0): Linear(in_features=50176, out_features=1024, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  (3): Softmax(dim=None)\n",
      ")\n",
      "10 Linear(in_features=50176, out_features=1024, bias=True)\n",
      "11 Sigmoid()\n",
      "12 Linear(in_features=1024, out_features=10, bias=True)\n",
      "13 Softmax(dim=None)\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(model.modules()):\n",
    "    print(i, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Softmax(dim=None)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 28, 28])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 28, 28])\n",
      "torch.Size([4, 50176])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(inp)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 28, 28])\n",
      "torch.Size([4, 50176])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1136, 0.0901, 0.1069, 0.0795, 0.1266, 0.1200, 0.0647, 0.0674, 0.0797,\n",
       "         0.1515],\n",
       "        [0.1210, 0.0832, 0.1121, 0.0882, 0.1193, 0.1120, 0.0669, 0.0732, 0.0795,\n",
       "         0.1446],\n",
       "        [0.1286, 0.0911, 0.1008, 0.0853, 0.1124, 0.1069, 0.0695, 0.0701, 0.0821,\n",
       "         0.1532],\n",
       "        [0.1121, 0.0821, 0.1133, 0.0803, 0.1131, 0.1189, 0.0678, 0.0741, 0.0859,\n",
       "         0.1524]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(inp)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4433e-43, 1.5975e-43, 1.3593e-43, 1.4013e-43, 1.3312e-43, 1.6395e-43,\n",
       "        1.5414e-43, 1.4714e-43, 1.4293e-43, 1.5554e-43])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp1 = torch.FloatTensor(10)\n",
    "inp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2048, 1.4941, 1.5475, 1.3633, 1.5686, 1.6029, 1.4818, 1.8714, 1.6811,\n",
       "        1.9181])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.uniform_(inp1,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp2 = torch.empty(3,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.]]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.dirac_(inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much Better uhu?\n",
    "\n",
    "Did you notice that `conv_block1` and `conv_block2` looks almost the same? We could create a function that reteurns a `nn.Sequential` to even simplify the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_f, out_f, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_f),\n",
    "        nn.ReLU()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can just call this function in our Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = conv_block(in_c, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv_block2 = conv_block(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32 * 28 * 28, 1024),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(1024, n_classes)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (conv_block1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=1024, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyCNNClassifier(1, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even cleaner! Still `conv_block1` and `conv_block2` are almost the same! We can merge them using `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, n_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv_block(in_c, 32, kernel_size=3, padding=1),\n",
    "            conv_block(32, 64, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32 * 28 * 28, 1024),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(1024, n_classes)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=1024, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyCNNClassifier(1, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`self.encoder` now holds booth `conv_block`. We have decoupled logic for our model and make it easier to read and reuse. Our `conv_block` function can be imported and used in another model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Sequential: create multiple layers at once\n",
    "\n",
    "What if we can to add a new layers in `self.encoder`, hardcoded them is not convinient:\n",
    "\n",
    "```python\n",
    "self.encoder = nn.Sequential(\n",
    "            conv_block(in_c, 32, kernel_size=3, padding=1),\n",
    "            conv_block(32, 64, kernel_size=3, padding=1),\n",
    "            conv_block(64, 128, kernel_size=3, padding=1),\n",
    "            conv_block(128, 256, kernel_size=3, padding=1),\n",
    "\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would it be nice if we can define the sizes as an array and automatically create all the layers without writing each one of them? Fortunately we can create an array and pass it to `Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, n_classes):\n",
    "        super().__init__()\n",
    "        self.enc_sizes = [in_c, 32, 64]\n",
    "        \n",
    "        conv_blocks = [conv_block(in_f, out_f, kernel_size=3, padding=1) \n",
    "                       for in_f, out_f in zip(self.enc_sizes, self.enc_sizes[1:])]\n",
    "        \n",
    "        self.encoder = nn.Sequential(*conv_blocks)\n",
    "\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32 * 28 * 28, 1024),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(1024, n_classes)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=1024, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyCNNClassifier(1, 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break it down. We created an array `self.enc_sizes` that holds the sizes of our encoder. Then we create an array `conv_blocks` by iterating the sizes. Since we have to give booth a in size and an outsize for each layer we `zip`ed the size'array with itself by shifting it by one. \n",
    "\n",
    "Just to be clear, take a look at the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 32\n",
      "32 64\n"
     ]
    }
   ],
   "source": [
    "sizes = [1, 32, 64]\n",
    "\n",
    "for in_f,out_f in zip(sizes, sizes[1:]):\n",
    "    print(in_f,out_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, since `Sequential` does not accept a list, we decompose it by using the `*` operator.\n",
    "\n",
    "Tada! Now if we just want to add a size, we can easily add a new number to the list. It is a common practice to make the size a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, enc_sizes, n_classes):\n",
    "        super().__init__()\n",
    "        self.enc_sizes = [in_c, *enc_sizes]\n",
    "        \n",
    "        conv_blokcs = [conv_block(in_f, out_f, kernel_size=3, padding=1) \n",
    "                       for in_f, out_f in zip(self.enc_sizes, self.enc_sizes[1:])]\n",
    "        \n",
    "        self.encoder = nn.Sequential(*conv_blokcs)\n",
    "\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32 * 28 * 28, 1024),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(1024, n_classes)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=1024, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyCNNClassifier(1, [32,64, 128], 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the decoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_block(in_f, out_f):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_f, out_f),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, enc_sizes, dec_sizes,  n_classes):\n",
    "        super().__init__()\n",
    "        self.enc_sizes = [in_c, *enc_sizes]\n",
    "        self.dec_sizes = [32 * 28 * 28, *dec_sizes]\n",
    "\n",
    "        conv_blokcs = [conv_block(in_f, out_f, kernel_size=3, padding=1) \n",
    "                       for in_f, out_f in zip(self.enc_sizes, self.enc_sizes[1:])]\n",
    "        \n",
    "        self.encoder = nn.Sequential(*conv_blokcs)\n",
    "\n",
    "        \n",
    "        dec_blocks = [dec_block(in_f, out_f) \n",
    "                       for in_f, out_f in zip(self.dec_sizes, self.dec_sizes[1:])]\n",
    "        \n",
    "        self.decoder = nn.Sequential(*dec_blocks)\n",
    "        \n",
    "        self.last = nn.Linear(self.dec_sizes[-1], n_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=1024, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (last): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyCNNClassifier(1, [32,64], [1024, 512], 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We followed the same pattern, we create a new block for the decoding part, linear + sigmoid, and we pass an array with the sizes. We had to add a `self.last` since we do not want to activate the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can even break down our model in two! Encoder + Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEncoder(nn.Module):\n",
    "    def __init__(self, enc_sizes):\n",
    "        super().__init__()\n",
    "        self.conv_blokcs = nn.Sequential(*[conv_block(in_f, out_f, kernel_size=3, padding=1) \n",
    "                       for in_f, out_f in zip(enc_sizes, enc_sizes[1:])])\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.conv_blokcs(x)\n",
    "        \n",
    "class MyDecoder(nn.Module):\n",
    "    def __init__(self, dec_sizes, n_classes):\n",
    "        super().__init__()\n",
    "        self.dec_blocks = nn.Sequential(*[dec_block(in_f, out_f) \n",
    "                       for in_f, out_f in zip(dec_sizes, dec_sizes[1:])])\n",
    "        self.last = nn.Linear(dec_sizes[-1], n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dec_blocks()\n",
    "    \n",
    "    \n",
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, enc_sizes, dec_sizes,  n_classes):\n",
    "        super().__init__()\n",
    "        self.enc_sizes = [in_c, *enc_sizes]\n",
    "        self.dec_sizes = [32 * 28 * 28, *dec_sizes]\n",
    "\n",
    "        self.encoder = MyEncoder(self.enc_sizes)\n",
    "        \n",
    "        self.decoder = MyDecoder(dec_sizes, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = x.flatten(1) # flat\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (encoder): MyEncoder(\n",
      "    (conv_blokcs): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): MyDecoder(\n",
      "    (dec_blocks): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (last): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyCNNClassifier(1, [32,64], [1024, 512], 10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that `MyEncoder` and `MyDecoder` could also be functions that returns a `nn.Sequential`. I prefer to use the first pattern for models and the second for building blocks.\n",
    "\n",
    "By diving our module into submodules it is easier to **share** the code, **debug** it and **test** it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModuleList : when we need to iterate\n",
    "\n",
    "`ModuleList` allows you to store `Module` as a list. It can be useful when you need to iterate through layer and store/use some information, like in U-net.\n",
    "\n",
    "The main difference between `Sequential` is that `ModuleList` have not a `forward` method so the inner layers are not connected. Assuming we need each output of each layer in the decoder, we can store it by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(in_f, out_f) for in_f, out_f in zip(sizes, sizes[1:])])\n",
    "        self.trace = []\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            self.trace.append(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16])\n",
      "torch.Size([4, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModule([1, 16, 32])\n",
    "import torch\n",
    "\n",
    "model(torch.rand((4,1)))\n",
    "\n",
    "[print(trace.shape) for trace in model.trace]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModuleDict: when we need to choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to switch to `LearkyRelu` in our `conv_block`? We can use `ModuleDict` to create a dictionary of `Module` and dynamically switch `Module` when we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_f, out_f, activation='relu', *args, **kwargs):\n",
    "    \n",
    "    activations = nn.ModuleDict([\n",
    "                ['lrelu', nn.LeakyReLU()],\n",
    "                ['relu', nn.ReLU()]\n",
    "    ])\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_f),\n",
    "        activations[activation]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): LeakyReLU(negative_slope=0.01)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(conv_block(1, 32,'lrelu', kernel_size=3, padding=1))\n",
    "print(conv_block(1, 32,'relu', kernel_size=3, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap it up everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_f, out_f, activation='relu', *args, **kwargs):\n",
    "    activations = nn.ModuleDict([\n",
    "                ['lrelu', nn.LeakyReLU()],\n",
    "                ['relu', nn.ReLU()]\n",
    "    ])\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_f),\n",
    "        activations[activation]\n",
    "    )\n",
    "\n",
    "def dec_block(in_f, out_f):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_f, out_f),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "class MyEncoder(nn.Module):\n",
    "    def __init__(self, enc_sizes, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv_blokcs = nn.Sequential(*[conv_block(in_f, out_f, kernel_size=3, padding=1, *args, **kwargs) \n",
    "                       for in_f, out_f in zip(enc_sizes, enc_sizes[1:])])\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.conv_blokcs(x)\n",
    "        \n",
    "class MyDecoder(nn.Module):\n",
    "    def __init__(self, dec_sizes, n_classes):\n",
    "        super().__init__()\n",
    "        self.dec_blocks = nn.Sequential(*[dec_block(in_f, out_f) \n",
    "                       for in_f, out_f in zip(dec_sizes, dec_sizes[1:])])\n",
    "        self.last = nn.Linear(dec_sizes[-1], n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dec_blocks()\n",
    "    \n",
    "    \n",
    "class MyCNNClassifier(nn.Module):\n",
    "    def __init__(self, in_c, enc_sizes, dec_sizes,  n_classes, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.enc_sizes = [in_c, *enc_sizes]\n",
    "        self.dec_sizes = [32 * 28 * 28, *dec_sizes]\n",
    "\n",
    "        self.encoder = MyEncoder(self.enc_sizes, activation=activation)\n",
    "        \n",
    "        self.decoder = MyDecoder(dec_sizes, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = x.flatten(1) # flat\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNNClassifier(\n",
      "  (encoder): MyEncoder(\n",
      "    (conv_blokcs): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): MyDecoder(\n",
      "    (dec_blocks): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (1): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (last): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyCNNClassifier(1, [32,64], [1024, 512], 10, activation='lrelu')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "So, in summary.\n",
    "\n",
    "- Use `Module` when you have a big block compose of multiple smaller blocks\n",
    "- Use `Sequential` when you want to create a small block from layers\n",
    "- Use `ModuleList` when you need to iterate through some layers or building blocks and do something\n",
    "- Use `ModuleDict` when you need to parametise some blocks of your model, for example an activation function\n",
    "\n",
    "That's all folks!\n",
    "\n",
    "Thank you for reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
