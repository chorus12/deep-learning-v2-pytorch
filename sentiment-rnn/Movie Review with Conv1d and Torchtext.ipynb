{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dirty-deadline",
   "metadata": {},
   "source": [
    "## Let's make a **convolution** work on MOVIE REVIEW CLASSIFICATION and utilize TORCHTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "public-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext as TT\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "featured-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unavailable-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/reviews.txt','r') as f:\n",
    "    data = f.readlines()\n",
    "with open('./data/labels.txt', 'r') as f:\n",
    "    labels = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "empty-personal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am    and i still like most of the scooby doo movies and the old episodes . i love the     s movies  and recently we were treated to one of the better direct to dvd scooby doo outings of this decade  scooby doo and the goblin king  which i wasn  t expecting to be as good as it was . anyway  back to get a clue  i watched some episodes  expecting something very good  but from what i saw of it  i wasn  t impressed at all . first of all  i hated the animation . it was flat  deflated and very saturday  morning  cartoon  standard  easily the worst aspect of the series . even some shows i really hate had slightly better animation . even worse  shaggy and scooby looked like aliens  and i really missed fred  velma and daphne  as they added a lot to the old episodes  when scooby doo was positively good . i also hated the character changes  because it seemed like instead of solving mysteries  shaggy and scooby were now playing superhero  something they would  ve never had done in the movies or in the scooby  doo where are you  show . the theme tune wasn  t very good either  i can  t even remember it  and the jokes were lame and contrived . though  i do acknowledge that there is a very talented voice cast  had they had better material  and hadn  t been told to sound as different to the original voices as humanly possible  which they did  might i add . in conclusion  i personally thought it was awful  and i am not trying to discredit it  it  s what i personally feel .     bethany cox  \n",
      " negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(low=1,high=len(data))\n",
    "print(data[i], labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "checked-label",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "therapeutic-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-going",
   "metadata": {},
   "source": [
    "### Install spacy\n",
    "```sh\n",
    "sudo su - \n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "studied-manual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20001, 22500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train, test, validation split\n",
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(data)*split_frac) + 1\n",
    "test_val_idx = split_idx + int((len(data) - split_idx)//2)\n",
    "split_idx, test_val_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-essence",
   "metadata": {},
   "source": [
    "###  Make a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tough-defensive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 s, sys: 259 ms, total: 23 s\n",
      "Wall time: 23.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "tokenizer = TT.data.utils.get_tokenizer('spacy', 'en_core_web_sm')\n",
    "train_iter = zip(labels[:split_idx], data[:split_idx])\n",
    "counter = Counter()\n",
    "\n",
    "for (label, line) in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "    \n",
    "vocab = TT.vocab.Vocab(counter,min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "consecutive-compensation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'really', 'liked', 'tom', 'barman', ' ', 's', 'awtwb', '.', 'you']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(line)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-senator",
   "metadata": {},
   "source": [
    "### Load pre-trained word embeddings from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cosmetic-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.load_vectors('glove.6B.50d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tested-transfer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([67359, 50])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vectors.shape # these are the vectors that we can set as weights for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "duplicate-color",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5204, -0.8314,  0.4996,  1.2893,  0.1151,  0.0575, -1.3753, -0.9731,\n",
       "         0.1835,  0.4767, -0.1511,  0.3553,  0.2591, -0.7786,  0.5218,  0.4769,\n",
       "        -1.4251,  0.8580,  0.5982, -1.0903,  0.3357, -0.6089,  0.4174,  0.2157,\n",
       "        -0.0742, -0.5822, -0.4502,  0.1725,  0.1645, -0.3841,  2.3283, -0.6668,\n",
       "        -0.5818,  0.7439,  0.0950, -0.4787, -0.8459,  0.3870,  0.2369, -1.5523,\n",
       "         0.6480, -0.1652, -1.4719, -0.1622,  0.7986,  0.9739,  0.4003, -0.2191,\n",
       "        -0.3094,  0.2658])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vectors[vocab.stoi['apple']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-mason",
   "metadata": {},
   "source": [
    "#### Check if load_vectors gives the same result as file with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "theoretical-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "    with open('glove.6B.50d.txt','r') as f:\n",
    "        while True:\n",
    "            line = f.readline().split()\n",
    "            if line[0] == word: return(torch.from_numpy(np.array(line[1:], dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "tender-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_word_embedding('apple').equal(vocab.vectors[vocab.stoi['apple']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "perceived-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the UNK vector with the mean of all known words\n",
    "vocab.vectors[0] = vocab.vectors.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "constitutional-formula",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 496874),\n",
       " ('the', 268219),\n",
       " ('.', 261097),\n",
       " ('and', 131323),\n",
       " ('a', 129967),\n",
       " ('of', 116991),\n",
       " ('to', 108242),\n",
       " ('is', 85587),\n",
       " ('br', 81038),\n",
       " ('it', 77070)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show 10 most frequent tokens along with the frequency\n",
    "sorted(vocab.freqs.items(), key=lambda x: x[-1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "variable-processor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', ' ', 'the', '.', 'and', 'a', 'of', 'to', 'is']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary : map integer to a string\n",
    "vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "communist-dream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', '.', 'it', 'ran', 'at']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 words in a zero sentence\n",
    "[s for s in data[0].split()[:10]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "harmful-citation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18777, 318, 9, 6, 1025, 217, 4, 11, 2194, 37]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 fifst codes of words in a zero sentence\n",
    "[vocab[s] for s in data[0].split()[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "single-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that converts a line of text into a line of vocab indexes\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "surprising-maintenance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "digital-invite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18777, 318, 9, 6, 1025, 217, 4, 11, 2194, 37, 3, 177, 63, 20, 56, 86, 5487, 50, 396, 119, 2, 144, 20, 2, 4896, 2, 4, 66, 13, 159, 12, 3, 4956, 5940, 497, 77, 8, 268, 16, 18777, 318, 2, 17, 2108, 9, 79, 2497, 8, 602, 80, 9, 2, 4896, 2, 4, 3, 25309, 8, 2111, 9960, 2, 3, 5679, 1501, 41, 55, 72, 212, 152, 74, 1202, 4896, 2, 18267, 2, 3, 39298, 7, 3, 231, 894, 2, 36, 3177, 77, 7, 3, 6291, 14, 689, 5, 74, 1501, 4, 60, 14, 221, 3, 372, 12, 68, 6, 1420, 4059, 817, 8, 3647, 186, 3, 396, 2, 14, 1198, 15590, 4, 4, 4, 4, 4, 4, 4, 4, 4, 37, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 318, 4, 6, 360, 351, 2742, 14, 2, 148, 134, 8, 7578, 35, 7, 136, 4896, 4, 1420, 2374, 8, 18777, 318, 4, 14, 529, 16, 117, 1496, 7, 66, 558, 108, 16, 18777, 318, 9, 237, 4233, 4, 54, 6, 2216, 16, 11, 227, 2, 28, 214]\n"
     ]
    }
   ],
   "source": [
    "print(text_pipeline(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "monetary-unknown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214, '  \\n')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi['  \\n'], vocab.itos[214]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "decent-malawi",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[613, 16520]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('scary succumb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "artificial-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert text label to a number label 0|1\n",
    "label_pipeline = lambda x: 1 if x == 'positive\\n' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "enclosed-laugh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline(labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "concerned-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    very basic dataset for processing text data\n",
    "    holds text and label\n",
    "    implements len and getitem methods\n",
    "    '''\n",
    "    def __init__(self, labels, text):\n",
    "        super(TextData, self).__init__()\n",
    "        self.labels = labels\n",
    "        self.text = text\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.text[index], self.labels[index] \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "experienced-heater",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.]])\n",
      "tensor([[0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "one, two = torch.ones(3).unsqueeze(dim=0), torch.zeros(3).unsqueeze(dim=0)\n",
    "print(one)\n",
    "print(two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "friendly-batch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([one, two], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fallen-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(batch, max_len=200):\n",
    "    '''\n",
    "    tokenizer to use in DataLoader\n",
    "    takes a text batch of text dataset and produces a tensor batch, converting text and labels though tokenizer, labeler\n",
    "    tokenizer is a global function text_pipeline\n",
    "    labeler is a global function label_pipeline\n",
    "    max_len is a fixed len size, if text is less than max_len it is padded with ones\n",
    "    if text is larger that max_len it is truncated but from the end of the string\n",
    "    '''\n",
    "    labels_list, text_list = [], []\n",
    "    for _text, _label in batch:\n",
    "        labels_list.append(label_pipeline(_label))\n",
    "        text_holder = torch.ones(max_len, dtype=torch.int32) # fixed size tensor of max_len\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int32)\n",
    "        pos = min(200, len(processed_text))\n",
    "        text_holder[-pos:] = processed_text[-pos:]\n",
    "        text_list.append(text_holder.unsqueeze(dim=0))\n",
    "    return torch.cat(text_list, dim=0), torch.FloatTensor(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "naval-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextData(labels[:split_idx], data[:split_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "solid-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=False, collate_fn=tokenize_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "australian-vessel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "suburban-cooperation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18777"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi['bromwell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aggressive-despite",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt, lbl = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "lightweight-policy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 0.]),\n",
       " tensor([[    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1, 18777,   318,\n",
       "              9,     6,  1025,   217,     4,    11,  2194,    37,     3,   177,\n",
       "             63,    20,    56,    86,  5487,    50,   396,   119,     2,   144,\n",
       "             20,     2,  4896,     2,     4,    66,    13,   159,    12,     3,\n",
       "           4956,  5940,   497,    77,     8,   268,    16, 18777,   318,     2,\n",
       "             17,  2108,     9,    79,  2497,     8,   602,    80,     9,     2,\n",
       "           4896,     2,     4,     3, 25309,     8,  2111,  9960,     2,     3,\n",
       "           5679,  1501,    41,    55,    72,   212,   152,    74,  1202,  4896,\n",
       "              2, 18267,     2,     3, 39298,     7,     3,   231,   894,     2,\n",
       "             36,  3177,    77,     7,     3,  6291,    14,   689,     5,    74,\n",
       "           1501,     4,    60,    14,   221,     3,   372,    12,    68,     6,\n",
       "           1420,  4059,   817,     8,  3647,   186,     3,   396,     2,    14,\n",
       "           1198, 15590,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "              4,    37,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "              4,     4,   318,     4,     6,   360,   351,  2742,    14,     2,\n",
       "            148,   134,     8,  7578,    35,     7,   136,  4896,     4,  1420,\n",
       "           2374,     8, 18777,   318,     4,    14,   529,    16,   117,  1496,\n",
       "              7,    66,   558,   108,    16, 18777,   318,     9,   237,  4233,\n",
       "              4,    54,     6,  2216,    16,    11,   227,     2,    28,   214],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,    69,     7,     6,   133,    41,    53,\n",
       "           8181,  1422,    21,     6,  3984,     4,   526,    51,    22,     6,\n",
       "            628,   142,    16,     9,     6,  1268,   466,     7,  1748,   217,\n",
       "              4,     6, 11253,  7558,   299,     9,   676,    92,    40,  2229,\n",
       "              2,  1173,  3453,    39,     3,   930, 45759,     7,    11,     2,\n",
       "             17,  5364,     4,   479,    11,  2715,  1748,     3,   231,    63,\n",
       "             22,    64,   810,  1401,   862,   236,    11,    49,   104,   131,\n",
       "           1459,     4,    67,   153,    43,     3,   952,   145,    34,   676,\n",
       "            131,     4,     3, 12883,   409,    65,   103,  1620,   312,   781,\n",
       "              8,     6,   864, 10275,     4,    26,     6,  1808,   641,    11,\n",
       "              2,    17,   135,    80,    27,   240,   108,    22,    56,    57,\n",
       "            626,    39,   710,    87, 41756, 42117,     4,   710,   382,  3629,\n",
       "          12053,     5, 16107,  7815,    55,    34,   115,  3344,     4,    45]],\n",
       "        dtype=torch.int32))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "finished-chambers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), torch.Size([2, 200]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl.shape, txt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "found-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_dataset = TextData(labels[:split_idx-1], data[:split_idx-1])\n",
    "valid_dataset = TextData(labels[split_idx-1:test_val_idx], data[split_idx-1:test_val_idx])\n",
    "test_dataset = TextData(labels[test_val_idx:], data[test_val_idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "generic-luxembourg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 2500, 20000)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset), len(valid_dataset), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "becoming-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader =  DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=tokenize_batch)\n",
    "valid_loader =  DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=tokenize_batch)\n",
    "test_loader =  DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=tokenize_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-correlation",
   "metadata": {},
   "source": [
    "## Make a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "first-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#         Input shape     torch.Size([2, 200])\n",
    "#         Embedding shape torch.Size([2, 200, 32]) 50\n",
    "#         Conv1d shape    torch.Size([2, 32, 30]) 48\n",
    "#         MaxPool shape   torch.Size([2, 32, 15]) 24\n",
    "#         LSTM shape      torch.Size([2, 32, 64])\n",
    "#         Dense shape     torch.Size([2, 32, 1])\n",
    "#         Sigmoid shape   torch.Size([2])\n",
    "\n",
    "class SentimentConvNN(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size=1, embedding_dim=32, hidden_dim=64, out_channels=32, drop_prob=0.5, vocab_vectors=None):\n",
    "        super(SentimentConvNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_size = output_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.drop_prob = drop_prob\n",
    "        self.out_channels = out_channels\n",
    "        self.n_layers = 1\n",
    "        \n",
    "        # if we provide vocab_vectors then initialize weights\n",
    "        if vocab_vectors is not None:\n",
    "            self.embed = nn.Embedding.from_pretrained(vocab_vectors, freeze=True)\n",
    "        else:\n",
    "            self.embed = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "            \n",
    "        self.conv1d = nn.Conv1d(in_channels=200, out_channels=self.out_channels, kernel_size=3, bias=False, padding=False)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        lstm_input = int((self.embed.weight.shape[-1]-2)/2) # repoduce the logic of conv1d resulting dimention\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=lstm_input,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            dropout=0)\n",
    "        self.dense = nn.Linear(hidden_dim, 1)\n",
    "        self.drop = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        self.bn_embedding = nn.BatchNorm1d(num_features=200)\n",
    "        self.bn_conv1d = nn.BatchNorm1d(num_features=self.out_channels)\n",
    "        self.bn_lstm = nn.BatchNorm1d(num_features=self.out_channels)\n",
    "    \n",
    "    def num_parameters(self):\n",
    "        '''\n",
    "        get the number of parameters in a network\n",
    "        '''\n",
    "\n",
    "        # return sum((list(map(lambda x: torch.as_tensor(x.flatten().size()).sum().item(), self.parameters()))))\n",
    "        s=\"\"\n",
    "        for k, v in self.named_parameters():\n",
    "            s+=f'{k:20} {v.shape}\\n'\n",
    "        s+=f'Total number of parameters = {sum(list(map(lambda x: x.numel(), self.parameters()))):,}'\n",
    "        return s\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new_zeros(self.n_layers, batch_size, self.hidden_dim),\n",
    "                  weight.new_zeros(self.n_layers, batch_size, self.hidden_dim))\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        '''\n",
    "        Input shape \ttorch.Size([2, 200])\n",
    "        Embedding shape torch.Size([2, 200, 32])\n",
    "        Conv1d shape \ttorch.Size([2, 32, 30])\n",
    "        MaxPool shape \ttorch.Size([2, 32, 15])\n",
    "        LSTM shape \t\ttorch.Size([2, 32, 64])\n",
    "        Dense shape \ttorch.Size([2, 32, 1])\n",
    "        Sigmoid shape \ttorch.Size([2])\n",
    "        '''\n",
    "        #print(x.dtype)\n",
    "        embed_out = self.embed(x)\n",
    "        embed_out = self.bn_embedding(embed_out)\n",
    "        embed_out = self.drop(embed_out)\n",
    "        \n",
    "        conv_out = self.conv1d(embed_out)\n",
    "        conv_out = self.bn_conv1d(conv_out)\n",
    "        conv_out_relu = F.relu(conv_out)\n",
    "        maxpool_out = self.maxpool(conv_out_relu)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(maxpool_out, hidden)\n",
    "        lstm_out = self.bn_lstm(lstm_out)\n",
    "        lstm_out = self.drop(lstm_out)\n",
    "        \n",
    "        out_dense = self.dense(lstm_out)\n",
    "        out = nn.Sigmoid()(out_dense[:,-1,:]).view(out_dense.shape[0])\n",
    "        \n",
    "        return out, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "english-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparams\n",
    "vocab_size = len(vocab) \n",
    "output_size = 1 # not needed\n",
    "embedding_dim = 32\n",
    "hidden_dim = 64\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "special-dodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.weight         torch.Size([67359, 32])\n",
      "conv1d.weight        torch.Size([32, 200, 3])\n",
      "lstm.weight_ih_l0    torch.Size([256, 15])\n",
      "lstm.weight_hh_l0    torch.Size([256, 64])\n",
      "lstm.bias_ih_l0      torch.Size([256])\n",
      "lstm.bias_hh_l0      torch.Size([256])\n",
      "dense.weight         torch.Size([1, 64])\n",
      "dense.bias           torch.Size([1])\n",
      "bn_embedding.weight  torch.Size([200])\n",
      "bn_embedding.bias    torch.Size([200])\n",
      "bn_conv1d.weight     torch.Size([32])\n",
      "bn_conv1d.bias       torch.Size([32])\n",
      "bn_lstm.weight       torch.Size([32])\n",
      "bn_lstm.bias         torch.Size([32])\n",
      "Total number of parameters = 2,196,017\n"
     ]
    }
   ],
   "source": [
    "convRNN = SentimentConvNN(vocab_size=vocab_size)\n",
    "print(convRNN.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "wireless-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = convRNN.init_hidden(batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "massive-surveillance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1950, 0.5091]), torch.Size([2]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out, _ = convRNN.forward(txt, h0)\n",
    "out, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "awful-binary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentConvNN(\n",
       "  (embed): Embedding(67359, 32)\n",
       "  (conv1d): Conv1d(200, 32, kernel_size=(3,), stride=(1,), bias=False)\n",
       "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (lstm): LSTM(15, 64, batch_first=True)\n",
       "  (dense): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (bn_embedding): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_conv1d): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_lstm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "convRNN.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-christmas",
   "metadata": {},
   "source": [
    "### Training conv RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "civil-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_score(net, val_loader, criterion):\n",
    "    '''\n",
    "    calculates validation loss\n",
    "    does not put a net into eval mode - have to do this manually before val_score call\n",
    "    \n",
    "    '''\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    loss = []\n",
    "    # that is the number of objects in loader, not batches\n",
    "    number_of_objects = len(val_loader.dataset)\n",
    "    \n",
    "    # make array of zeros with the shape of response\n",
    "    pred_y = np.zeros(number_of_objects)\n",
    "    true_y = np.zeros_like(pred_y)\n",
    "\n",
    "    # store a batch size \n",
    "    batch_size = val_loader.batch_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ii, (test_x, test_y) in enumerate(val_loader):\n",
    "            h = net.init_hidden(test_y.shape[0])\n",
    "            test_x, test_y = test_x.to(device), test_y.to(device)\n",
    "            out, _ = net.forward(test_x, h)\n",
    "            batch_loss = criterion(out, test_y)\n",
    "            \n",
    "            # store predictions and true labels\n",
    "            pred_y[ii*batch_size:ii*batch_size + len(test_y)] = out.to('cpu').numpy()\n",
    "            true_y[ii*batch_size:ii*batch_size + len(test_y)] = test_y.to('cpu').numpy()\n",
    "            \n",
    "            loss.append(batch_loss.item())\n",
    "    \n",
    "    precision = precision_score(true_y, np.round(pred_y))\n",
    "    recall = recall_score(true_y, np.round(pred_y))\n",
    "    accuracy = accuracy_score(true_y, np.round(pred_y))\n",
    "    fscore = f1_score(true_y, np.round(pred_y))\n",
    "    \n",
    "    metrics = {'precision':precision, 'recall':recall, 'accuracy':accuracy, 'fscore':fscore}\n",
    "    \n",
    "    return np.mean(loss), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "comparative-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_SWA(net, criterion, optimizer, train_loader, valid_loader, clip_value=5, epochs=10, print_every=200, max_fscore=-np.inf):\n",
    "    '''\n",
    "    Train the network\n",
    "        net - network to trian\n",
    "        criterion - loss function \n",
    "        optimizer - your optimiser of choice \n",
    "        train_loader - loader for training data\n",
    "        vlid_loader - lodaer for validation/test data\n",
    "        clip_value - upper limit for gradient \n",
    "        epochs - number of epochs to train the net\n",
    "        print_every - prin stats every number of batches\n",
    "        max_fscore - best fscore on validation set - used in mutiple runs of training\n",
    "    '''\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "    \n",
    "    steps = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    # run over epochs of training\n",
    "    for e in trange(epochs):\n",
    "        \n",
    "        # array to keep value of losses over current epoch\n",
    "        train_loss = []\n",
    "\n",
    "        # run one pass through training samples = one epoch\n",
    "        for train_x, train_y in train_loader:\n",
    "            steps +=1\n",
    "\n",
    "            # zero out the grads \n",
    "            net.zero_grad()\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            # send data to device\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "\n",
    "            # initialize hidden state\n",
    "            h = net.init_hidden(len(train_x))\n",
    "\n",
    "            # calculate the output of the network\n",
    "            out, _ = net(train_x, h)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = criterion(out, train_y)\n",
    "            # backprop grads of the loss wrt to net parameters\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip_value)\n",
    "\n",
    "            # upadate parameters of network\n",
    "            optimizer.step()\n",
    "\n",
    "            # append current batch loss (loss per object in current batch)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            # test loss calc every \n",
    "            if steps%print_every == 0:\n",
    "                # calculate loss on test set\n",
    "                if max_fscore > 0.75:\n",
    "                    optimizer.update_swa()\n",
    "                    optimizer.swap_swa_sgd() # use SWA weights for the calc of validation loss\n",
    "                    for g in optimizer.param_groups:\n",
    "                        g['lr'] = 0.0005\n",
    "                net.eval()\n",
    "                test_loss, metrics = val_score(net, valid_loader, criterion)\n",
    "                if metrics['fscore'] > max_fscore:\n",
    "                    max_fscore = metrics['fscore']\n",
    "                    message = '=)'\n",
    "                    check_point = {'vocab_size':net.vocab_size, \n",
    "                                   'embedding_dim': net.embedding_dim, \n",
    "                                   'hidden_dim':net.hidden_dim, \n",
    "                                   'n_layers':net.n_layers, \n",
    "                                   'net_params':net.state_dict()}\n",
    "                    torch.save(check_point, f\"spam_model_fscore_{metrics['fscore']:.3f}.pt\")\n",
    "                else:\n",
    "                    message = ';('\n",
    "                if max_fscore > 0.75:\n",
    "                    optimizer.swap_swa_sgd() # swap back normal weights and continue training\n",
    "                net.train()\n",
    "                print(f\"Step {steps} epoch {e+1}. {message}\\nTest loss is {test_loss:.4f}. Train loss is {np.mean(train_loss):.4f}.\\\n",
    "                F1 Score={metrics['fscore']:.2%} Precision={metrics['precision']:.2%} Recall={metrics['recall']:.2%} Accuracy={metrics['accuracy']:.2%}\\n\")\n",
    "    return max_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "atlantic-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fscore = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "crude-cycling",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SWA in module torchcontrib.optim.swa:\n",
      "\n",
      "class SWA(torch.optim.optimizer.Optimizer)\n",
      " |  Base class for all optimizers.\n",
      " |  \n",
      " |  .. warning::\n",
      " |      Parameters need to be specified as collections that have a deterministic\n",
      " |      ordering that is consistent between runs. Examples of objects that don't\n",
      " |      satisfy those properties are sets and iterators over values of dictionaries.\n",
      " |  \n",
      " |  Args:\n",
      " |      params (iterable): an iterable of :class:`torch.Tensor` s or\n",
      " |          :class:`dict` s. Specifies what Tensors should be optimized.\n",
      " |      defaults: (dict): a dict containing default values of optimization\n",
      " |          options (used when a parameter group doesn't specify them).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SWA\n",
      " |      torch.optim.optimizer.Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, optimizer, swa_start=None, swa_freq=None, swa_lr=None)\n",
      " |      Implements Stochastic Weight Averaging (SWA).\n",
      " |      \n",
      " |      Stochastic Weight Averaging was proposed in `Averaging Weights Leads to\n",
      " |      Wider Optima and Better Generalization`_ by Pavel Izmailov, Dmitrii\n",
      " |      Podoprikhin, Timur Garipov, Dmitry Vetrov and Andrew Gordon Wilson\n",
      " |      (UAI 2018).\n",
      " |      \n",
      " |      SWA is implemented as a wrapper class taking optimizer instance as input\n",
      " |      and applying SWA on top of that optimizer.\n",
      " |      \n",
      " |      SWA can be used in two modes: automatic and manual. In the automatic\n",
      " |      mode SWA running averages are automatically updated every\n",
      " |      :attr:`swa_freq` steps after :attr:`swa_start` steps of optimization. If\n",
      " |      :attr:`swa_lr` is provided, the learning rate of the optimizer is reset\n",
      " |      to :attr:`swa_lr` at every step starting from :attr:`swa_start`. To use\n",
      " |      SWA in automatic mode provide values for both :attr:`swa_start` and\n",
      " |      :attr:`swa_freq` arguments.\n",
      " |      \n",
      " |      Alternatively, in the manual mode, use :meth:`update_swa` or\n",
      " |      :meth:`update_swa_group` methods to update the SWA running averages.\n",
      " |      \n",
      " |      In the end of training use `swap_swa_sgd` method to set the optimized\n",
      " |      variables to the computed averages.\n",
      " |      \n",
      " |      Args:\n",
      " |          optimizer (torch.optim.Optimizer): optimizer to use with SWA\n",
      " |          swa_start (int): number of steps before starting to apply SWA in\n",
      " |              automatic mode; if None, manual mode is selected (default: None)\n",
      " |          swa_freq (int): number of steps between subsequent updates of\n",
      " |              SWA running averages in automatic mode; if None, manual mode is\n",
      " |              selected (default: None)\n",
      " |          swa_lr (float): learning rate to use starting from step swa_start\n",
      " |              in automatic mode; if None, learning rate is not changed\n",
      " |              (default: None)\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> # automatic mode\n",
      " |          >>> base_opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
      " |          >>> opt = torchcontrib.optim.SWA(\n",
      " |          >>>                 base_opt, swa_start=10, swa_freq=5, swa_lr=0.05)\n",
      " |          >>> for _ in range(100):\n",
      " |          >>>     opt.zero_grad()\n",
      " |          >>>     loss_fn(model(input), target).backward()\n",
      " |          >>>     opt.step()\n",
      " |          >>> opt.swap_swa_sgd()\n",
      " |          >>> # manual mode\n",
      " |          >>> opt = torchcontrib.optim.SWA(base_opt)\n",
      " |          >>> for i in range(100):\n",
      " |          >>>     opt.zero_grad()\n",
      " |          >>>     loss_fn(model(input), target).backward()\n",
      " |          >>>     opt.step()\n",
      " |          >>>     if i > 10 and i % 5 == 0:\n",
      " |          >>>         opt.update_swa()\n",
      " |          >>> opt.swap_swa_sgd()\n",
      " |      \n",
      " |      .. note::\n",
      " |          SWA does not support parameter-specific values of :attr:`swa_start`,\n",
      " |          :attr:`swa_freq` or :attr:`swa_lr`. In automatic mode SWA uses the\n",
      " |          same :attr:`swa_start`, :attr:`swa_freq` and :attr:`swa_lr` for all\n",
      " |          parameter groups. If needed, use manual mode with\n",
      " |          :meth:`update_swa_group` to use different update schedules for\n",
      " |          different parameter groups.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Call :meth:`swap_swa_sgd` in the end of training to use the computed\n",
      " |          running averages.\n",
      " |      \n",
      " |      .. note::\n",
      " |          If you are using SWA to optimize the parameters of a Neural Network\n",
      " |          containing Batch Normalization layers, you need to update the\n",
      " |          :attr:`running_mean` and :attr:`running_var` statistics of the\n",
      " |          Batch Normalization module. You can do so by using\n",
      " |          `torchcontrib.optim.swa.bn_update` utility.\n",
      " |      \n",
      " |      .. _Averaging Weights Leads to Wider Optima and Better Generalization:\n",
      " |          https://arxiv.org/abs/1803.05407\n",
      " |      .. _Improving Consistency-Based Semi-Supervised Learning with Weight\n",
      " |          Averaging:\n",
      " |          https://arxiv.org/abs/1806.05594\n",
      " |  \n",
      " |  add_param_group(self, param_group)\n",
      " |      Add a param group to the :class:`Optimizer` s `param_groups`.\n",
      " |      \n",
      " |      This can be useful when fine tuning a pre-trained network as frozen\n",
      " |      layers can be made trainable and added to the :class:`Optimizer` as\n",
      " |      training progresses.\n",
      " |      \n",
      " |      Args:\n",
      " |          param_group (dict): Specifies what Tensors should be optimized along\n",
      " |          with group specific optimization options.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Loads the optimizer state.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): SWA optimizer state. Should be an object returned\n",
      " |              from a call to `state_dict`.\n",
      " |  \n",
      " |  state_dict(self)\n",
      " |      Returns the state of SWA as a :class:`dict`.\n",
      " |      \n",
      " |      It contains three entries:\n",
      " |          * opt_state - a dict holding current optimization state of the base\n",
      " |              optimizer. Its content differs between optimizer classes.\n",
      " |          * swa_state - a dict containing current state of SWA. For each\n",
      " |              optimized variable it contains swa_buffer keeping the running\n",
      " |              average of the variable\n",
      " |          * param_groups - a dict containing all parameter groups\n",
      " |  \n",
      " |  step(self, closure=None)\n",
      " |      Performs a single optimization step.\n",
      " |      \n",
      " |      In automatic mode also updates SWA running averages.\n",
      " |  \n",
      " |  swap_swa_sgd(self)\n",
      " |      Swaps the values of the optimized variables and swa buffers.\n",
      " |      \n",
      " |      It's meant to be called in the end of training to use the collected\n",
      " |      swa running averages. It can also be used to evaluate the running\n",
      " |      averages during training; to continue training `swap_swa_sgd`\n",
      " |      should be called again.\n",
      " |  \n",
      " |  update_swa(self)\n",
      " |      Updates the SWA running averages of all optimized parameters.\n",
      " |  \n",
      " |  update_swa_group(self, group)\n",
      " |      Updates the SWA running averages for the given parameter group.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          param_group (dict): Specifies for what parameter group SWA running\n",
      " |              averages should be updated\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> # automatic mode\n",
      " |          >>> base_opt = torch.optim.SGD([{'params': [x]},\n",
      " |          >>>             {'params': [y], 'lr': 1e-3}], lr=1e-2, momentum=0.9)\n",
      " |          >>> opt = torchcontrib.optim.SWA(base_opt)\n",
      " |          >>> for i in range(100):\n",
      " |          >>>     opt.zero_grad()\n",
      " |          >>>     loss_fn(model(input), target).backward()\n",
      " |          >>>     opt.step()\n",
      " |          >>>     if i > 10 and i % 5 == 0:\n",
      " |          >>>         # Update SWA for the second parameter group\n",
      " |          >>>         opt.update_swa_group(opt.param_groups[1])\n",
      " |          >>> opt.swap_swa_sgd()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  bn_update(loader, model, device=None)\n",
      " |      Updates BatchNorm running_mean, running_var buffers in the model.\n",
      " |      \n",
      " |      It performs one pass over data in `loader` to estimate the activation\n",
      " |      statistics for BatchNorm layers in the model.\n",
      " |      \n",
      " |      Args:\n",
      " |          loader (torch.utils.data.DataLoader): dataset loader to compute the\n",
      " |              activation statistics on. Each data batch should be either a\n",
      " |              tensor, or a list/tuple whose first element is a tensor\n",
      " |              containing data.\n",
      " |      \n",
      " |          model (torch.nn.Module): model for which we seek to update BatchNorm\n",
      " |              statistics.\n",
      " |      \n",
      " |          device (torch.device, optional): If set, data will be trasferred to\n",
      " |              :attr:`device` before being passed into :attr:`model`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  zero_grad(self, set_to_none:bool=False)\n",
      " |      Sets the gradients of all optimized :class:`torch.Tensor` s to zero.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              This will in general have lower memory footprint, and can modestly improve performance.\n",
      " |              However, it changes certain behaviors. For example:\n",
      " |              1. When the user tries to access a gradient and perform manual ops on it,\n",
      " |              a None attribute or a Tensor full of 0s will behave differently.\n",
      " |              2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s\n",
      " |              are guaranteed to be None for params that did not receive a gradient.\n",
      " |              3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n",
      " |              (in one case it does the step with a gradient of 0 and in the other it skips\n",
      " |              the step altogether).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SWA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fleet-access",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vocab_size', 'embedding_dim', 'hidden_dim', 'n_layers', 'net_params'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('spam_model_fscore_0.843.pt')\n",
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fossil-blood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convRNN.load_state_dict(model['net_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "stuck-elevation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(convRNN.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "# opt = torchcontrib.optim.SWA(\n",
    "#                 base_opt, swa_start=10, swa_freq=5, swa_lr=0.05)\n",
    "# for _ in range(100):\n",
    "#     opt.zero_grad()\n",
    "#     loss_fn(model(input), target).backward()\n",
    "#     opt.step()\n",
    "# opt.swap_swa_sgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "operating-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "criterion = nn.BCELoss(reduction='mean')\n",
    "base_opt = torch.optim.Adam(convRNN.parameters(), lr=lr, weight_decay=0.005)\n",
    "#optimizer = torch.optim.SGD(convRNN.parameters(), lr=lr, momentum=0.9, nesterov=True)\n",
    "#optimizer = torch.optim.RMSprop(convRNN.parameters(), lr=lr, momentum=0.9)\n",
    "lr = 0.001\n",
    "#base_opt = torch.optim.SGD(convRNN.parameters(), lr=lr)\n",
    "optimizer = SWA(base_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "eligible-facial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    print(g['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "educated-council",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e24a71fc1f9485890973e65b0843f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 epoch 1. ;(\n",
      "Test loss is 0.8857. Train loss is 0.1112.                F1 Score=84.88% Precision=86.99% Recall=82.88% Accuracy=85.24%\n",
      "\n",
      "Step 400 epoch 1. ;(\n",
      "Test loss is 0.9245. Train loss is 0.1272.                F1 Score=84.68% Precision=87.27% Recall=82.24% Accuracy=85.12%\n",
      "\n",
      "Step 600 epoch 2. ;(\n",
      "Test loss is 0.9929. Train loss is 0.1085.                F1 Score=84.89% Precision=86.73% Recall=83.12% Accuracy=85.20%\n",
      "\n",
      "Step 800 epoch 2. ;(\n",
      "Test loss is 1.0517. Train loss is 0.1230.                F1 Score=84.57% Precision=87.04% Recall=82.24% Accuracy=85.00%\n",
      "\n",
      "Step 1000 epoch 3. ;(\n",
      "Test loss is 1.1424. Train loss is 0.1166.                F1 Score=84.91% Precision=86.61% Recall=83.28% Accuracy=85.20%\n",
      "\n",
      "Step 1200 epoch 3. ;(\n",
      "Test loss is 1.3974. Train loss is 0.1272.                F1 Score=84.97% Precision=86.56% Recall=83.44% Accuracy=85.24%\n",
      "\n",
      "Step 1400 epoch 4. ;(\n",
      "Test loss is 2.1828. Train loss is 0.1135.                F1 Score=85.09% Precision=86.46% Recall=83.76% Accuracy=85.32%\n",
      "\n",
      "Step 1600 epoch 4. ;(\n",
      "Test loss is 2.4573. Train loss is 0.1258.                F1 Score=84.92% Precision=85.61% Recall=84.24% Accuracy=85.04%\n",
      "\n",
      "Step 1800 epoch 5. ;(\n",
      "Test loss is 3.0516. Train loss is 0.1142.                F1 Score=84.86% Precision=84.76% Recall=84.96% Accuracy=84.84%\n",
      "\n",
      "Step 2000 epoch 5. ;(\n",
      "Test loss is 3.2994. Train loss is 0.1295.                F1 Score=84.62% Precision=84.93% Recall=84.32% Accuracy=84.68%\n",
      "\n",
      "Step 2200 epoch 6. ;(\n",
      "Test loss is 3.2048. Train loss is 0.1132.                F1 Score=84.70% Precision=85.01% Recall=84.40% Accuracy=84.76%\n",
      "\n",
      "Step 2400 epoch 6. ;(\n",
      "Test loss is 3.1211. Train loss is 0.1275.                F1 Score=84.60% Precision=84.81% Recall=84.40% Accuracy=84.64%\n",
      "\n",
      "Step 2600 epoch 7. ;(\n",
      "Test loss is 3.1813. Train loss is 0.1014.                F1 Score=84.63% Precision=85.11% Recall=84.16% Accuracy=84.72%\n",
      "\n",
      "Step 2800 epoch 7. ;(\n",
      "Test loss is 3.2187. Train loss is 0.1223.                F1 Score=84.60% Precision=84.81% Recall=84.40% Accuracy=84.64%\n",
      "\n",
      "Step 3000 epoch 8. ;(\n",
      "Test loss is 3.1873. Train loss is 0.1097.                F1 Score=84.80% Precision=84.80% Recall=84.80% Accuracy=84.80%\n",
      "\n",
      "Step 3200 epoch 8. ;(\n",
      "Test loss is 3.0252. Train loss is 0.1241.                F1 Score=84.70% Precision=85.25% Recall=84.16% Accuracy=84.80%\n",
      "\n",
      "Step 3400 epoch 9. ;(\n",
      "Test loss is 3.2734. Train loss is 0.1109.                F1 Score=84.67% Precision=85.18% Recall=84.16% Accuracy=84.76%\n",
      "\n",
      "Step 3600 epoch 9. ;(\n",
      "Test loss is 2.8786. Train loss is 0.1249.                F1 Score=84.61% Precision=85.23% Recall=84.00% Accuracy=84.72%\n",
      "\n",
      "Step 3800 epoch 10. ;(\n",
      "Test loss is 2.8520. Train loss is 0.1098.                F1 Score=84.79% Precision=85.52% Recall=84.08% Accuracy=84.92%\n",
      "\n",
      "Step 4000 epoch 10. ;(\n",
      "Test loss is 2.8650. Train loss is 0.1252.                F1 Score=84.84% Precision=85.05% Recall=84.64% Accuracy=84.88%\n",
      "\n",
      "Step 4200 epoch 11. ;(\n",
      "Test loss is 2.7497. Train loss is 0.1082.                F1 Score=84.84% Precision=85.53% Recall=84.16% Accuracy=84.96%\n",
      "\n",
      "Step 4400 epoch 11. ;(\n",
      "Test loss is 2.6334. Train loss is 0.1233.                F1 Score=84.89% Precision=85.30% Recall=84.48% Accuracy=84.96%\n",
      "\n",
      "Step 4600 epoch 12. ;(\n",
      "Test loss is 2.5041. Train loss is 0.1120.                F1 Score=84.96% Precision=85.87% Recall=84.08% Accuracy=85.12%\n",
      "\n",
      "Step 4800 epoch 12. ;(\n",
      "Test loss is 2.4952. Train loss is 0.1220.                F1 Score=84.88% Precision=86.03% Recall=83.76% Accuracy=85.08%\n",
      "\n",
      "Step 5000 epoch 13. ;(\n",
      "Test loss is 2.6730. Train loss is 0.1155.                F1 Score=84.96% Precision=85.87% Recall=84.08% Accuracy=85.12%\n",
      "\n",
      "Step 5200 epoch 13. ;(\n",
      "Test loss is 2.5161. Train loss is 0.1313.                F1 Score=84.75% Precision=85.26% Recall=84.24% Accuracy=84.84%\n",
      "\n",
      "Step 5400 epoch 14. ;(\n",
      "Test loss is 2.4409. Train loss is 0.1077.                F1 Score=84.99% Precision=86.00% Recall=84.00% Accuracy=85.16%\n",
      "\n",
      "Step 5600 epoch 14. ;(\n",
      "Test loss is 2.3140. Train loss is 0.1249.                F1 Score=84.61% Precision=85.90% Recall=83.36% Accuracy=84.84%\n",
      "\n",
      "Step 5800 epoch 15. ;(\n",
      "Test loss is 2.4138. Train loss is 0.1028.                F1 Score=84.88% Precision=86.03% Recall=83.76% Accuracy=85.08%\n",
      "\n",
      "Step 6000 epoch 15. ;(\n",
      "Test loss is 2.3780. Train loss is 0.1235.                F1 Score=84.84% Precision=85.77% Recall=83.92% Accuracy=85.00%\n",
      "\n",
      "Step 6200 epoch 16. ;(\n",
      "Test loss is 2.3430. Train loss is 0.1104.                F1 Score=84.87% Precision=85.60% Recall=84.16% Accuracy=85.00%\n",
      "\n",
      "Step 6400 epoch 16. ;(\n",
      "Test loss is 2.3356. Train loss is 0.1257.                F1 Score=84.70% Precision=86.18% Recall=83.28% Accuracy=84.96%\n",
      "\n",
      "Step 6600 epoch 17. ;(\n",
      "Test loss is 2.2052. Train loss is 0.1118.                F1 Score=84.53% Precision=86.07% Recall=83.04% Accuracy=84.80%\n",
      "\n",
      "Step 6800 epoch 17. ;(\n",
      "Test loss is 2.2647. Train loss is 0.1230.                F1 Score=84.74% Precision=85.75% Recall=83.76% Accuracy=84.92%\n",
      "\n",
      "Step 7000 epoch 18. ;(\n",
      "Test loss is 2.2124. Train loss is 0.1140.                F1 Score=84.76% Precision=85.88% Recall=83.68% Accuracy=84.96%\n",
      "\n",
      "Step 7200 epoch 18. ;(\n",
      "Test loss is 2.2311. Train loss is 0.1255.                F1 Score=84.76% Precision=86.13% Recall=83.44% Accuracy=85.00%\n",
      "\n",
      "Step 7400 epoch 19. ;(\n",
      "Test loss is 1.9894. Train loss is 0.1123.                F1 Score=84.53% Precision=86.07% Recall=83.04% Accuracy=84.80%\n",
      "\n",
      "Step 7600 epoch 19. ;(\n",
      "Test loss is 1.9181. Train loss is 0.1304.                F1 Score=84.53% Precision=86.07% Recall=83.04% Accuracy=84.80%\n",
      "\n",
      "Step 7800 epoch 20. ;(\n",
      "Test loss is 1.7975. Train loss is 0.1101.                F1 Score=84.40% Precision=86.23% Recall=82.64% Accuracy=84.72%\n",
      "\n",
      "Step 8000 epoch 20. ;(\n",
      "Test loss is 1.9128. Train loss is 0.1267.                F1 Score=84.67% Precision=86.10% Recall=83.28% Accuracy=84.92%\n",
      "\n",
      "Step 8200 epoch 21. ;(\n",
      "Test loss is 1.7727. Train loss is 0.1069.                F1 Score=84.57% Precision=86.08% Recall=83.12% Accuracy=84.84%\n",
      "\n",
      "Step 8400 epoch 21. ;(\n",
      "Test loss is 1.5424. Train loss is 0.1259.                F1 Score=84.40% Precision=86.23% Recall=82.64% Accuracy=84.72%\n",
      "\n",
      "Step 8600 epoch 22. ;(\n",
      "Test loss is 1.8044. Train loss is 0.1139.                F1 Score=84.54% Precision=86.01% Recall=83.12% Accuracy=84.80%\n",
      "\n",
      "Step 8800 epoch 22. ;(\n",
      "Test loss is 1.6370. Train loss is 0.1276.                F1 Score=84.67% Precision=86.10% Recall=83.28% Accuracy=84.92%\n",
      "\n",
      "Step 9000 epoch 23. ;(\n",
      "Test loss is 1.6318. Train loss is 0.1075.                F1 Score=84.04% Precision=86.27% Recall=81.92% Accuracy=84.44%\n",
      "\n",
      "Step 9200 epoch 23. ;(\n",
      "Test loss is 1.5312. Train loss is 0.1223.                F1 Score=84.27% Precision=86.13% Recall=82.48% Accuracy=84.60%\n",
      "\n",
      "Step 9400 epoch 24. ;(\n",
      "Test loss is 1.6536. Train loss is 0.1074.                F1 Score=84.40% Precision=85.98% Recall=82.88% Accuracy=84.68%\n",
      "\n",
      "Step 9600 epoch 24. ;(\n",
      "Test loss is 1.6162. Train loss is 0.1295.                F1 Score=84.28% Precision=86.07% Recall=82.56% Accuracy=84.60%\n",
      "\n",
      "Step 9800 epoch 25. ;(\n",
      "Test loss is 1.4244. Train loss is 0.1086.                F1 Score=83.74% Precision=86.00% Recall=81.60% Accuracy=84.16%\n",
      "\n",
      "Step 10000 epoch 25. ;(\n",
      "Test loss is 1.4452. Train loss is 0.1274.                F1 Score=84.21% Precision=85.93% Recall=82.56% Accuracy=84.52%\n",
      "\n",
      "Step 10200 epoch 26. ;(\n",
      "Test loss is 1.3920. Train loss is 0.1090.                F1 Score=83.98% Precision=86.06% Recall=82.00% Accuracy=84.36%\n",
      "\n",
      "Step 10400 epoch 26. ;(\n",
      "Test loss is 1.4786. Train loss is 0.1231.                F1 Score=83.68% Precision=86.05% Recall=81.44% Accuracy=84.12%\n",
      "\n",
      "Step 10600 epoch 27. ;(\n",
      "Test loss is 1.3799. Train loss is 0.1089.                F1 Score=83.68% Precision=86.31% Recall=81.20% Accuracy=84.16%\n",
      "\n",
      "Step 10800 epoch 27. ;(\n",
      "Test loss is 1.2623. Train loss is 0.1269.                F1 Score=83.90% Precision=85.98% Recall=81.92% Accuracy=84.28%\n",
      "\n",
      "Step 11000 epoch 28. ;(\n",
      "Test loss is 0.7866. Train loss is 0.1048.                F1 Score=82.90% Precision=88.77% Recall=77.76% Accuracy=83.96%\n",
      "\n",
      "Step 11200 epoch 28. ;(\n",
      "Test loss is 1.2033. Train loss is 0.1172.                F1 Score=83.55% Precision=86.67% Recall=80.64% Accuracy=84.12%\n",
      "\n",
      "Step 11400 epoch 29. ;(\n",
      "Test loss is 1.2270. Train loss is 0.1089.                F1 Score=83.77% Precision=86.33% Recall=81.36% Accuracy=84.24%\n",
      "\n",
      "Step 11600 epoch 29. ;(\n",
      "Test loss is 1.2247. Train loss is 0.1259.                F1 Score=83.82% Precision=86.34% Recall=81.44% Accuracy=84.28%\n",
      "\n",
      "Step 11800 epoch 30. ;(\n",
      "Test loss is 1.3264. Train loss is 0.1079.                F1 Score=83.65% Precision=86.43% Recall=81.04% Accuracy=84.16%\n",
      "\n",
      "Step 12000 epoch 30. ;(\n",
      "Test loss is 1.2347. Train loss is 0.1230.                F1 Score=83.71% Precision=86.38% Recall=81.20% Accuracy=84.20%\n",
      "\n",
      "Step 12200 epoch 31. ;(\n",
      "Test loss is 1.1536. Train loss is 0.1122.                F1 Score=83.57% Precision=86.35% Recall=80.96% Accuracy=84.08%\n",
      "\n",
      "Step 12400 epoch 31. ;(\n",
      "Test loss is 1.2172. Train loss is 0.1300.                F1 Score=83.58% Precision=86.29% Recall=81.04% Accuracy=84.08%\n",
      "\n",
      "Step 12600 epoch 32. ;(\n",
      "Test loss is 1.2262. Train loss is 0.1184.                F1 Score=83.72% Precision=86.32% Recall=81.28% Accuracy=84.20%\n",
      "\n",
      "Step 12800 epoch 32. ;(\n",
      "Test loss is 1.1536. Train loss is 0.1325.                F1 Score=83.71% Precision=85.93% Recall=81.60% Accuracy=84.12%\n",
      "\n",
      "Step 13000 epoch 33. ;(\n",
      "Test loss is 1.1867. Train loss is 0.1045.                F1 Score=83.68% Precision=85.86% Recall=81.60% Accuracy=84.08%\n",
      "\n",
      "Step 13200 epoch 33. ;(\n",
      "Test loss is 1.1234. Train loss is 0.1266.                F1 Score=83.91% Precision=86.17% Recall=81.76% Accuracy=84.32%\n",
      "\n",
      "Step 13400 epoch 34. ;(\n",
      "Test loss is 1.1606. Train loss is 0.1063.                F1 Score=83.62% Precision=86.56% Recall=80.88% Accuracy=84.16%\n",
      "\n",
      "Step 13600 epoch 34. ;(\n",
      "Test loss is 1.2190. Train loss is 0.1194.                F1 Score=83.86% Precision=86.62% Recall=81.28% Accuracy=84.36%\n",
      "\n",
      "Step 13800 epoch 35. ;(\n",
      "Test loss is 1.1051. Train loss is 0.1069.                F1 Score=83.55% Precision=86.67% Recall=80.64% Accuracy=84.12%\n",
      "\n",
      "Step 14000 epoch 35. ;(\n",
      "Test loss is 0.9779. Train loss is 0.1241.                F1 Score=83.92% Precision=86.56% Recall=81.44% Accuracy=84.40%\n",
      "\n",
      "Step 14200 epoch 36. ;(\n",
      "Test loss is 1.1389. Train loss is 0.1011.                F1 Score=83.58% Precision=86.29% Recall=81.04% Accuracy=84.08%\n",
      "\n",
      "Step 14400 epoch 36. ;(\n",
      "Test loss is 1.1564. Train loss is 0.1273.                F1 Score=83.67% Precision=86.83% Recall=80.72% Accuracy=84.24%\n",
      "\n",
      "Step 14600 epoch 37. ;(\n",
      "Test loss is 1.1135. Train loss is 0.1102.                F1 Score=83.68% Precision=86.77% Recall=80.80% Accuracy=84.24%\n",
      "\n",
      "Step 14800 epoch 37. ;(\n",
      "Test loss is 1.1127. Train loss is 0.1238.                F1 Score=83.60% Precision=86.42% Recall=80.96% Accuracy=84.12%\n",
      "\n",
      "Step 15000 epoch 38. ;(\n",
      "Test loss is 1.1417. Train loss is 0.1008.                F1 Score=83.57% Precision=86.81% Recall=80.56% Accuracy=84.16%\n",
      "\n",
      "Step 15200 epoch 38. ;(\n",
      "Test loss is 1.0006. Train loss is 0.1163.                F1 Score=83.27% Precision=87.01% Recall=79.84% Accuracy=83.96%\n",
      "\n",
      "Step 15400 epoch 39. ;(\n",
      "Test loss is 1.0339. Train loss is 0.1136.                F1 Score=83.71% Precision=86.64% Recall=80.96% Accuracy=84.24%\n",
      "\n",
      "Step 15600 epoch 39. ;(\n",
      "Test loss is 1.0066. Train loss is 0.1259.                F1 Score=83.53% Precision=86.54% Recall=80.72% Accuracy=84.08%\n",
      "\n",
      "Step 15800 epoch 40. ;(\n",
      "Test loss is 0.9986. Train loss is 0.1110.                F1 Score=83.75% Precision=86.20% Recall=81.44% Accuracy=84.20%\n",
      "\n",
      "Step 16000 epoch 40. ;(\n",
      "Test loss is 0.9791. Train loss is 0.1293.                F1 Score=83.64% Precision=86.24% Recall=81.20% Accuracy=84.12%\n",
      "\n",
      "Step 16200 epoch 41. ;(\n",
      "Test loss is 1.0202. Train loss is 0.1075.                F1 Score=83.54% Precision=86.47% Recall=80.80% Accuracy=84.08%\n",
      "\n",
      "Step 16400 epoch 41. ;(\n",
      "Test loss is 0.9642. Train loss is 0.1206.                F1 Score=83.93% Precision=86.31% Recall=81.68% Accuracy=84.36%\n",
      "\n",
      "Step 16600 epoch 42. ;(\n",
      "Test loss is 1.0217. Train loss is 0.1005.                F1 Score=83.55% Precision=86.41% Recall=80.88% Accuracy=84.08%\n",
      "\n",
      "Step 16800 epoch 42. ;(\n",
      "Test loss is 0.9945. Train loss is 0.1172.                F1 Score=83.45% Precision=86.19% Recall=80.88% Accuracy=83.96%\n",
      "\n",
      "Step 17000 epoch 43. ;(\n",
      "Test loss is 0.9933. Train loss is 0.1151.                F1 Score=83.55% Precision=86.67% Recall=80.64% Accuracy=84.12%\n",
      "\n",
      "Step 17200 epoch 43. ;(\n",
      "Test loss is 0.9562. Train loss is 0.1265.                F1 Score=83.57% Precision=86.81% Recall=80.56% Accuracy=84.16%\n",
      "\n",
      "Step 17400 epoch 44. ;(\n",
      "Test loss is 0.9995. Train loss is 0.1025.                F1 Score=83.65% Precision=86.43% Recall=81.04% Accuracy=84.16%\n",
      "\n",
      "Step 17600 epoch 44. ;(\n",
      "Test loss is 0.9433. Train loss is 0.1203.                F1 Score=83.39% Precision=86.70% Recall=80.32% Accuracy=84.00%\n",
      "\n",
      "Step 17800 epoch 45. ;(\n",
      "Test loss is 1.0004. Train loss is 0.0925.                F1 Score=83.71% Precision=86.64% Recall=80.96% Accuracy=84.24%\n",
      "\n",
      "Step 18000 epoch 45. ;(\n",
      "Test loss is 0.9457. Train loss is 0.1210.                F1 Score=83.38% Precision=86.50% Recall=80.48% Accuracy=83.96%\n",
      "\n",
      "Step 18200 epoch 46. ;(\n",
      "Test loss is 0.9955. Train loss is 0.1051.                F1 Score=83.33% Precision=86.96% Recall=80.00% Accuracy=84.00%\n",
      "\n",
      "Step 18400 epoch 46. ;(\n",
      "Test loss is 0.9093. Train loss is 0.1254.                F1 Score=83.35% Precision=86.63% Recall=80.32% Accuracy=83.96%\n",
      "\n",
      "Step 18600 epoch 47. ;(\n",
      "Test loss is 0.9391. Train loss is 0.1068.                F1 Score=83.27% Precision=86.54% Recall=80.24% Accuracy=83.88%\n",
      "\n",
      "Step 18800 epoch 47. ;(\n",
      "Test loss is 0.9192. Train loss is 0.1232.                F1 Score=83.44% Precision=86.71% Recall=80.40% Accuracy=84.04%\n",
      "\n",
      "Step 19000 epoch 48. ;(\n",
      "Test loss is 0.9211. Train loss is 0.1068.                F1 Score=83.33% Precision=86.96% Recall=80.00% Accuracy=84.00%\n",
      "\n",
      "Step 19200 epoch 48. ;(\n",
      "Test loss is 0.8822. Train loss is 0.1254.                F1 Score=83.71% Precision=86.38% Recall=81.20% Accuracy=84.20%\n",
      "\n",
      "Step 19400 epoch 49. ;(\n",
      "Test loss is 0.8975. Train loss is 0.1092.                F1 Score=83.67% Precision=86.57% Recall=80.96% Accuracy=84.20%\n",
      "\n",
      "Step 19600 epoch 49. ;(\n",
      "Test loss is 0.9203. Train loss is 0.1203.                F1 Score=83.73% Precision=86.52% Recall=81.12% Accuracy=84.24%\n",
      "\n",
      "Step 19800 epoch 50. ;(\n",
      "Test loss is 0.8858. Train loss is 0.1139.                F1 Score=83.15% Precision=86.85% Recall=79.76% Accuracy=83.84%\n",
      "\n",
      "Step 20000 epoch 50. ;(\n",
      "Test loss is 0.8690. Train loss is 0.1242.                F1 Score=83.70% Precision=86.45% Recall=81.12% Accuracy=84.20%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_fscore = trainer_SWA(convRNN, criterion, optimizer, train_loader, valid_loader, clip_value=10, epochs=50, print_every=200, max_fscore=max_fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-craft",
   "metadata": {},
   "source": [
    "## SWA result - you can not train a ntework with SGD... SWA does not give the result here\n",
    "# BUT with Adam in our case it works almost on par with vanilla attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-gospel",
   "metadata": {},
   "source": [
    "## Make convRNN with pretrained embedding from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "collected-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparams\n",
    "vocab_size = len(vocab) \n",
    "output_size = 1 # not needed\n",
    "embedding_dim = 50\n",
    "hidden_dim = 64\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "thermal-trance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.weight         torch.Size([67359, 50])\n",
      "conv1d.weight        torch.Size([32, 200, 3])\n",
      "lstm.weight_ih_l0    torch.Size([256, 24])\n",
      "lstm.weight_hh_l0    torch.Size([256, 64])\n",
      "lstm.bias_ih_l0      torch.Size([256])\n",
      "lstm.bias_hh_l0      torch.Size([256])\n",
      "dense.weight         torch.Size([1, 64])\n",
      "dense.bias           torch.Size([1])\n",
      "bn_embedding.weight  torch.Size([200])\n",
      "bn_embedding.bias    torch.Size([200])\n",
      "bn_conv1d.weight     torch.Size([32])\n",
      "bn_conv1d.bias       torch.Size([32])\n",
      "bn_lstm.weight       torch.Size([32])\n",
      "bn_lstm.bias         torch.Size([32])\n",
      "Total number of parameters = 3,410,783\n"
     ]
    }
   ],
   "source": [
    "# vocab_size, output_size=1, embedding_dim=32, hidden_dim=64, out_channels=32, drop_prob=0.5, vocab_vectors=None\n",
    "convRNN = SentimentConvNN(vocab_size=vocab_size, embedding_dim=embedding_dim, vocab_vectors=vocab.vectors)\n",
    "print(convRNN.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "thermal-dallas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0460, -0.1792, -0.1163,  ..., -0.0013, -0.0254, -0.0351],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you want to make a grad\n",
    "convRNN.embed.weight.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "sound-elder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convRNN.embed.weight.requires_grad, convRNN.dense.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "conditional-engineering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentConvNN(\n",
       "  (embed): Embedding(67359, 50)\n",
       "  (conv1d): Conv1d(200, 32, kernel_size=(3,), stride=(1,), bias=False)\n",
       "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (lstm): LSTM(24, 64, batch_first=True)\n",
       "  (dense): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (bn_embedding): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_conv1d): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_lstm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "convRNN.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "committed-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(net, criterion, optimizer, train_loader, valid_loader, clip_value=5, epochs=10, print_every=200, max_fscore=-np.inf):\n",
    "    '''\n",
    "    Train the network\n",
    "        net - network to trian\n",
    "        criterion - loss function \n",
    "        optimizer - your optimiser of choice \n",
    "        train_loader - loader for training data\n",
    "        vlid_loader - lodaer for validation/test data\n",
    "        clip_value - upper limit for gradient \n",
    "        epochs - number of epochs to train the net\n",
    "        print_every - prin stats every number of batches\n",
    "        max_fscore - best fscore on validation set - used in mutiple runs of training\n",
    "    '''\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "    \n",
    "    steps = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    # run over epochs of training\n",
    "    for e in trange(epochs):\n",
    "        \n",
    "        # array to keep value of losses over current epoch\n",
    "        train_loss = []\n",
    "\n",
    "        # run one pass through training samples = one epoch\n",
    "        for train_x, train_y in train_loader:\n",
    "            steps +=1\n",
    "\n",
    "            # zero out the grads \n",
    "            net.zero_grad()\n",
    "            # optimioptimizer.zero_grad()\n",
    "\n",
    "            # send data to device\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "\n",
    "            # initialize hidden state\n",
    "            h = net.init_hidden(len(train_x))\n",
    "\n",
    "            # calculate the output of the network\n",
    "            out, _ = net(train_x, h)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = criterion(out, train_y)\n",
    "            # backprop grads of the loss wrt to net parameters\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip_value)\n",
    "\n",
    "            # upadate parameters of network\n",
    "            optimizer.step()\n",
    "\n",
    "            # append current batch loss (loss per object in current batch)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            # test loss calc every \n",
    "            if steps%print_every == 0:\n",
    "                # calculate loss on test set\n",
    "                test_loss, metrics = val_score(net, valid_loader, criterion)\n",
    "                if metrics['fscore'] > max_fscore:\n",
    "                    max_fscore = metrics['fscore']\n",
    "                    message = '=)'\n",
    "                    check_point = {'vocab_size':net.vocab_size, \n",
    "                                   'embedding_dim': net.embedding_dim, \n",
    "                                   'hidden_dim':net.hidden_dim, \n",
    "                                   'n_layers':net.n_layers, \n",
    "                                   'net_params':net.state_dict()}\n",
    "                    torch.save(check_point, f\"spam_model_fscore_{metrics['fscore']:.3f}.pt\")\n",
    "                else:\n",
    "                    message = ';('\n",
    "                net.train()\n",
    "                print(f\"Step {steps} epoch {e+1}. {message}\\nTest loss is {test_loss:.4f}. Train loss is {np.mean(train_loss):.4f}.\\\n",
    "                F1 Score={metrics['fscore']:.2%} Precision={metrics['precision']:.2%} Recall={metrics['recall']:.2%} Accuracy={metrics['accuracy']:.2%}\\n\")\n",
    "    return max_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "comprehensive-turkish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "neutral-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr = 0.001\n",
    "criterion = nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(convRNN.parameters(), lr=lr, weight_decay=1e-2)\n",
    "#optimizer = torch.optim.SGD(convRNN.parameters(), lr=lr, momentum=0.9, nesterov=True, weight_decay=1e-2)\n",
    "#optimizer = torch.optim.RMSprop(convRNN.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "compound-architect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8638963019443385"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "abroad-listing",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249415a414f24f32ae57f33f7b5409f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 epoch 1. ;(\n",
      "Test loss is 0.6891. Train loss is 0.6993.                F1 Score=64.30% Precision=51.78% Recall=84.80% Accuracy=52.92%\n",
      "\n",
      "Step 400 epoch 1. ;(\n",
      "Test loss is 0.6878. Train loss is 0.6941.                F1 Score=34.81% Precision=60.24% Recall=24.48% Accuracy=54.16%\n",
      "\n",
      "Step 600 epoch 2. ;(\n",
      "Test loss is 0.6638. Train loss is 0.6787.                F1 Score=64.02% Precision=58.37% Recall=70.88% Accuracy=60.16%\n",
      "\n",
      "Step 800 epoch 2. ;(\n",
      "Test loss is 0.6365. Train loss is 0.6688.                F1 Score=61.05% Precision=67.21% Recall=55.92% Accuracy=64.32%\n",
      "\n",
      "Step 1000 epoch 3. ;(\n",
      "Test loss is 0.5935. Train loss is 0.6214.                F1 Score=65.12% Precision=73.35% Recall=58.56% Accuracy=68.64%\n",
      "\n",
      "Step 1200 epoch 3. ;(\n",
      "Test loss is 0.5561. Train loss is 0.6003.                F1 Score=69.35% Precision=73.65% Recall=65.52% Accuracy=71.04%\n",
      "\n",
      "Step 1400 epoch 4. ;(\n",
      "Test loss is 0.5144. Train loss is 0.5418.                F1 Score=74.96% Precision=76.46% Recall=73.52% Accuracy=75.44%\n",
      "\n",
      "Step 1600 epoch 4. ;(\n",
      "Test loss is 0.4939. Train loss is 0.5216.                F1 Score=76.11% Precision=74.09% Recall=78.24% Accuracy=75.44%\n",
      "\n",
      "Step 1800 epoch 5. ;(\n",
      "Test loss is 0.4603. Train loss is 0.4682.                F1 Score=78.18% Precision=78.12% Recall=78.24% Accuracy=78.16%\n",
      "\n",
      "Step 2000 epoch 5. ;(\n",
      "Test loss is 0.4477. Train loss is 0.4612.                F1 Score=79.50% Precision=77.94% Recall=81.12% Accuracy=79.08%\n",
      "\n",
      "Step 2200 epoch 6. ;(\n",
      "Test loss is 0.4343. Train loss is 0.4150.                F1 Score=79.40% Precision=82.19% Recall=76.80% Accuracy=80.08%\n",
      "\n",
      "Step 2400 epoch 6. ;(\n",
      "Test loss is 0.3989. Train loss is 0.4197.                F1 Score=82.74% Precision=79.64% Recall=86.08% Accuracy=82.04%\n",
      "\n",
      "Step 2600 epoch 7. ;(\n",
      "Test loss is 0.4130. Train loss is 0.3841.                F1 Score=81.95% Precision=75.96% Recall=88.96% Accuracy=80.40%\n",
      "\n",
      "Step 2800 epoch 7. ;(\n",
      "Test loss is 0.4009. Train loss is 0.3875.                F1 Score=81.65% Precision=82.52% Recall=80.80% Accuracy=81.84%\n",
      "\n",
      "Step 3000 epoch 8. ;(\n",
      "Test loss is 0.4053. Train loss is 0.3608.                F1 Score=81.74% Precision=81.64% Recall=81.84% Accuracy=81.72%\n",
      "\n",
      "Step 3200 epoch 8. ;(\n",
      "Test loss is 0.3909. Train loss is 0.3655.                F1 Score=82.99% Precision=83.84% Recall=82.16% Accuracy=83.16%\n",
      "\n",
      "Step 3400 epoch 9. ;(\n",
      "Test loss is 0.4085. Train loss is 0.3245.                F1 Score=81.68% Precision=80.14% Recall=83.28% Accuracy=81.32%\n",
      "\n",
      "Step 3600 epoch 9. ;(\n",
      "Test loss is 0.3978. Train loss is 0.3441.                F1 Score=82.34% Precision=83.01% Recall=81.68% Accuracy=82.48%\n",
      "\n",
      "Step 3800 epoch 10. ;(\n",
      "Test loss is 0.4027. Train loss is 0.2994.                F1 Score=82.71% Precision=80.42% Recall=85.12% Accuracy=82.20%\n",
      "\n",
      "Step 4000 epoch 10. ;(\n",
      "Test loss is 0.3994. Train loss is 0.3278.                F1 Score=81.87% Precision=82.64% Recall=81.12% Accuracy=82.04%\n",
      "\n",
      "Step 4200 epoch 11. ;(\n",
      "Test loss is 0.4064. Train loss is 0.2946.                F1 Score=81.56% Precision=83.61% Recall=79.60% Accuracy=82.00%\n",
      "\n",
      "Step 4400 epoch 11. ;(\n",
      "Test loss is 0.4072. Train loss is 0.3154.                F1 Score=81.10% Precision=84.94% Recall=77.60% Accuracy=81.92%\n",
      "\n",
      "Step 4600 epoch 12. ;(\n",
      "Test loss is 0.4068. Train loss is 0.2713.                F1 Score=82.95% Precision=83.01% Recall=82.88% Accuracy=82.96%\n",
      "\n",
      "Step 4800 epoch 12. ;(\n",
      "Test loss is 0.4010. Train loss is 0.3028.                F1 Score=82.97% Precision=81.59% Recall=84.40% Accuracy=82.68%\n",
      "\n",
      "Step 5000 epoch 13. ;(\n",
      "Test loss is 0.4300. Train loss is 0.2788.                F1 Score=81.17% Precision=82.72% Recall=79.68% Accuracy=81.52%\n",
      "\n",
      "Step 5200 epoch 13. ;(\n",
      "Test loss is 0.4081. Train loss is 0.2942.                F1 Score=81.98% Precision=79.83% Recall=84.24% Accuracy=81.48%\n",
      "\n",
      "Step 5400 epoch 14. ;(\n",
      "Test loss is 0.4344. Train loss is 0.2583.                F1 Score=82.04% Precision=82.23% Recall=81.84% Accuracy=82.08%\n",
      "\n",
      "Step 5600 epoch 14. ;(\n",
      "Test loss is 0.3962. Train loss is 0.2896.                F1 Score=82.21% Precision=84.10% Recall=80.40% Accuracy=82.60%\n",
      "\n",
      "Step 5800 epoch 15. ;(\n",
      "Test loss is 0.4456. Train loss is 0.2529.                F1 Score=80.54% Precision=80.13% Recall=80.96% Accuracy=80.44%\n",
      "\n",
      "Step 6000 epoch 15. ;(\n",
      "Test loss is 0.4164. Train loss is 0.2851.                F1 Score=80.65% Precision=84.43% Recall=77.20% Accuracy=81.48%\n",
      "\n",
      "Step 6200 epoch 16. ;(\n",
      "Test loss is 0.4357. Train loss is 0.2472.                F1 Score=81.02% Precision=81.98% Recall=80.08% Accuracy=81.24%\n",
      "\n",
      "Step 6400 epoch 16. ;(\n",
      "Test loss is 0.4255. Train loss is 0.2798.                F1 Score=81.46% Precision=81.72% Recall=81.20% Accuracy=81.52%\n",
      "\n",
      "Step 6600 epoch 17. ;(\n",
      "Test loss is 0.4243. Train loss is 0.2516.                F1 Score=81.83% Precision=82.23% Recall=81.44% Accuracy=81.92%\n",
      "\n",
      "Step 6800 epoch 17. ;(\n",
      "Test loss is 0.4072. Train loss is 0.2759.                F1 Score=80.08% Precision=84.73% Recall=75.92% Accuracy=81.12%\n",
      "\n",
      "Step 7000 epoch 18. ;(\n",
      "Test loss is 0.4265. Train loss is 0.2403.                F1 Score=82.83% Precision=81.92% Recall=83.76% Accuracy=82.64%\n",
      "\n",
      "Step 7200 epoch 18. ;(\n",
      "Test loss is 0.4091. Train loss is 0.2706.                F1 Score=81.67% Precision=82.64% Recall=80.72% Accuracy=81.88%\n",
      "\n",
      "Step 7400 epoch 19. ;(\n",
      "Test loss is 0.4340. Train loss is 0.2463.                F1 Score=81.84% Precision=80.29% Recall=83.44% Accuracy=81.48%\n",
      "\n",
      "Step 7600 epoch 19. ;(\n",
      "Test loss is 0.4150. Train loss is 0.2714.                F1 Score=81.95% Precision=82.72% Recall=81.20% Accuracy=82.12%\n",
      "\n",
      "Step 7800 epoch 20. ;(\n",
      "Test loss is 0.4423. Train loss is 0.2350.                F1 Score=81.42% Precision=81.00% Recall=81.84% Accuracy=81.32%\n",
      "\n",
      "Step 8000 epoch 20. ;(\n",
      "Test loss is 0.3999. Train loss is 0.2656.                F1 Score=81.69% Precision=82.36% Recall=81.04% Accuracy=81.84%\n",
      "\n",
      "Step 8200 epoch 21. ;(\n",
      "Test loss is 0.4554. Train loss is 0.2404.                F1 Score=81.57% Precision=79.85% Recall=83.36% Accuracy=81.16%\n",
      "\n",
      "Step 8400 epoch 21. ;(\n",
      "Test loss is 0.4173. Train loss is 0.2683.                F1 Score=80.77% Precision=82.49% Recall=79.12% Accuracy=81.16%\n",
      "\n",
      "Step 8600 epoch 22. ;(\n",
      "Test loss is 0.4525. Train loss is 0.2343.                F1 Score=80.74% Precision=82.59% Recall=78.96% Accuracy=81.16%\n",
      "\n",
      "Step 8800 epoch 22. ;(\n",
      "Test loss is 0.4009. Train loss is 0.2673.                F1 Score=82.65% Precision=82.59% Recall=82.72% Accuracy=82.64%\n",
      "\n",
      "Step 9000 epoch 23. ;(\n",
      "Test loss is 0.4619. Train loss is 0.2287.                F1 Score=79.49% Precision=79.94% Recall=79.04% Accuracy=79.60%\n",
      "\n",
      "Step 9200 epoch 23. ;(\n",
      "Test loss is 0.4215. Train loss is 0.2610.                F1 Score=81.91% Precision=81.42% Recall=82.40% Accuracy=81.80%\n",
      "\n",
      "Step 9400 epoch 24. ;(\n",
      "Test loss is 0.4254. Train loss is 0.2410.                F1 Score=82.22% Precision=83.09% Recall=81.36% Accuracy=82.40%\n",
      "\n",
      "Step 9600 epoch 24. ;(\n",
      "Test loss is 0.4173. Train loss is 0.2658.                F1 Score=82.62% Precision=80.05% Recall=85.36% Accuracy=82.04%\n",
      "\n",
      "Step 9800 epoch 25. ;(\n",
      "Test loss is 0.4507. Train loss is 0.2403.                F1 Score=81.50% Precision=81.08% Recall=81.92% Accuracy=81.40%\n",
      "\n",
      "Step 10000 epoch 25. ;(\n",
      "Test loss is 0.4222. Train loss is 0.2640.                F1 Score=81.03% Precision=83.22% Recall=78.96% Accuracy=81.52%\n",
      "\n",
      "Step 10200 epoch 26. ;(\n",
      "Test loss is 0.4728. Train loss is 0.2302.                F1 Score=80.13% Precision=81.77% Recall=78.56% Accuracy=80.52%\n",
      "\n",
      "Step 10400 epoch 26. ;(\n",
      "Test loss is 0.4060. Train loss is 0.2594.                F1 Score=81.23% Precision=81.99% Recall=80.48% Accuracy=81.40%\n",
      "\n",
      "Step 10600 epoch 27. ;(\n",
      "Test loss is 0.4900. Train loss is 0.2308.                F1 Score=78.97% Precision=82.36% Recall=75.84% Accuracy=79.80%\n",
      "\n",
      "Step 10800 epoch 27. ;(\n",
      "Test loss is 0.4611. Train loss is 0.2602.                F1 Score=80.38% Precision=78.74% Recall=82.08% Accuracy=79.96%\n",
      "\n",
      "Step 11000 epoch 28. ;(\n",
      "Test loss is 0.4469. Train loss is 0.2258.                F1 Score=82.10% Precision=82.20% Recall=82.00% Accuracy=82.12%\n",
      "\n",
      "Step 11200 epoch 28. ;(\n",
      "Test loss is 0.4551. Train loss is 0.2566.                F1 Score=80.08% Precision=79.45% Recall=80.72% Accuracy=79.92%\n",
      "\n",
      "Step 11400 epoch 29. ;(\n",
      "Test loss is 0.4673. Train loss is 0.2214.                F1 Score=80.82% Precision=82.86% Recall=78.88% Accuracy=81.28%\n",
      "\n",
      "Step 11600 epoch 29. ;(\n",
      "Test loss is 0.4481. Train loss is 0.2539.                F1 Score=81.16% Precision=80.81% Recall=81.52% Accuracy=81.08%\n",
      "\n",
      "Step 11800 epoch 30. ;(\n",
      "Test loss is 0.4551. Train loss is 0.2301.                F1 Score=81.60% Precision=81.77% Recall=81.44% Accuracy=81.64%\n",
      "\n",
      "Step 12000 epoch 30. ;(\n",
      "Test loss is 0.4258. Train loss is 0.2559.                F1 Score=80.36% Precision=85.75% Recall=75.60% Accuracy=81.52%\n",
      "\n",
      "Step 12200 epoch 31. ;(\n",
      "Test loss is 0.4607. Train loss is 0.2272.                F1 Score=81.18% Precision=81.97% Recall=80.40% Accuracy=81.36%\n",
      "\n",
      "Step 12400 epoch 31. ;(\n",
      "Test loss is 0.4254. Train loss is 0.2541.                F1 Score=82.56% Precision=81.63% Recall=83.52% Accuracy=82.36%\n",
      "\n",
      "Step 12600 epoch 32. ;(\n",
      "Test loss is 0.4425. Train loss is 0.2282.                F1 Score=81.29% Precision=81.23% Recall=81.36% Accuracy=81.28%\n",
      "\n",
      "Step 12800 epoch 32. ;(\n",
      "Test loss is 0.4059. Train loss is 0.2564.                F1 Score=82.81% Precision=82.58% Recall=83.04% Accuracy=82.76%\n",
      "\n",
      "Step 13000 epoch 33. ;(\n",
      "Test loss is 0.4524. Train loss is 0.2310.                F1 Score=80.71% Precision=83.07% Recall=78.48% Accuracy=81.24%\n",
      "\n",
      "Step 13200 epoch 33. ;(\n",
      "Test loss is 0.4322. Train loss is 0.2543.                F1 Score=81.42% Precision=78.43% Recall=84.64% Accuracy=80.68%\n",
      "\n",
      "Step 13400 epoch 34. ;(\n",
      "Test loss is 0.4475. Train loss is 0.2217.                F1 Score=81.79% Precision=83.14% Recall=80.48% Accuracy=82.08%\n",
      "\n",
      "Step 13600 epoch 34. ;(\n",
      "Test loss is 0.4252. Train loss is 0.2566.                F1 Score=81.70% Precision=81.80% Recall=81.60% Accuracy=81.72%\n",
      "\n",
      "Step 13800 epoch 35. ;(\n",
      "Test loss is 0.4771. Train loss is 0.2258.                F1 Score=79.10% Precision=84.73% Recall=74.16% Accuracy=80.40%\n",
      "\n",
      "Step 14000 epoch 35. ;(\n",
      "Test loss is 0.4132. Train loss is 0.2575.                F1 Score=82.07% Precision=80.89% Recall=83.28% Accuracy=81.80%\n",
      "\n",
      "Step 14200 epoch 36. ;(\n",
      "Test loss is 0.4456. Train loss is 0.2258.                F1 Score=81.16% Precision=81.52% Recall=80.80% Accuracy=81.24%\n",
      "\n",
      "Step 14400 epoch 36. ;(\n",
      "Test loss is 0.4394. Train loss is 0.2559.                F1 Score=81.24% Precision=79.43% Recall=83.12% Accuracy=80.80%\n",
      "\n",
      "Step 14600 epoch 37. ;(\n",
      "Test loss is 0.4586. Train loss is 0.2287.                F1 Score=79.60% Precision=79.60% Recall=79.60% Accuracy=79.60%\n",
      "\n",
      "Step 14800 epoch 37. ;(\n",
      "Test loss is 0.4085. Train loss is 0.2584.                F1 Score=81.26% Precision=81.17% Recall=81.36% Accuracy=81.24%\n",
      "\n",
      "Step 15000 epoch 38. ;(\n",
      "Test loss is 0.4596. Train loss is 0.2265.                F1 Score=82.93% Precision=81.37% Recall=84.56% Accuracy=82.60%\n",
      "\n",
      "Step 15200 epoch 38. ;(\n",
      "Test loss is 0.4333. Train loss is 0.2527.                F1 Score=81.11% Precision=82.59% Recall=79.68% Accuracy=81.44%\n",
      "\n",
      "Step 15400 epoch 39. ;(\n",
      "Test loss is 0.4767. Train loss is 0.2208.                F1 Score=79.66% Precision=81.04% Recall=78.32% Accuracy=80.00%\n",
      "\n",
      "Step 15600 epoch 39. ;(\n",
      "Test loss is 0.4462. Train loss is 0.2492.                F1 Score=79.38% Precision=81.41% Recall=77.44% Accuracy=79.88%\n",
      "\n",
      "Step 15800 epoch 40. ;(\n",
      "Test loss is 0.4638. Train loss is 0.2337.                F1 Score=82.32% Precision=78.15% Recall=86.96% Accuracy=81.32%\n",
      "\n",
      "Step 16000 epoch 40. ;(\n",
      "Test loss is 0.4032. Train loss is 0.2557.                F1 Score=82.99% Precision=81.54% Recall=84.48% Accuracy=82.68%\n",
      "\n",
      "Step 16200 epoch 41. ;(\n",
      "Test loss is 0.4306. Train loss is 0.2304.                F1 Score=82.93% Precision=80.00% Recall=86.08% Accuracy=82.28%\n",
      "\n",
      "Step 16400 epoch 41. ;(\n",
      "Test loss is 0.4107. Train loss is 0.2608.                F1 Score=81.62% Precision=80.17% Recall=83.12% Accuracy=81.28%\n",
      "\n",
      "Step 16600 epoch 42. ;(\n",
      "Test loss is 0.4693. Train loss is 0.2247.                F1 Score=79.77% Precision=82.60% Recall=77.12% Accuracy=80.44%\n",
      "\n",
      "Step 16800 epoch 42. ;(\n",
      "Test loss is 0.4279. Train loss is 0.2504.                F1 Score=81.55% Precision=81.25% Recall=81.84% Accuracy=81.48%\n",
      "\n",
      "Step 17000 epoch 43. ;(\n",
      "Test loss is 0.4623. Train loss is 0.2354.                F1 Score=82.51% Precision=78.95% Recall=86.40% Accuracy=81.68%\n",
      "\n",
      "Step 17200 epoch 43. ;(\n",
      "Test loss is 0.4236. Train loss is 0.2579.                F1 Score=80.66% Precision=82.87% Recall=78.56% Accuracy=81.16%\n",
      "\n",
      "Step 17400 epoch 44. ;(\n",
      "Test loss is 0.4399. Train loss is 0.2222.                F1 Score=80.65% Precision=81.81% Recall=79.52% Accuracy=80.92%\n",
      "\n",
      "Step 17600 epoch 44. ;(\n",
      "Test loss is 0.4362. Train loss is 0.2544.                F1 Score=81.47% Precision=82.41% Recall=80.56% Accuracy=81.68%\n",
      "\n",
      "Step 17800 epoch 45. ;(\n",
      "Test loss is 0.4690. Train loss is 0.2199.                F1 Score=80.59% Precision=82.02% Recall=79.20% Accuracy=80.92%\n",
      "\n",
      "Step 18000 epoch 45. ;(\n",
      "Test loss is 0.4305. Train loss is 0.2515.                F1 Score=80.65% Precision=81.30% Recall=80.00% Accuracy=80.80%\n",
      "\n",
      "Step 18200 epoch 46. ;(\n",
      "Test loss is 0.4595. Train loss is 0.2233.                F1 Score=81.41% Precision=81.71% Recall=81.12% Accuracy=81.48%\n",
      "\n",
      "Step 18400 epoch 46. ;(\n",
      "Test loss is 0.4312. Train loss is 0.2523.                F1 Score=80.83% Precision=80.54% Recall=81.12% Accuracy=80.76%\n",
      "\n",
      "Step 18600 epoch 47. ;(\n",
      "Test loss is 0.4504. Train loss is 0.2351.                F1 Score=81.88% Precision=78.87% Recall=85.12% Accuracy=81.16%\n",
      "\n",
      "Step 18800 epoch 47. ;(\n",
      "Test loss is 0.4169. Train loss is 0.2592.                F1 Score=82.59% Precision=81.00% Recall=84.24% Accuracy=82.24%\n",
      "\n",
      "Step 19000 epoch 48. ;(\n",
      "Test loss is 0.4498. Train loss is 0.2246.                F1 Score=80.90% Precision=80.83% Recall=80.96% Accuracy=80.88%\n",
      "\n",
      "Step 19200 epoch 48. ;(\n",
      "Test loss is 0.4284. Train loss is 0.2558.                F1 Score=82.22% Precision=81.04% Recall=83.44% Accuracy=81.96%\n",
      "\n",
      "Step 19400 epoch 49. ;(\n",
      "Test loss is 0.4759. Train loss is 0.2200.                F1 Score=80.40% Precision=84.16% Recall=76.96% Accuracy=81.24%\n",
      "\n",
      "Step 19600 epoch 49. ;(\n",
      "Test loss is 0.4405. Train loss is 0.2499.                F1 Score=80.77% Precision=84.78% Recall=77.12% Accuracy=81.64%\n",
      "\n",
      "Step 19800 epoch 50. ;(\n",
      "Test loss is 0.4874. Train loss is 0.2303.                F1 Score=81.29% Precision=80.06% Recall=82.56% Accuracy=81.00%\n",
      "\n",
      "Step 20000 epoch 50. ;(\n",
      "Test loss is 0.4225. Train loss is 0.2586.                F1 Score=81.12% Precision=81.12% Recall=81.12% Accuracy=81.12%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_fscore = trainer(convRNN, criterion, optimizer, train_loader, valid_loader, clip_value=10, epochs=50, print_every=200, max_fscore=max_fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-insulation",
   "metadata": {},
   "source": [
    "## CONV RNN with pretrained embedding gives no gain in metrics - you must make embedding trainable otherwise the results are horrible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-forest",
   "metadata": {},
   "source": [
    "---\n",
    "### Manual conv network assembly\n",
    "---\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length,input_length=max_review_length)) \n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2)) \n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "endless-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "conv1d_sentiment = nn.Conv1d(in_channels=200, out_channels=32, kernel_size=3, bias=False, padding=False)\n",
    "maxpool_sentiment = nn.MaxPool1d(kernel_size=2)\n",
    "lstm_sentiment = nn.LSTM(input_size=15,\n",
    "                         hidden_size=hidden_dim,\n",
    "                         num_layers=1,\n",
    "                         batch_first=True,\n",
    "                         dropout=0)\n",
    "lstm_h0 = (torch.zeros(1,2,hidden_dim), torch.zeros(1,2,hidden_dim))\n",
    "dense_sentiment = nn.Linear(hidden_dim, 1)\n",
    "drop_sentiment = nn.Dropout(p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "bearing-screw",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 200, 3])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_sentiment.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "cultural-shanghai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape \ttorch.Size([2, 200])\n",
      "Embedding shape torch.Size([2, 200, 32])\n",
      "Conv1d shape \ttorch.Size([2, 32, 30])\n",
      "MaxPool shape \ttorch.Size([2, 32, 15])\n",
      "LSTM shape \ttorch.Size([2, 32, 64])\n",
      "Dense shape \ttorch.Size([2, 32, 1])\n",
      "Sigmoid shape \ttorch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embed_out = embed(txt)\n",
    "    embed_out = drop_sentiment(embed_out)\n",
    "    conv_out = conv1d_sentiment(embed_out)\n",
    "    conv_out_relu = F.relu(conv_out)\n",
    "    maxpool_out = maxpool_sentiment(conv_out_relu)\n",
    "    lstm_out, _ = lstm_sentiment(maxpool_out, lstm_h0)\n",
    "    lstm_out = drop_sentiment(lstm_out)\n",
    "    out_dense = dense_sentiment(lstm_out)\n",
    "    out = nn.Sigmoid()(out_dense[:,-1,:]).view(out_dense.shape[0])\n",
    "# Input:  (N, Cin,  Lin)\n",
    "# Output: (N, Cout, Lout)\n",
    "print(f'Input shape \\t{txt.shape}\\nEmbedding shape {embed_out.shape}\\nConv1d shape \\t{conv_out.shape}\\\n",
    "\\nMaxPool shape \\t{maxpool_out.shape}\\nLSTM shape \\t{lstm_out.shape}\\nDense shape \\t{out_dense.shape}\\nSigmoid shape \\t{out.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-venice",
   "metadata": {},
   "source": [
    "---\n",
    "### END:Manual conv network assembly\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-operator",
   "metadata": {},
   "source": [
    "# Conv1d\n",
    "in the simplest case, the output value of the layer with input size (N,Cin,L) and output (N,Cout,Lout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "infectious-probe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  2.],\n",
       "          [ 3.,  4.,  5.]],\n",
       " \n",
       "         [[ 6.,  7.,  8.],\n",
       "          [ 9., 10., 11.]],\n",
       " \n",
       "         [[12., 13., 14.],\n",
       "          [15., 16., 17.]]]),\n",
       " torch.Size([3, 2, 3]))"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_1d_input_temp = torch.arange(18).to(torch.float).view(3,2,-1)\n",
    "conv_1d_input_temp, conv_1d_input_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "bridal-raising",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0],\n",
       "          [1]],\n",
       " \n",
       "         [[2],\n",
       "          [3]],\n",
       " \n",
       "         [[4],\n",
       "          [5]]]),\n",
       " torch.Size([3, 2, 1]))"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra = torch.arange(6).view((3,2,1))\n",
    "extra, extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "numerous-explorer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  2.,  0.],\n",
       "          [ 3.,  4.,  5.,  1.]],\n",
       " \n",
       "         [[ 6.,  7.,  8.,  2.],\n",
       "          [ 9., 10., 11.,  3.]],\n",
       " \n",
       "         [[12., 13., 14.,  4.],\n",
       "          [15., 16., 17.,  5.]]]),\n",
       " torch.Size([3, 2, 4]))"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_1d_input = torch.cat((conv_1d_input_temp, extra), dim = 2)\n",
    "conv_1d_input, conv_1d_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "broadband-advocate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1.],\n",
       "          [1., 1.]],\n",
       " \n",
       "         [[2., 2.],\n",
       "          [2., 2.]],\n",
       " \n",
       "         [[1., 1.],\n",
       "          [1., 1.]],\n",
       " \n",
       "         [[1., 1.],\n",
       "          [1., 1.]]]),\n",
       " torch.Size([4, 2, 2]))"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weight(m):\n",
    "    from functools import reduce\n",
    "    l = reduce(lambda x,y: x*y, m.weight.data.shape)\n",
    "    if type(m) == nn.Conv1d:\n",
    "        m.weight.data = torch.ones(l).to(torch.float).reshape(m.weight.data.shape)\n",
    "        m.weight.data[1] += m.weight.data[1]\n",
    "\n",
    "layer_conv1d = nn.Conv1d(in_channels=2, out_channels=4, kernel_size=2, bias=False)\n",
    "layer_conv1d.apply(init_weight)\n",
    "\n",
    "layer_conv1d.weight.data, layer_conv1d.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "emotional-membrane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  8.,  12.,   8.],\n",
       "         [ 16.,  24.,  16.],\n",
       "         [  8.,  12.,   8.],\n",
       "         [  8.,  12.,   8.]],\n",
       "\n",
       "        [[ 32.,  36.,  24.],\n",
       "         [ 64.,  72.,  48.],\n",
       "         [ 32.,  36.,  24.],\n",
       "         [ 32.,  36.,  24.]],\n",
       "\n",
       "        [[ 56.,  60.,  40.],\n",
       "         [112., 120.,  80.],\n",
       "         [ 56.,  60.,  40.],\n",
       "         [ 56.,  60.,  40.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_1d_out = layer_conv1d(conv_1d_input)\n",
    "\n",
    "conv_1d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-rental",
   "metadata": {},
   "source": [
    "##  Convolution of kernel size 1\n",
    "it is equal to linear layer for point-wise transformation in transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "rolled-climate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.],\n",
       "          [1.],\n",
       "          [1.]],\n",
       " \n",
       "         [[2.],\n",
       "          [2.],\n",
       "          [2.]],\n",
       " \n",
       "         [[1.],\n",
       "          [1.],\n",
       "          [1.]],\n",
       " \n",
       "         [[1.],\n",
       "          [1.],\n",
       "          [1.]]]),\n",
       " torch.Size([4, 3, 1]))"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv1d = nn.Conv1d(in_channels=3, out_channels=4, kernel_size=1, bias=False)\n",
    "layer_conv1d.apply(init_weight)\n",
    "\n",
    "layer_conv1d.weight.data, layer_conv1d.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "significant-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.],\n",
       "          [1.],\n",
       "          [2.]]]),\n",
       " torch.Size([1, 3, 1]))"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel1input = torch.arange(3).to(torch.float).view(1,3,-1)\n",
    "kernel1input, kernel1input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "dirty-metallic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.],\n",
       "         [6.],\n",
       "         [3.],\n",
       "         [3.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv1d(kernel1input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
