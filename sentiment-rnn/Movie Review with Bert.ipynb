{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "respective-nerve",
   "metadata": {},
   "source": [
    "## Let's make a **convolution** work on MOVIE REVIEW CLASSIFICATION and utilize TORCHTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bulgarian-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext as TT\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developmental-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "constant-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/reviews.txt','r') as f:\n",
    "    data = f.readlines()\n",
    "with open('./data/labels.txt', 'r') as f:\n",
    "    labels = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "floppy-germany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# optional - remove punctuation\n",
    "from string import punctuation\n",
    "print(punctuation)\n",
    "\n",
    "# get rid of punctuation\n",
    "all_data = []\n",
    "for review in data:\n",
    "    all_data.append(\"\".join(c for c in review if c not in punctuation))\n",
    "del data\n",
    "data = all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "collaborative-tucson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film is a spicy little piece of film  making from sam fuller which gives richard widmark the chance to show of some of his best  most edgy acting in the role of skip mccoy  a small  time thief who stumbles onto a military secret while picking beautiful candy  s  jean peters  pocket on a crowded bus . it turns out candy was doing a favor for her  ex   boyfriend  who  s working for the  commies  .  br    br   superficially  there  s a mystery here regarding candy  s motives and skip spends much of the film determining her motives . actually he seems to just initially assume that she  s a  commie   going so far as to pour beer in her face in a callous gesture . but the real question is  what  s going on with skip  what are his motives  and why does candy like him so much  why do we  the audience  want to like him so much  basically what the film  makers have done here is create a very striking  male fatale  in widmark  s character and his performance . just as the male audience tends to ponder through the length of a film like  the big sleep  or  the glass key  along with the main characters whether the female character is trustworthy or just a pretty face  the film  makers have here created a similar quandary for female viewers . widmark is handsome  and there  s also a charm in his boyish insouciance  but the first two times he meets our leading lady  he robs her and then punches her in the face . eventually the question becomes  would skip sink so low as to sell out his country for a buck  his comments to the police  like  you  re waving a flag at me   make us suspect he would  or is he simply out for revenge for the murder of his friend moe  thelma ritter   i  m not sure that the film gives us a conclusive answer either way .  br    br   thelma ritter  s character work deserves special mention  she has created a truly indelible character here . fuller isn  t afraid to give her plenty of  business   in the form of physical objects that she uses to draw the audience into her world  particularly her used ties . another example of fuller  s  business  would be the scene with victor perry  an actor i  ve seen elsewhere used to less effect  using the chopsticks to intimidate candy .  br    br   the emphasis on moe  s relationship with skip provides one of cinema  s most revealing  honor among thieves  themes . in fact skip has the same kind of ease and the same kind of casual relationship with the police  with the notable exception of capt . tiger  murvyn vye  who has a grudge against him . i loved the scene where he invited the cops in by name and offered them a beer when they came to pick him up at his shack . those are the kind of details that make this film feel real  whether or not it really is  realistic  or whether that would matter are entirely separate questions .  br    br   all told  i would say this is an essential crime film which displays a lot of the best and most durable attributes of the  film noir  school of film  making . a predictable plot is off  set by a host of colorful characters  uniformly well  performed   cheap sets are disguised by the film  s unrelenting pace  and the final product feels a lot more substantial than it probably is . this is the best film i  ve seen so far by sam fuller and helps me to see better why he  s regarded as a master director  here he accomplished some things that i think he tried but ultimately failed to do in other films like  the crimson kimono  and  shock corridor  as far as very emphatic acting styles and really gripping suspense . this is one of my favorite performances from widmark that i  ve seen so far  and widmark was a talent that i  m tempted to say  based on the few extraordinary films i  ve seen with him  was comparable to that of alan ladd or humphrey bogart  although arguably he didn  t make as many classic films .  \n",
      " positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(low=1,high=len(data))\n",
    "print(data[i], labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "victorian-maria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "experimental-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_length = DataFrame([len(item.split()) for item in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "handy-investing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>253.895520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>186.927241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>139.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>309.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2633.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "count  25000.000000\n",
       "mean     253.895520\n",
       "std      186.927241\n",
       "min       11.000000\n",
       "25%      139.000000\n",
       "50%      190.000000\n",
       "75%      309.000000\n",
       "max     2633.000000"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "olive-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-rolling",
   "metadata": {},
   "source": [
    "###  Define Dataset and DataLoader along with Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "provincial-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "sophisticated-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    very basic dataset for processing text data\n",
    "    holds text and label\n",
    "    implements len and getitem methods\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        labels,\n",
    "        text, \n",
    "        label_dict = None,\n",
    "        max_seq_length = 200, \n",
    "        model_name=\"distilbert-base-uncased\"\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Constructor for the text dataset&\n",
    "        Args:\n",
    "        labels - list with labels\n",
    "        text - a list with texts to classify\n",
    "        label_dict - a dictionary to map label into a class id\n",
    "        max_seq_length - max length of a single text in tokens, text is truncated or padded to equal max_seq_length\n",
    "        model_name - specific model name from transformers library\n",
    "        \"\"\" \n",
    "        if labels is not None:\n",
    "            assert len(text) == len(labels)\n",
    "        super(TextData, self).__init__()\n",
    "        self.labels = labels\n",
    "        self.text = text\n",
    "        self.label_dict = label_dict\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        if self.label_dict is None and labels is not None:\n",
    "            # auto-encode labels to 0,1,2,... lebel_ids\n",
    "            self.label_dict = dict(zip(sorted(set(labels)), range(len(set(labels)))))\n",
    "        \n",
    "        # init the tokenizer\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.tokenizer.padding_side = 'left'\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        txt = self.text[index]\n",
    "        \n",
    "        # a dictionary with `input_ids` and `attention_mask` as keys\n",
    "        output_dict = self.tokenizer.encode_plus( \n",
    "            txt, \n",
    "            add_special_tokens=True, \n",
    "            padding='max_length',\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        output_dict['input_ids'] = output_dict['input_ids'].squeeze()\n",
    "        output_dict['attention_mask'] = output_dict['attention_mask'].squeeze()\n",
    "        \n",
    "        # target\n",
    "        if self.labels is not None:\n",
    "            y = self.labels[index]\n",
    "            y_encoded = torch.Tensor([self.label_dict.get(y, -1)]).long().squeeze()\n",
    "            output_dict[\"targets\"] = y_encoded\n",
    "        \n",
    "        return output_dict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-intervention",
   "metadata": {},
   "source": [
    "#### Try out the class/object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "mysterious-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_data = TextData(labels, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "spiritual-visit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased'\n",
      " vocab_size=30522\n",
      " model_max_len=512\n",
      " is_fast=True\n",
      " padding_side='left'\n",
      " special_tokens={'unk_token': '[UNK]'\n",
      " 'sep_token': '[SEP]'\n",
      " 'pad_token': '[PAD]'\n",
      " 'cls_token': '[CLS]'\n",
      " 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "# see the parameters of our tokenizer\n",
    "print(*txt_data.tokenizer.__repr__().split(\",\"), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-briefing",
   "metadata": {},
   "source": [
    "#### Take a look at a sample text - how it gets encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ruled-reply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contrary to my principles  let me first come up with a conclusion  because i have just seen this piece of  art   and still am under strong impressions . the reader is asked to excuse my stronger vocabulary .  br    br   well  this movie is absolutely horrible  and i would never bother to write a single word about it  if it were not for the fact that    minutes  made me sick to death  which rarely happens to me . the fact that i paid for that does not exactly makes me feel better  as well as the fact the movie deserved the high user rating here .  br    br   so what is wrong with the movie  it has a fashionable title     minutes  . one first thinks about    minutes   which is by the way a much better movie  but still bad in my book  and indeed the two can be compared to some extent . but  as luck would have it  the things they share are their worst characteristics . they both feature mr . oleg taktarov  who with his strong russian accent obviously meets the popular expectations and prejudices . his purpose is to appeal to the cold war mind . ah  do we miss the good old times . now  i don  t imply that he is a bad actor  i am yet to judge his true performance  but he is simply not a true individual here  he is more like an archetype . how anyone can still indulge in such things is completely beyond my comprehension . we can recognize modern american xenophobia here . the point in the movie when taktarov explains to his companion that romanians are not germans  and that they are in america is truly laughable . are we to assume that the greatest desire of the wretched duo is to become  true  americans   br    br   then  there is the media issue . yes  it seems that the most of what we learn comes from cameras  interviews and reporters . the director should have made us feel the rhythm of the presumed   minutes . instead he bores us with interviews throughout the movie like in a cheap tv show  trying to reinvent the wheel . in   minutes the issue of media is the central one . the point is presented in a way a teacher addresses an obtuse student  but that deserves a separate comment  we are focusing on   minutes now . so  i have been trying to identify the purpose of this movie . what is it  to provide good time for the audience  to glorify weapons  to glorify police  portray violence  oh yes  the officer gives the bible to the underage delinquent . so it must promote peace and understanding after all  i don  t think so  but don  t ask me . i only know i didn  t enjoy any of this .  br    br   ah  michael madsen . i admit  i am a big fan . i hoped he would be a bright point  but i was wrong . it  s not his fault though .  br    br   as the final note  comparing  firepower  to  willpower  at the end of the movie was one of the worst lines i have ever heard .  br    br   to summarize  on the scale      i give it a pure  unadulterated  .  \n",
      "\n",
      "negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnd_review = np.random.randint(0,len(data))\n",
    "print(data[rnd_review], labels[rnd_review], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "possible-berlin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 10043,  2000,  2026,  6481,  2292,  2033,  2034,  2272,  2039,\n",
       "         2007,  1037,  7091,  2138,  1045,  2031,  2074,  2464,  2023,  3538,\n",
       "         1997,  2396,  1998,  2145,  2572,  2104,  2844, 19221,  1012,  1996,\n",
       "         8068,  2003,  2356,  2000,  8016,  2026,  6428, 16188,  1012,  7987,\n",
       "         7987,  2092,  2023,  3185,  2003,  7078,  9202,  1998,  1045,  2052,\n",
       "         2196,  8572,  2000,  4339,  1037,  2309,  2773,  2055,  2009,  2065,\n",
       "         2009,  2020,  2025,  2005,  1996,  2755,  2008,  2781,  2081,  2033,\n",
       "         5305,  2000,  2331,  2029,  6524,  6433,  2000,  2033,  1012,  1996,\n",
       "         2755,  2008,  1045,  3825,  2005,  2008,  2515,  2025,  3599,  3084,\n",
       "         2033,  2514,  2488,  2004,  2092,  2004,  1996,  2755,  1996,  3185,\n",
       "        10849,  1996,  2152,  5310,  5790,  2182,  1012,  7987,  7987,  2061,\n",
       "         2054,  2003,  3308,  2007,  1996,  3185,  2009,  2038,  1037, 19964,\n",
       "         2516,  2781,  1012,  2028,  2034,  6732,  2055,  2781,  2029,  2003,\n",
       "         2011,  1996,  2126,  1037,  2172,  2488,  3185,  2021,  2145,  2919,\n",
       "         1999,  2026,  2338,  1998,  5262,  1996,  2048,  2064,  2022,  4102,\n",
       "         2000,  2070,  6698,  1012,  2021,  2004,  6735,  2052,  2031,  2009,\n",
       "         1996,  2477,  2027,  3745,  2024,  2037,  5409,  6459,  1012,  2027,\n",
       "         2119,  3444,  2720,  1012, 25841, 27006, 28160,  2615,  2040,  2007,\n",
       "         2010,  2844,  2845,  9669,  5525,  6010,  1996,  2759, 10908,  1998,\n",
       "        18024,  2015,  1012,  2010,  3800,  2003,  2000,  5574,  2000,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1]), 'targets': tensor(0)}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_data[rnd_review]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-merit",
   "metadata": {},
   "source": [
    "#### Vocabulary from tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "aware-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = txt_data.tokenizer.get_vocab()\n",
    "reverse_vocab = dict(sorted((value,key) for (key,value) in vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "satisfied-radio",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dried': 9550,\n",
       " '##icus': 14239,\n",
       " '##escu': 19434,\n",
       " 'dylan': 7758,\n",
       " 'need': 2342,\n",
       " 'thanking': 28638,\n",
       " 'prizes': 11580,\n",
       " 'orson': 25026,\n",
       " '##llar': 17305,\n",
       " 'cara': 14418,\n",
       " 'fathers': 11397,\n",
       " 'propelled': 15801,\n",
       " 'frescoes': 23360,\n",
       " 'breathless': 16701,\n",
       " 'turns': 4332,\n",
       " 'pay': 3477,\n",
       " 'awe': 15180,\n",
       " '1721': 27689,\n",
       " 'colonel': 4327,\n",
       " 'baggage': 20220,\n",
       " 'crosby': 14282,\n",
       " 'decreased': 10548,\n",
       " 'jaya': 24120,\n",
       " 'prevailed': 19914,\n",
       " 'appointment': 6098,\n",
       " 'regulations': 7040,\n",
       " 'includes': 2950,\n",
       " 'drawer': 13065,\n",
       " 'famed': 15607,\n",
       " 'gable': 13733,\n",
       " 'danzig': 26669,\n",
       " '##arus': 29133,\n",
       " 'consisting': 5398,\n",
       " '[unused356]': 361,\n",
       " 'located': 2284,\n",
       " 'every': 2296,\n",
       " 'stadium': 3346,\n",
       " 'arrived': 3369,\n",
       " 'gazed': 11114,\n",
       " '##cise': 18380,\n",
       " 'fashionable': 19964,\n",
       " 'ornate': 18099,\n",
       " 'custer': 28888,\n",
       " 'covent': 29456,\n",
       " 'revolves': 19223,\n",
       " '扌': 1859,\n",
       " 'once': 2320,\n",
       " 'denis': 11064,\n",
       " 'vulture': 27588,\n",
       " 'churches': 5231,\n",
       " 'ultimately': 4821,\n",
       " 'songwriting': 14029,\n",
       " 'singer': 3220,\n",
       " 'annoying': 15703,\n",
       " 'vase': 18781,\n",
       " 'autopsy': 24534,\n",
       " '[unused851]': 856,\n",
       " 'thoughtfully': 19897,\n",
       " '##lika': 25421,\n",
       " 'attire': 20426,\n",
       " '##dington': 21504,\n",
       " 'parker': 6262,\n",
       " 'propeller': 15692,\n",
       " '224': 19711,\n",
       " '##ele': 12260,\n",
       " 'salmon': 11840,\n",
       " 'relic': 24933,\n",
       " '[unused270]': 275,\n",
       " 'opposing': 10078,\n",
       " 'catholicism': 16138,\n",
       " 'would': 2052,\n",
       " 'cleopatra': 22003,\n",
       " 'myriad': 25028,\n",
       " 'tidy': 29369,\n",
       " 'frightening': 17115,\n",
       " 'treat': 7438,\n",
       " '##ndra': 17670,\n",
       " 'permission': 6656,\n",
       " 'syndicate': 16229,\n",
       " '⅔': 1582,\n",
       " 'yanked': 10963,\n",
       " 'finding': 4531,\n",
       " '##for': 29278,\n",
       " '##aling': 21682,\n",
       " '##aire': 14737,\n",
       " 'braden': 27232,\n",
       " 'perimeter': 13443,\n",
       " 'route': 2799,\n",
       " '1911': 5184,\n",
       " 'written': 2517,\n",
       " 'expected': 3517,\n",
       " 'valuable': 7070,\n",
       " 'estadio': 14143,\n",
       " 'hiss': 19074,\n",
       " 'tattoo': 11660,\n",
       " '##リ': 30258,\n",
       " '[unused406]': 411,\n",
       " 'barge': 19398,\n",
       " 'ruby': 10090,\n",
       " 'opened': 2441,\n",
       " 'cubic': 11919,\n",
       " '##宣': 30349,\n",
       " 'kelly': 5163,\n",
       " '##lga': 27887,\n",
       " 'town': 2237,\n",
       " 'neighborhood': 5101,\n",
       " 'acute': 11325,\n",
       " 'destroyer': 9799,\n",
       " 'bugs': 12883,\n",
       " 'eliminating': 15349,\n",
       " 'venues': 9356,\n",
       " 'ᄋ': 1463,\n",
       " '##oi': 10448,\n",
       " 'stillness': 29435,\n",
       " 'improved': 5301,\n",
       " '##´s': 21932,\n",
       " 'erected': 7019,\n",
       " 'enhancement': 22415,\n",
       " 'wilder': 18463,\n",
       " 'salle': 18005,\n",
       " 'quarter': 4284,\n",
       " 'pumpkin': 25730,\n",
       " 'rm': 28549,\n",
       " 'starter': 11753,\n",
       " 'assassinated': 16370,\n",
       " 'shocking': 16880,\n",
       " 'parana': 28383,\n",
       " 'dahl': 27934,\n",
       " 'lockheed': 17646,\n",
       " '[unused160]': 165,\n",
       " 'mermaid': 22322,\n",
       " 'wealthiest': 27809,\n",
       " '##yu': 10513,\n",
       " 'cloth': 8416,\n",
       " '1812': 9842,\n",
       " 'conservatives': 11992,\n",
       " 'bremen': 16314,\n",
       " '##vier': 14356,\n",
       " 'ethel': 19180,\n",
       " 'awhile': 19511,\n",
       " '##nin': 11483,\n",
       " 'administratively': 23710,\n",
       " 'armor': 8177,\n",
       " 'investigates': 28062,\n",
       " '##urities': 29366,\n",
       " '##ball': 7384,\n",
       " 'park': 2380,\n",
       " '##nies': 15580,\n",
       " 'topography': 21535,\n",
       " 'ethnic': 5636,\n",
       " 'silver': 3165,\n",
       " '##+': 29622,\n",
       " 'surveying': 19654,\n",
       " 'los': 3050,\n",
       " 'photo': 6302,\n",
       " 'parking': 5581,\n",
       " '##₈': 30084,\n",
       " '##fr': 19699,\n",
       " 'probe': 15113,\n",
       " 'killings': 16431,\n",
       " '##lism': 28235,\n",
       " 'fulfillment': 29362,\n",
       " 'babe': 11561,\n",
       " 'marianne': 19887,\n",
       " '1744': 25846,\n",
       " '12': 2260,\n",
       " '##iled': 18450,\n",
       " 'clair': 17936,\n",
       " 'drumming': 22980,\n",
       " 'csi': 22174,\n",
       " 'haven': 4033,\n",
       " 'astronomers': 26357,\n",
       " '##ক': 29889,\n",
       " '##ᆫ': 30021,\n",
       " 'referendum': 9782,\n",
       " 'iris': 11173,\n",
       " '##iving': 14966,\n",
       " 'simulator': 25837,\n",
       " 'walled': 17692,\n",
       " '##ม': 29951,\n",
       " 'meadows': 13524,\n",
       " 'kane': 8472,\n",
       " 'ticked': 26019,\n",
       " '##erman': 18689,\n",
       " 'spouse': 18591,\n",
       " 'jurassic': 19996,\n",
       " 'bob': 3960,\n",
       " 'aluminium': 14794,\n",
       " '##rder': 26764,\n",
       " 'org': 8917,\n",
       " 'jericho': 17309,\n",
       " 'disgusting': 19424,\n",
       " '##kia': 21128,\n",
       " 'faithful': 11633,\n",
       " 'stein': 14233,\n",
       " 'kamal': 21911,\n",
       " '##keepers': 24764,\n",
       " '[unused837]': 842,\n",
       " '[unused386]': 391,\n",
       " '##kian': 28545,\n",
       " 'yale': 7996,\n",
       " 'limo': 23338,\n",
       " '1708': 27337,\n",
       " 'divisional': 14167,\n",
       " '##rley': 12866,\n",
       " 'bribery': 27748,\n",
       " '##sc': 11020,\n",
       " '##fication': 10803,\n",
       " '##lberg': 22927,\n",
       " '##uel': 16284,\n",
       " 'fucking': 8239,\n",
       " 'hack': 20578,\n",
       " '##dra': 7265,\n",
       " 'asked': 2356,\n",
       " 'commons': 7674,\n",
       " 'primarily': 3952,\n",
       " 'wholesale': 17264,\n",
       " 'po': 13433,\n",
       " '##irus': 26013,\n",
       " 'throw': 5466,\n",
       " 'holmes': 9106,\n",
       " '##some': 14045,\n",
       " 'accompanied': 5642,\n",
       " '¹⁄₂': 18728,\n",
       " '##ons': 5644,\n",
       " '##dents': 28986,\n",
       " 'fairbanks': 24502,\n",
       " 'spirited': 24462,\n",
       " 'nbl': 28013,\n",
       " 'paces': 24785,\n",
       " '##hall': 9892,\n",
       " '京': 1755,\n",
       " 'schwartz': 16756,\n",
       " 'older': 3080,\n",
       " 'repay': 24565,\n",
       " 'pigs': 14695,\n",
       " '##ening': 7406,\n",
       " '##ulu': 20391,\n",
       " 'eats': 20323,\n",
       " '[unused194]': 199,\n",
       " 'demons': 7942,\n",
       " 'ھ': 1307,\n",
       " 'manly': 19385,\n",
       " 'surveillance': 9867,\n",
       " 'excerpt': 28142,\n",
       " 'auditor': 20964,\n",
       " '##tish': 24788,\n",
       " 'noble': 7015,\n",
       " '##e': 2063,\n",
       " 'prolific': 12807,\n",
       " 'stanza': 29509,\n",
       " '##tou': 24826,\n",
       " 'forested': 15205,\n",
       " 'flair': 22012,\n",
       " '：': 1993,\n",
       " 'exists': 6526,\n",
       " 'tam': 17214,\n",
       " 'roar': 11950,\n",
       " 'stevenson': 13636,\n",
       " '430': 19540,\n",
       " 'christians': 8135,\n",
       " 'clans': 16411,\n",
       " 'fritz': 12880,\n",
       " '止': 1887,\n",
       " 'uc': 15384,\n",
       " '##vna': 29207,\n",
       " 'vinegar': 29387,\n",
       " 'joyah': 27098,\n",
       " 'æ': 1097,\n",
       " 'dancers': 10487,\n",
       " 'fletcher': 10589,\n",
       " 'commissioners': 12396,\n",
       " '##藤': 30470,\n",
       " 'returns': 5651,\n",
       " '##nished': 28357,\n",
       " 'squid': 26852,\n",
       " 'simulations': 24710,\n",
       " '##œ': 29674,\n",
       " 'interested': 4699,\n",
       " '[unused31]': 32,\n",
       " 'locking': 14889,\n",
       " 'defect': 21262,\n",
       " 'advanced': 3935,\n",
       " 'stade': 15649,\n",
       " '##dder': 20791,\n",
       " 'monday': 6928,\n",
       " 'fare': 13258,\n",
       " '##phus': 25255,\n",
       " 'vanish': 25887,\n",
       " 'nadia': 14942,\n",
       " 'bubbles': 17255,\n",
       " 'impressed': 7622,\n",
       " '佐': 1764,\n",
       " 'accusations': 13519,\n",
       " 'brief': 4766,\n",
       " 'broncos': 14169,\n",
       " 'jesse': 7627,\n",
       " '105': 8746,\n",
       " 'sienna': 20210,\n",
       " '149': 17332,\n",
       " '##nist': 26942,\n",
       " 'director': 2472,\n",
       " '##ல': 29928,\n",
       " 'lester': 14131,\n",
       " 'predecessors': 16372,\n",
       " 'interim': 9455,\n",
       " 'governed': 9950,\n",
       " '##well': 4381,\n",
       " 'travelled': 7837,\n",
       " 'presidents': 11274,\n",
       " 'inmates': 13187,\n",
       " 'succeeding': 13034,\n",
       " 'ymca': 26866,\n",
       " '[unused325]': 330,\n",
       " 'chester': 8812,\n",
       " '[unused868]': 873,\n",
       " 'ski': 8301,\n",
       " 'chrysler': 17714,\n",
       " 'mcgrath': 23220,\n",
       " '##oxide': 28479,\n",
       " '##lices': 29146,\n",
       " '##istic': 6553,\n",
       " '##zon': 11597,\n",
       " 'dublin': 5772,\n",
       " 'mu': 14163,\n",
       " 'turkish': 5037,\n",
       " 'whereas': 6168,\n",
       " 'rift': 16931,\n",
       " 'nation': 3842,\n",
       " 'manipulated': 20063,\n",
       " 'stu': 24646,\n",
       " 'ian': 4775,\n",
       " '##uin': 20023,\n",
       " '##had': 16102,\n",
       " 'sailors': 11279,\n",
       " '##ration': 8156,\n",
       " 'distributed': 5500,\n",
       " '##sport': 20205,\n",
       " 'defeated': 3249,\n",
       " 'savior': 24859,\n",
       " 'hancock': 13849,\n",
       " 'manifest': 19676,\n",
       " 'loves': 7459,\n",
       " 'straightening': 27508,\n",
       " 'grandchildren': 13628,\n",
       " 'shield': 6099,\n",
       " '##lau': 17298,\n",
       " 'coronation': 12773,\n",
       " '##§': 29650,\n",
       " 'pays': 12778,\n",
       " '##inski': 19880,\n",
       " 'grayson': 17556,\n",
       " 'legislation': 6094,\n",
       " 'ga': 11721,\n",
       " 'unpredictable': 21446,\n",
       " 'noir': 15587,\n",
       " '##ᵒ': 30039,\n",
       " 'multiple': 3674,\n",
       " 'smith': 3044,\n",
       " 'shady': 22824,\n",
       " '##fo': 14876,\n",
       " 'matches': 3503,\n",
       " '##keeper': 13106,\n",
       " 'fraction': 12884,\n",
       " 'relational': 28771,\n",
       " 'prestigious': 8919,\n",
       " '##ency': 11916,\n",
       " 'editors': 10195,\n",
       " 'sites': 4573,\n",
       " 'stroke': 6909,\n",
       " 'refuge': 9277,\n",
       " 'gabriel': 6127,\n",
       " 'respectable': 19416,\n",
       " '##rase': 23797,\n",
       " 'brianna': 25558,\n",
       " 'speculated': 15520,\n",
       " 'complained': 10865,\n",
       " 'shay': 18789,\n",
       " 'clothed': 24963,\n",
       " '##uh': 27225,\n",
       " 'twelve': 4376,\n",
       " '##heart': 22375,\n",
       " 'round': 2461,\n",
       " 'hailey': 21664,\n",
       " '##bay': 15907,\n",
       " 'unique': 4310,\n",
       " 'barrow': 15355,\n",
       " 'flickered': 15999,\n",
       " 'dissolution': 12275,\n",
       " 'autonomous': 8392,\n",
       " 'disease': 4295,\n",
       " 'successors': 18530,\n",
       " 'pots': 18911,\n",
       " '##ignon': 24796,\n",
       " 'crying': 6933,\n",
       " 'federal': 2976,\n",
       " 'brought': 2716,\n",
       " '##xia': 14787,\n",
       " '##tidae': 21861,\n",
       " 'pressured': 25227,\n",
       " 'survived': 5175,\n",
       " 'respected': 9768,\n",
       " 'branch': 3589,\n",
       " 'plastic': 6081,\n",
       " 'imply': 19515,\n",
       " '1646': 28783,\n",
       " 'shipping': 7829,\n",
       " '1979': 3245,\n",
       " 'compiled': 9227,\n",
       " '##ser': 8043,\n",
       " '202': 16798,\n",
       " 'greyhound': 21220,\n",
       " 'erroneously': 29411,\n",
       " 'tandem': 18231,\n",
       " '##த': 29921,\n",
       " '##uring': 12228,\n",
       " 'ruining': 27853,\n",
       " 'sparrow': 19479,\n",
       " '##ᵐ': 30038,\n",
       " 'cas': 25222,\n",
       " '##play': 13068,\n",
       " '##ity': 3012,\n",
       " '49': 4749,\n",
       " 'ャ': 1728,\n",
       " 'transformation': 8651,\n",
       " 'tango': 17609,\n",
       " 'recoil': 27429,\n",
       " '1712': 28460,\n",
       " 'ballard': 21896,\n",
       " 'sis': 24761,\n",
       " 'respectful': 26438,\n",
       " 'sired': 26940,\n",
       " 'vol': 5285,\n",
       " 'liberties': 18271,\n",
       " 'temporal': 15850,\n",
       " 'loser': 10916,\n",
       " 'maharaja': 21609,\n",
       " 'panama': 8515,\n",
       " '[unused431]': 436,\n",
       " '##風': 30503,\n",
       " 'ᆼ': 1489,\n",
       " 'rooftop': 23308,\n",
       " 'nam': 15125,\n",
       " '[unused635]': 640,\n",
       " 'taxes': 7773,\n",
       " 'deceased': 10181,\n",
       " '##ading': 23782,\n",
       " 'princes': 12000,\n",
       " 'bilateral': 17758,\n",
       " '##urs': 9236,\n",
       " 'pneumonia': 18583,\n",
       " 'shapes': 10466,\n",
       " 'oxide': 15772,\n",
       " 'nearer': 20388,\n",
       " '##nard': 16564,\n",
       " '##sur': 26210,\n",
       " 'squire': 21263,\n",
       " 'kellan': 19697,\n",
       " 'emerged': 6003,\n",
       " '##ots': 12868,\n",
       " '##ows': 15568,\n",
       " 'aspirations': 22877,\n",
       " 'stats': 26319,\n",
       " 'sermons': 20855,\n",
       " 'thread': 11689,\n",
       " 'unpublished': 19106,\n",
       " '122': 13092,\n",
       " 'custody': 9968,\n",
       " 'opener': 16181,\n",
       " 'crippled': 24433,\n",
       " 'uphold': 27329,\n",
       " 'marketed': 11625,\n",
       " 'diplomats': 23473,\n",
       " '1903': 5778,\n",
       " 'unite': 15908,\n",
       " '1808': 13040,\n",
       " 'atoll': 22292,\n",
       " '269': 25717,\n",
       " 'curly': 17546,\n",
       " 'actress': 3883,\n",
       " 'portfolio': 11103,\n",
       " 'accounted': 14729,\n",
       " 'press': 2811,\n",
       " 'phoebe': 18188,\n",
       " 'դ': 1222,\n",
       " '[unused76]': 77,\n",
       " 'literary': 4706,\n",
       " 'halt': 9190,\n",
       " 'spire': 19823,\n",
       " 'ᴰ': 1492,\n",
       " 'poor': 3532,\n",
       " 'stalk': 23899,\n",
       " '1707': 25029,\n",
       " '##fleet': 27657,\n",
       " 'tor': 17153,\n",
       " 'telescopes': 28026,\n",
       " 'outrage': 19006,\n",
       " 'inline': 23881,\n",
       " 'designing': 12697,\n",
       " '##fort': 13028,\n",
       " 'cut': 3013,\n",
       " 'choirs': 24743,\n",
       " 'rb': 21144,\n",
       " 'heroism': 27117,\n",
       " 'offensive': 5805,\n",
       " 'pinnacle': 26007,\n",
       " '1200': 14840,\n",
       " '##maker': 8571,\n",
       " 'broadcasting': 5062,\n",
       " '[unused651]': 656,\n",
       " 'hayward': 21506,\n",
       " 'pendant': 23152,\n",
       " 'arcs': 29137,\n",
       " 'accident': 4926,\n",
       " 'fascism': 23779,\n",
       " 'deployment': 10813,\n",
       " 'sits': 7719,\n",
       " 'girls': 3057,\n",
       " 'upcoming': 9046,\n",
       " 'authorization': 20104,\n",
       " 'iaaf': 21259,\n",
       " 'shrine': 9571,\n",
       " 'espionage': 21003,\n",
       " 'squad': 4686,\n",
       " '##cake': 17955,\n",
       " 'ornaments': 24005,\n",
       " '##hell': 18223,\n",
       " 'complaint': 12087,\n",
       " 'plausible': 24286,\n",
       " 'matilda': 17981,\n",
       " 'slater': 17916,\n",
       " 'campaigning': 18524,\n",
       " 'affiliated': 6989,\n",
       " 'looks': 3504,\n",
       " '##ople': 27469,\n",
       " 'ocean': 4153,\n",
       " 'leafs': 21349,\n",
       " '・': 1738,\n",
       " 'brave': 9191,\n",
       " '##ছ': 29893,\n",
       " '##hiko': 22204,\n",
       " 'otto': 8064,\n",
       " 'deciding': 10561,\n",
       " '宇': 1819,\n",
       " 'intervention': 8830,\n",
       " '##alic': 27072,\n",
       " 'close': 2485,\n",
       " '##mer': 5017,\n",
       " '[unused2]': 3,\n",
       " 'catastrophic': 23546,\n",
       " '##86': 20842,\n",
       " 'flashbacks': 28945,\n",
       " '[unused114]': 119,\n",
       " 'arthritis': 27641,\n",
       " 'bluff': 14441,\n",
       " 'norwood': 22804,\n",
       " 'taxpayers': 26457,\n",
       " '139': 16621,\n",
       " 'susceptible': 18002,\n",
       " 'security': 3036,\n",
       " 'pri': 26927,\n",
       " '[unused732]': 737,\n",
       " 'stocked': 24802,\n",
       " 'therese': 25598,\n",
       " 'hedge': 17834,\n",
       " 'hardwood': 23165,\n",
       " 'waterford': 17769,\n",
       " 'varied': 9426,\n",
       " 'represents': 5836,\n",
       " '##ality': 23732,\n",
       " 'worked': 2499,\n",
       " 'sectional': 27197,\n",
       " 'morally': 28980,\n",
       " 'academics': 15032,\n",
       " '##turing': 16037,\n",
       " 'wedding': 5030,\n",
       " '##odon': 28716,\n",
       " 'flying': 3909,\n",
       " 'lean': 8155,\n",
       " '##rial': 14482,\n",
       " 'rug': 20452,\n",
       " 'sociology': 11507,\n",
       " '[unused433]': 438,\n",
       " '[unused624]': 629,\n",
       " '##les': 4244,\n",
       " 'interpretations': 15931,\n",
       " 'stamford': 22469,\n",
       " '[unused556]': 561,\n",
       " 'reacting': 24868,\n",
       " '287': 23090,\n",
       " '[unused799]': 804,\n",
       " 'irregular': 12052,\n",
       " '[unused859]': 864,\n",
       " 'jerry': 6128,\n",
       " 'flags': 9245,\n",
       " 'groundwater': 22761,\n",
       " 'singh': 5960,\n",
       " '1823': 12522,\n",
       " 'gorilla': 23526,\n",
       " 'totaled': 23596,\n",
       " '##ksha': 28132,\n",
       " 'almeida': 29555,\n",
       " '##ව': 29943,\n",
       " 'p': 1052,\n",
       " 'smashing': 21105,\n",
       " '##gnan': 28207,\n",
       " 'autumn': 7114,\n",
       " '##zd': 26494,\n",
       " '##£': 29646,\n",
       " '##rba': 28483,\n",
       " 'colorful': 14231,\n",
       " 'feeding': 8521,\n",
       " 'persians': 27229,\n",
       " 'influential': 6383,\n",
       " 'rod': 8473,\n",
       " 'vigorous': 21813,\n",
       " 'covered': 3139,\n",
       " 'sensing': 13851,\n",
       " 'asher': 17243,\n",
       " 'reversal': 23163,\n",
       " 'ecumenical': 23540,\n",
       " 'battery': 6046,\n",
       " 'windshield': 19521,\n",
       " '##將': 30354,\n",
       " '##す': 30184,\n",
       " 'square': 2675,\n",
       " 'biased': 25352,\n",
       " 'medallist': 28595,\n",
       " 'gateway': 11909,\n",
       " 'delivers': 18058,\n",
       " 'equity': 10067,\n",
       " 'quarterback': 9074,\n",
       " 'ninja': 14104,\n",
       " '##ums': 18163,\n",
       " 'florence': 7701,\n",
       " 'germans': 7074,\n",
       " 'turnover': 20991,\n",
       " 'lobbying': 19670,\n",
       " 'assembly': 3320,\n",
       " 'pascal': 17878,\n",
       " '[unused286]': 291,\n",
       " 'tasting': 18767,\n",
       " '870': 28864,\n",
       " '##ich': 7033,\n",
       " 'greenwood': 16827,\n",
       " 'importance': 5197,\n",
       " '##vo': 6767,\n",
       " 'german': 2446,\n",
       " '125': 8732,\n",
       " 'environment': 4044,\n",
       " '##fall': 13976,\n",
       " 'faber': 21720,\n",
       " 'mm': 3461,\n",
       " 'fatty': 19101,\n",
       " 'masters': 5972,\n",
       " 'simeon': 24371,\n",
       " 'roll': 4897,\n",
       " '##ken': 7520,\n",
       " 'cleavage': 28691,\n",
       " 'galactic': 21375,\n",
       " 'repository': 22409,\n",
       " 'ulster': 11059,\n",
       " 'inspiring': 18988,\n",
       " '##anto': 21634,\n",
       " 'eighties': 27690,\n",
       " 'buren': 29470,\n",
       " 'vain': 15784,\n",
       " 'た': 1661,\n",
       " 'demo': 9703,\n",
       " '##ament': 24996,\n",
       " '##lysis': 26394,\n",
       " 'relation': 7189,\n",
       " '##coll': 26895,\n",
       " 'distrust': 29245,\n",
       " 'comfort': 7216,\n",
       " 'tool': 6994,\n",
       " '##aran': 20486,\n",
       " 'wolfgang': 13865,\n",
       " '♣': 1624,\n",
       " 'almighty': 26668,\n",
       " '##brook': 9697,\n",
       " 'postal': 10690,\n",
       " 'sampling': 16227,\n",
       " 'watched': 3427,\n",
       " 'packages': 14555,\n",
       " 'crate': 27297,\n",
       " 'semester': 13609,\n",
       " '##pu': 14289,\n",
       " 'fences': 21549,\n",
       " '##teacher': 24741,\n",
       " 'faulkner': 25109,\n",
       " 'barcelona': 7623,\n",
       " 'mysteriously': 29239,\n",
       " 'objection': 22224,\n",
       " 'beale': 28371,\n",
       " '##pi': 8197,\n",
       " 'accuse': 26960,\n",
       " 'subsidiary': 7506,\n",
       " '##tablished': 28146,\n",
       " '##selle': 19358,\n",
       " 'das': 8695,\n",
       " 'tracing': 16907,\n",
       " 'section': 2930,\n",
       " 'agency': 4034,\n",
       " 'nationwide': 9053,\n",
       " 'fk': 14352,\n",
       " 'harcourt': 22714,\n",
       " 'balkans': 19733,\n",
       " '##io': 3695,\n",
       " 'methods': 4725,\n",
       " 'sensible': 21082,\n",
       " 'jia': 25871,\n",
       " 'johor': 25268,\n",
       " '##hane': 28006,\n",
       " '10th': 6049,\n",
       " '1776': 13963,\n",
       " 'nj': 19193,\n",
       " 'and': 1998,\n",
       " '35': 3486,\n",
       " 'internal': 4722,\n",
       " 'm3': 29061,\n",
       " 'namesake': 17283,\n",
       " '##yse': 23274,\n",
       " '6th': 5351,\n",
       " 'beijing': 7211,\n",
       " 'dwarfs': 28984,\n",
       " 'glowing': 10156,\n",
       " '##ntal': 15758,\n",
       " 'croydon': 21838,\n",
       " 'manufacture': 9922,\n",
       " 'eligible': 7792,\n",
       " 'juris': 27551,\n",
       " 'corresponded': 27601,\n",
       " 'flanks': 23547,\n",
       " 'caf': 24689,\n",
       " 'purchasing': 13131,\n",
       " 'fangs': 11738,\n",
       " 'danes': 27476,\n",
       " 'reputation': 5891,\n",
       " 'any': 2151,\n",
       " 'rushing': 8375,\n",
       " '##rys': 24769,\n",
       " 'professorship': 22661,\n",
       " 'roche': 20162,\n",
       " 'supervision': 10429,\n",
       " 'chow': 20209,\n",
       " '##som': 25426,\n",
       " '407': 28941,\n",
       " 'acc': 16222,\n",
       " 'evade': 26399,\n",
       " '加': 1779,\n",
       " '大': 1810,\n",
       " 'entrepreneur': 10670,\n",
       " 'marino': 17185,\n",
       " 'onto': 3031,\n",
       " 'elevated': 8319,\n",
       " 'evicted': 25777,\n",
       " 's': 1055,\n",
       " '##iating': 15370,\n",
       " 'phantom': 11588,\n",
       " '##uts': 16446,\n",
       " 'hush': 20261,\n",
       " 'johansson': 26447,\n",
       " '##—': 30052,\n",
       " 'bulbs': 25548,\n",
       " 'accessible': 7801,\n",
       " '##llus': 20159,\n",
       " 'margaret': 5545,\n",
       " 'stabilize': 27790,\n",
       " 'arden': 26225,\n",
       " 'tapped': 10410,\n",
       " 'softer': 19013,\n",
       " 'vet': 29525,\n",
       " '##mian': 20924,\n",
       " 'е': 1185,\n",
       " '##wari': 20031,\n",
       " '1703': 28366,\n",
       " 'portrayed': 6791,\n",
       " 'gps': 14658,\n",
       " 'deal': 3066,\n",
       " '##gor': 20255,\n",
       " 'escorts': 24877,\n",
       " 'contemplated': 23133,\n",
       " 'kisses': 8537,\n",
       " 'tough': 7823,\n",
       " 'trends': 12878,\n",
       " 'crystals': 14438,\n",
       " 'taxonomy': 25274,\n",
       " 'reprise': 16851,\n",
       " '##nc': 12273,\n",
       " 'wired': 17502,\n",
       " 'peninsular': 22682,\n",
       " 'blah': 27984,\n",
       " '##ags': 26454,\n",
       " '##bbly': 24200,\n",
       " '##500': 29345,\n",
       " '[unused608]': 613,\n",
       " 'circa': 12800,\n",
       " 'litre': 16812,\n",
       " 'mutually': 20271,\n",
       " 'serena': 14419,\n",
       " 'doncaster': 18895,\n",
       " 'allegations': 9989,\n",
       " 'circulating': 22458,\n",
       " 'illusion': 12492,\n",
       " '##otic': 20214,\n",
       " '##ion': 3258,\n",
       " 'canadiens': 21247,\n",
       " 'discussions': 10287,\n",
       " '##li': 3669,\n",
       " 'washed': 8871,\n",
       " 'loosely': 11853,\n",
       " '##vey': 12417,\n",
       " 'splash': 17624,\n",
       " 'advertised': 17099,\n",
       " '[unused389]': 394,\n",
       " '##uce': 18796,\n",
       " 'jenny': 8437,\n",
       " 'yanking': 25716,\n",
       " 'campground': 29144,\n",
       " 'celine': 24550,\n",
       " 'couple': 3232,\n",
       " 'stairway': 21952,\n",
       " 'outputs': 27852,\n",
       " 'situations': 8146,\n",
       " 'reconstruction': 8735,\n",
       " 'switches': 15924,\n",
       " '208': 18512,\n",
       " 'kenton': 25446,\n",
       " 'ᄒ': 1469,\n",
       " '##arte': 24847,\n",
       " '[unused416]': 421,\n",
       " 'woke': 8271,\n",
       " 'cantor': 26519,\n",
       " 'cluster': 9324,\n",
       " 'guided': 8546,\n",
       " 'trio': 7146,\n",
       " '##mal': 9067,\n",
       " 'bellevue': 26756,\n",
       " '##entes': 26933,\n",
       " 'leap': 11679,\n",
       " 'harmony': 9396,\n",
       " 'confidence': 7023,\n",
       " 'poised': 22303,\n",
       " 'passing': 4458,\n",
       " 'arrive': 7180,\n",
       " 'stubble': 26816,\n",
       " 'evelyn': 12903,\n",
       " 'grimaced': 19014,\n",
       " 'builds': 16473,\n",
       " 'basil': 14732,\n",
       " 'molecular': 8382,\n",
       " '##iary': 17302,\n",
       " '##rooms': 29020,\n",
       " '##ᴮ': 30027,\n",
       " '分': 1775,\n",
       " 'texted': 24637,\n",
       " 'owed': 12232,\n",
       " '##num': 19172,\n",
       " 'uefa': 6663,\n",
       " 'planetary': 17700,\n",
       " 'curated': 17940,\n",
       " 'roc': 21326,\n",
       " '##bush': 22427,\n",
       " 'tide': 10401,\n",
       " '##thing': 20744,\n",
       " 'premiership': 11264,\n",
       " 'annexed': 13291,\n",
       " '##tell': 23567,\n",
       " 'abrupt': 18772,\n",
       " '##ratic': 23671,\n",
       " '##jin': 14642,\n",
       " 'laundering': 28289,\n",
       " 'ying': 20879,\n",
       " 'responsibility': 5368,\n",
       " 'bell': 4330,\n",
       " '##kura': 28260,\n",
       " 'alaska': 7397,\n",
       " 'never': 2196,\n",
       " '275': 17528,\n",
       " 'swayed': 20122,\n",
       " 'tent': 9311,\n",
       " 'stabilized': 27697,\n",
       " 'thyroid': 29610,\n",
       " 'individuals': 3633,\n",
       " 'structural': 8332,\n",
       " 'bile': 23974,\n",
       " 'norwegian': 5046,\n",
       " 'civilization': 10585,\n",
       " 'kicked': 6476,\n",
       " '##ime': 14428,\n",
       " 'kidnapping': 15071,\n",
       " 'castro': 11794,\n",
       " '[unused517]': 522,\n",
       " 'herman': 11458,\n",
       " 'vane': 23334,\n",
       " 'screwing': 29082,\n",
       " 'oblique': 20658,\n",
       " 'bang': 9748,\n",
       " 'ヒ': 1719,\n",
       " 'flanders': 13998,\n",
       " 'zombies': 14106,\n",
       " 'withstand': 19319,\n",
       " 'juno': 20788,\n",
       " 'landscapes': 12793,\n",
       " 'temptation': 17232,\n",
       " '91': 6205,\n",
       " '##uno': 27819,\n",
       " '1661': 24046,\n",
       " 'realized': 3651,\n",
       " 'chemist': 15535,\n",
       " 'supplementary': 26215,\n",
       " 'anchored': 14453,\n",
       " 'thou': 15223,\n",
       " '##ɛ': 29275,\n",
       " 'apostles': 19178,\n",
       " 'cong': 26478,\n",
       " 'birth': 4182,\n",
       " 'mix': 4666,\n",
       " 'reunion': 10301,\n",
       " 'josh': 6498,\n",
       " 'costello': 21015,\n",
       " 'ultimate': 7209,\n",
       " '##bia': 11607,\n",
       " '##ウ': 30222,\n",
       " '##shah': 25611,\n",
       " 'f': 1042,\n",
       " 'vanguard': 18332,\n",
       " '298': 27240,\n",
       " '侍': 1765,\n",
       " 'markus': 23030,\n",
       " 'forearms': 27323,\n",
       " 'idle': 18373,\n",
       " 'destined': 16036,\n",
       " 'pam': 14089,\n",
       " 'fauna': 15018,\n",
       " 'weaving': 15360,\n",
       " '##eon': 10242,\n",
       " '##fles': 28331,\n",
       " 'restrictions': 9259,\n",
       " 'runes': 29161,\n",
       " 'permitted': 7936,\n",
       " 'worthy': 11007,\n",
       " 'sadly': 13718,\n",
       " '##tist': 16774,\n",
       " 'rhapsody': 29395,\n",
       " '[unused545]': 550,\n",
       " '生': 1910,\n",
       " 'abbreviated': 12066,\n",
       " 'textures': 29343,\n",
       " 'taps': 25316,\n",
       " 'cult': 8754,\n",
       " 'managing': 6605,\n",
       " 'camera': 4950,\n",
       " '##ע': 29805,\n",
       " 'dust': 6497,\n",
       " 'regime': 6939,\n",
       " 'darcy': 17685,\n",
       " 'ally': 9698,\n",
       " 'rotor': 18929,\n",
       " 'mistake': 6707,\n",
       " 'gel': 21500,\n",
       " '[unused98]': 99,\n",
       " 'tradition': 4535,\n",
       " '##lita': 27606,\n",
       " '124': 13412,\n",
       " '##combe': 14149,\n",
       " 'felony': 24648,\n",
       " 'recall': 9131,\n",
       " 'away': 2185,\n",
       " 'iihf': 26904,\n",
       " '[unused435]': 440,\n",
       " '##turn': 22299,\n",
       " 'sirius': 23466,\n",
       " 'mohammad': 12050,\n",
       " 'albeit': 12167,\n",
       " '##uss': 17854,\n",
       " 'sooner': 10076,\n",
       " 'setup': 16437,\n",
       " 'zhu': 15503,\n",
       " 'raids': 11217,\n",
       " 'telugu': 12180,\n",
       " '##jas': 17386,\n",
       " '##folia': 21710,\n",
       " 'awaiting': 15497,\n",
       " 'forcefully': 23097,\n",
       " 'middletown': 28747,\n",
       " 'ended': 3092,\n",
       " '##lt': 7096,\n",
       " 'erica': 19295,\n",
       " 'forgive': 9641,\n",
       " 'himself': 2370,\n",
       " 'retribution': 25928,\n",
       " 'pun': 26136,\n",
       " 'chairs': 8397,\n",
       " '##spar': 27694,\n",
       " '##ious': 6313,\n",
       " 'starts': 4627,\n",
       " 'distances': 12103,\n",
       " 'remainder': 6893,\n",
       " ...}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "neutral-colleague",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '[PAD]',\n",
       " 1: '[unused0]',\n",
       " 2: '[unused1]',\n",
       " 3: '[unused2]',\n",
       " 4: '[unused3]',\n",
       " 5: '[unused4]',\n",
       " 6: '[unused5]',\n",
       " 7: '[unused6]',\n",
       " 8: '[unused7]',\n",
       " 9: '[unused8]',\n",
       " 10: '[unused9]',\n",
       " 11: '[unused10]',\n",
       " 12: '[unused11]',\n",
       " 13: '[unused12]',\n",
       " 14: '[unused13]',\n",
       " 15: '[unused14]',\n",
       " 16: '[unused15]',\n",
       " 17: '[unused16]',\n",
       " 18: '[unused17]',\n",
       " 19: '[unused18]',\n",
       " 20: '[unused19]',\n",
       " 21: '[unused20]',\n",
       " 22: '[unused21]',\n",
       " 23: '[unused22]',\n",
       " 24: '[unused23]',\n",
       " 25: '[unused24]',\n",
       " 26: '[unused25]',\n",
       " 27: '[unused26]',\n",
       " 28: '[unused27]',\n",
       " 29: '[unused28]',\n",
       " 30: '[unused29]',\n",
       " 31: '[unused30]',\n",
       " 32: '[unused31]',\n",
       " 33: '[unused32]',\n",
       " 34: '[unused33]',\n",
       " 35: '[unused34]',\n",
       " 36: '[unused35]',\n",
       " 37: '[unused36]',\n",
       " 38: '[unused37]',\n",
       " 39: '[unused38]',\n",
       " 40: '[unused39]',\n",
       " 41: '[unused40]',\n",
       " 42: '[unused41]',\n",
       " 43: '[unused42]',\n",
       " 44: '[unused43]',\n",
       " 45: '[unused44]',\n",
       " 46: '[unused45]',\n",
       " 47: '[unused46]',\n",
       " 48: '[unused47]',\n",
       " 49: '[unused48]',\n",
       " 50: '[unused49]',\n",
       " 51: '[unused50]',\n",
       " 52: '[unused51]',\n",
       " 53: '[unused52]',\n",
       " 54: '[unused53]',\n",
       " 55: '[unused54]',\n",
       " 56: '[unused55]',\n",
       " 57: '[unused56]',\n",
       " 58: '[unused57]',\n",
       " 59: '[unused58]',\n",
       " 60: '[unused59]',\n",
       " 61: '[unused60]',\n",
       " 62: '[unused61]',\n",
       " 63: '[unused62]',\n",
       " 64: '[unused63]',\n",
       " 65: '[unused64]',\n",
       " 66: '[unused65]',\n",
       " 67: '[unused66]',\n",
       " 68: '[unused67]',\n",
       " 69: '[unused68]',\n",
       " 70: '[unused69]',\n",
       " 71: '[unused70]',\n",
       " 72: '[unused71]',\n",
       " 73: '[unused72]',\n",
       " 74: '[unused73]',\n",
       " 75: '[unused74]',\n",
       " 76: '[unused75]',\n",
       " 77: '[unused76]',\n",
       " 78: '[unused77]',\n",
       " 79: '[unused78]',\n",
       " 80: '[unused79]',\n",
       " 81: '[unused80]',\n",
       " 82: '[unused81]',\n",
       " 83: '[unused82]',\n",
       " 84: '[unused83]',\n",
       " 85: '[unused84]',\n",
       " 86: '[unused85]',\n",
       " 87: '[unused86]',\n",
       " 88: '[unused87]',\n",
       " 89: '[unused88]',\n",
       " 90: '[unused89]',\n",
       " 91: '[unused90]',\n",
       " 92: '[unused91]',\n",
       " 93: '[unused92]',\n",
       " 94: '[unused93]',\n",
       " 95: '[unused94]',\n",
       " 96: '[unused95]',\n",
       " 97: '[unused96]',\n",
       " 98: '[unused97]',\n",
       " 99: '[unused98]',\n",
       " 100: '[UNK]',\n",
       " 101: '[CLS]',\n",
       " 102: '[SEP]',\n",
       " 103: '[MASK]',\n",
       " 104: '[unused99]',\n",
       " 105: '[unused100]',\n",
       " 106: '[unused101]',\n",
       " 107: '[unused102]',\n",
       " 108: '[unused103]',\n",
       " 109: '[unused104]',\n",
       " 110: '[unused105]',\n",
       " 111: '[unused106]',\n",
       " 112: '[unused107]',\n",
       " 113: '[unused108]',\n",
       " 114: '[unused109]',\n",
       " 115: '[unused110]',\n",
       " 116: '[unused111]',\n",
       " 117: '[unused112]',\n",
       " 118: '[unused113]',\n",
       " 119: '[unused114]',\n",
       " 120: '[unused115]',\n",
       " 121: '[unused116]',\n",
       " 122: '[unused117]',\n",
       " 123: '[unused118]',\n",
       " 124: '[unused119]',\n",
       " 125: '[unused120]',\n",
       " 126: '[unused121]',\n",
       " 127: '[unused122]',\n",
       " 128: '[unused123]',\n",
       " 129: '[unused124]',\n",
       " 130: '[unused125]',\n",
       " 131: '[unused126]',\n",
       " 132: '[unused127]',\n",
       " 133: '[unused128]',\n",
       " 134: '[unused129]',\n",
       " 135: '[unused130]',\n",
       " 136: '[unused131]',\n",
       " 137: '[unused132]',\n",
       " 138: '[unused133]',\n",
       " 139: '[unused134]',\n",
       " 140: '[unused135]',\n",
       " 141: '[unused136]',\n",
       " 142: '[unused137]',\n",
       " 143: '[unused138]',\n",
       " 144: '[unused139]',\n",
       " 145: '[unused140]',\n",
       " 146: '[unused141]',\n",
       " 147: '[unused142]',\n",
       " 148: '[unused143]',\n",
       " 149: '[unused144]',\n",
       " 150: '[unused145]',\n",
       " 151: '[unused146]',\n",
       " 152: '[unused147]',\n",
       " 153: '[unused148]',\n",
       " 154: '[unused149]',\n",
       " 155: '[unused150]',\n",
       " 156: '[unused151]',\n",
       " 157: '[unused152]',\n",
       " 158: '[unused153]',\n",
       " 159: '[unused154]',\n",
       " 160: '[unused155]',\n",
       " 161: '[unused156]',\n",
       " 162: '[unused157]',\n",
       " 163: '[unused158]',\n",
       " 164: '[unused159]',\n",
       " 165: '[unused160]',\n",
       " 166: '[unused161]',\n",
       " 167: '[unused162]',\n",
       " 168: '[unused163]',\n",
       " 169: '[unused164]',\n",
       " 170: '[unused165]',\n",
       " 171: '[unused166]',\n",
       " 172: '[unused167]',\n",
       " 173: '[unused168]',\n",
       " 174: '[unused169]',\n",
       " 175: '[unused170]',\n",
       " 176: '[unused171]',\n",
       " 177: '[unused172]',\n",
       " 178: '[unused173]',\n",
       " 179: '[unused174]',\n",
       " 180: '[unused175]',\n",
       " 181: '[unused176]',\n",
       " 182: '[unused177]',\n",
       " 183: '[unused178]',\n",
       " 184: '[unused179]',\n",
       " 185: '[unused180]',\n",
       " 186: '[unused181]',\n",
       " 187: '[unused182]',\n",
       " 188: '[unused183]',\n",
       " 189: '[unused184]',\n",
       " 190: '[unused185]',\n",
       " 191: '[unused186]',\n",
       " 192: '[unused187]',\n",
       " 193: '[unused188]',\n",
       " 194: '[unused189]',\n",
       " 195: '[unused190]',\n",
       " 196: '[unused191]',\n",
       " 197: '[unused192]',\n",
       " 198: '[unused193]',\n",
       " 199: '[unused194]',\n",
       " 200: '[unused195]',\n",
       " 201: '[unused196]',\n",
       " 202: '[unused197]',\n",
       " 203: '[unused198]',\n",
       " 204: '[unused199]',\n",
       " 205: '[unused200]',\n",
       " 206: '[unused201]',\n",
       " 207: '[unused202]',\n",
       " 208: '[unused203]',\n",
       " 209: '[unused204]',\n",
       " 210: '[unused205]',\n",
       " 211: '[unused206]',\n",
       " 212: '[unused207]',\n",
       " 213: '[unused208]',\n",
       " 214: '[unused209]',\n",
       " 215: '[unused210]',\n",
       " 216: '[unused211]',\n",
       " 217: '[unused212]',\n",
       " 218: '[unused213]',\n",
       " 219: '[unused214]',\n",
       " 220: '[unused215]',\n",
       " 221: '[unused216]',\n",
       " 222: '[unused217]',\n",
       " 223: '[unused218]',\n",
       " 224: '[unused219]',\n",
       " 225: '[unused220]',\n",
       " 226: '[unused221]',\n",
       " 227: '[unused222]',\n",
       " 228: '[unused223]',\n",
       " 229: '[unused224]',\n",
       " 230: '[unused225]',\n",
       " 231: '[unused226]',\n",
       " 232: '[unused227]',\n",
       " 233: '[unused228]',\n",
       " 234: '[unused229]',\n",
       " 235: '[unused230]',\n",
       " 236: '[unused231]',\n",
       " 237: '[unused232]',\n",
       " 238: '[unused233]',\n",
       " 239: '[unused234]',\n",
       " 240: '[unused235]',\n",
       " 241: '[unused236]',\n",
       " 242: '[unused237]',\n",
       " 243: '[unused238]',\n",
       " 244: '[unused239]',\n",
       " 245: '[unused240]',\n",
       " 246: '[unused241]',\n",
       " 247: '[unused242]',\n",
       " 248: '[unused243]',\n",
       " 249: '[unused244]',\n",
       " 250: '[unused245]',\n",
       " 251: '[unused246]',\n",
       " 252: '[unused247]',\n",
       " 253: '[unused248]',\n",
       " 254: '[unused249]',\n",
       " 255: '[unused250]',\n",
       " 256: '[unused251]',\n",
       " 257: '[unused252]',\n",
       " 258: '[unused253]',\n",
       " 259: '[unused254]',\n",
       " 260: '[unused255]',\n",
       " 261: '[unused256]',\n",
       " 262: '[unused257]',\n",
       " 263: '[unused258]',\n",
       " 264: '[unused259]',\n",
       " 265: '[unused260]',\n",
       " 266: '[unused261]',\n",
       " 267: '[unused262]',\n",
       " 268: '[unused263]',\n",
       " 269: '[unused264]',\n",
       " 270: '[unused265]',\n",
       " 271: '[unused266]',\n",
       " 272: '[unused267]',\n",
       " 273: '[unused268]',\n",
       " 274: '[unused269]',\n",
       " 275: '[unused270]',\n",
       " 276: '[unused271]',\n",
       " 277: '[unused272]',\n",
       " 278: '[unused273]',\n",
       " 279: '[unused274]',\n",
       " 280: '[unused275]',\n",
       " 281: '[unused276]',\n",
       " 282: '[unused277]',\n",
       " 283: '[unused278]',\n",
       " 284: '[unused279]',\n",
       " 285: '[unused280]',\n",
       " 286: '[unused281]',\n",
       " 287: '[unused282]',\n",
       " 288: '[unused283]',\n",
       " 289: '[unused284]',\n",
       " 290: '[unused285]',\n",
       " 291: '[unused286]',\n",
       " 292: '[unused287]',\n",
       " 293: '[unused288]',\n",
       " 294: '[unused289]',\n",
       " 295: '[unused290]',\n",
       " 296: '[unused291]',\n",
       " 297: '[unused292]',\n",
       " 298: '[unused293]',\n",
       " 299: '[unused294]',\n",
       " 300: '[unused295]',\n",
       " 301: '[unused296]',\n",
       " 302: '[unused297]',\n",
       " 303: '[unused298]',\n",
       " 304: '[unused299]',\n",
       " 305: '[unused300]',\n",
       " 306: '[unused301]',\n",
       " 307: '[unused302]',\n",
       " 308: '[unused303]',\n",
       " 309: '[unused304]',\n",
       " 310: '[unused305]',\n",
       " 311: '[unused306]',\n",
       " 312: '[unused307]',\n",
       " 313: '[unused308]',\n",
       " 314: '[unused309]',\n",
       " 315: '[unused310]',\n",
       " 316: '[unused311]',\n",
       " 317: '[unused312]',\n",
       " 318: '[unused313]',\n",
       " 319: '[unused314]',\n",
       " 320: '[unused315]',\n",
       " 321: '[unused316]',\n",
       " 322: '[unused317]',\n",
       " 323: '[unused318]',\n",
       " 324: '[unused319]',\n",
       " 325: '[unused320]',\n",
       " 326: '[unused321]',\n",
       " 327: '[unused322]',\n",
       " 328: '[unused323]',\n",
       " 329: '[unused324]',\n",
       " 330: '[unused325]',\n",
       " 331: '[unused326]',\n",
       " 332: '[unused327]',\n",
       " 333: '[unused328]',\n",
       " 334: '[unused329]',\n",
       " 335: '[unused330]',\n",
       " 336: '[unused331]',\n",
       " 337: '[unused332]',\n",
       " 338: '[unused333]',\n",
       " 339: '[unused334]',\n",
       " 340: '[unused335]',\n",
       " 341: '[unused336]',\n",
       " 342: '[unused337]',\n",
       " 343: '[unused338]',\n",
       " 344: '[unused339]',\n",
       " 345: '[unused340]',\n",
       " 346: '[unused341]',\n",
       " 347: '[unused342]',\n",
       " 348: '[unused343]',\n",
       " 349: '[unused344]',\n",
       " 350: '[unused345]',\n",
       " 351: '[unused346]',\n",
       " 352: '[unused347]',\n",
       " 353: '[unused348]',\n",
       " 354: '[unused349]',\n",
       " 355: '[unused350]',\n",
       " 356: '[unused351]',\n",
       " 357: '[unused352]',\n",
       " 358: '[unused353]',\n",
       " 359: '[unused354]',\n",
       " 360: '[unused355]',\n",
       " 361: '[unused356]',\n",
       " 362: '[unused357]',\n",
       " 363: '[unused358]',\n",
       " 364: '[unused359]',\n",
       " 365: '[unused360]',\n",
       " 366: '[unused361]',\n",
       " 367: '[unused362]',\n",
       " 368: '[unused363]',\n",
       " 369: '[unused364]',\n",
       " 370: '[unused365]',\n",
       " 371: '[unused366]',\n",
       " 372: '[unused367]',\n",
       " 373: '[unused368]',\n",
       " 374: '[unused369]',\n",
       " 375: '[unused370]',\n",
       " 376: '[unused371]',\n",
       " 377: '[unused372]',\n",
       " 378: '[unused373]',\n",
       " 379: '[unused374]',\n",
       " 380: '[unused375]',\n",
       " 381: '[unused376]',\n",
       " 382: '[unused377]',\n",
       " 383: '[unused378]',\n",
       " 384: '[unused379]',\n",
       " 385: '[unused380]',\n",
       " 386: '[unused381]',\n",
       " 387: '[unused382]',\n",
       " 388: '[unused383]',\n",
       " 389: '[unused384]',\n",
       " 390: '[unused385]',\n",
       " 391: '[unused386]',\n",
       " 392: '[unused387]',\n",
       " 393: '[unused388]',\n",
       " 394: '[unused389]',\n",
       " 395: '[unused390]',\n",
       " 396: '[unused391]',\n",
       " 397: '[unused392]',\n",
       " 398: '[unused393]',\n",
       " 399: '[unused394]',\n",
       " 400: '[unused395]',\n",
       " 401: '[unused396]',\n",
       " 402: '[unused397]',\n",
       " 403: '[unused398]',\n",
       " 404: '[unused399]',\n",
       " 405: '[unused400]',\n",
       " 406: '[unused401]',\n",
       " 407: '[unused402]',\n",
       " 408: '[unused403]',\n",
       " 409: '[unused404]',\n",
       " 410: '[unused405]',\n",
       " 411: '[unused406]',\n",
       " 412: '[unused407]',\n",
       " 413: '[unused408]',\n",
       " 414: '[unused409]',\n",
       " 415: '[unused410]',\n",
       " 416: '[unused411]',\n",
       " 417: '[unused412]',\n",
       " 418: '[unused413]',\n",
       " 419: '[unused414]',\n",
       " 420: '[unused415]',\n",
       " 421: '[unused416]',\n",
       " 422: '[unused417]',\n",
       " 423: '[unused418]',\n",
       " 424: '[unused419]',\n",
       " 425: '[unused420]',\n",
       " 426: '[unused421]',\n",
       " 427: '[unused422]',\n",
       " 428: '[unused423]',\n",
       " 429: '[unused424]',\n",
       " 430: '[unused425]',\n",
       " 431: '[unused426]',\n",
       " 432: '[unused427]',\n",
       " 433: '[unused428]',\n",
       " 434: '[unused429]',\n",
       " 435: '[unused430]',\n",
       " 436: '[unused431]',\n",
       " 437: '[unused432]',\n",
       " 438: '[unused433]',\n",
       " 439: '[unused434]',\n",
       " 440: '[unused435]',\n",
       " 441: '[unused436]',\n",
       " 442: '[unused437]',\n",
       " 443: '[unused438]',\n",
       " 444: '[unused439]',\n",
       " 445: '[unused440]',\n",
       " 446: '[unused441]',\n",
       " 447: '[unused442]',\n",
       " 448: '[unused443]',\n",
       " 449: '[unused444]',\n",
       " 450: '[unused445]',\n",
       " 451: '[unused446]',\n",
       " 452: '[unused447]',\n",
       " 453: '[unused448]',\n",
       " 454: '[unused449]',\n",
       " 455: '[unused450]',\n",
       " 456: '[unused451]',\n",
       " 457: '[unused452]',\n",
       " 458: '[unused453]',\n",
       " 459: '[unused454]',\n",
       " 460: '[unused455]',\n",
       " 461: '[unused456]',\n",
       " 462: '[unused457]',\n",
       " 463: '[unused458]',\n",
       " 464: '[unused459]',\n",
       " 465: '[unused460]',\n",
       " 466: '[unused461]',\n",
       " 467: '[unused462]',\n",
       " 468: '[unused463]',\n",
       " 469: '[unused464]',\n",
       " 470: '[unused465]',\n",
       " 471: '[unused466]',\n",
       " 472: '[unused467]',\n",
       " 473: '[unused468]',\n",
       " 474: '[unused469]',\n",
       " 475: '[unused470]',\n",
       " 476: '[unused471]',\n",
       " 477: '[unused472]',\n",
       " 478: '[unused473]',\n",
       " 479: '[unused474]',\n",
       " 480: '[unused475]',\n",
       " 481: '[unused476]',\n",
       " 482: '[unused477]',\n",
       " 483: '[unused478]',\n",
       " 484: '[unused479]',\n",
       " 485: '[unused480]',\n",
       " 486: '[unused481]',\n",
       " 487: '[unused482]',\n",
       " 488: '[unused483]',\n",
       " 489: '[unused484]',\n",
       " 490: '[unused485]',\n",
       " 491: '[unused486]',\n",
       " 492: '[unused487]',\n",
       " 493: '[unused488]',\n",
       " 494: '[unused489]',\n",
       " 495: '[unused490]',\n",
       " 496: '[unused491]',\n",
       " 497: '[unused492]',\n",
       " 498: '[unused493]',\n",
       " 499: '[unused494]',\n",
       " 500: '[unused495]',\n",
       " 501: '[unused496]',\n",
       " 502: '[unused497]',\n",
       " 503: '[unused498]',\n",
       " 504: '[unused499]',\n",
       " 505: '[unused500]',\n",
       " 506: '[unused501]',\n",
       " 507: '[unused502]',\n",
       " 508: '[unused503]',\n",
       " 509: '[unused504]',\n",
       " 510: '[unused505]',\n",
       " 511: '[unused506]',\n",
       " 512: '[unused507]',\n",
       " 513: '[unused508]',\n",
       " 514: '[unused509]',\n",
       " 515: '[unused510]',\n",
       " 516: '[unused511]',\n",
       " 517: '[unused512]',\n",
       " 518: '[unused513]',\n",
       " 519: '[unused514]',\n",
       " 520: '[unused515]',\n",
       " 521: '[unused516]',\n",
       " 522: '[unused517]',\n",
       " 523: '[unused518]',\n",
       " 524: '[unused519]',\n",
       " 525: '[unused520]',\n",
       " 526: '[unused521]',\n",
       " 527: '[unused522]',\n",
       " 528: '[unused523]',\n",
       " 529: '[unused524]',\n",
       " 530: '[unused525]',\n",
       " 531: '[unused526]',\n",
       " 532: '[unused527]',\n",
       " 533: '[unused528]',\n",
       " 534: '[unused529]',\n",
       " 535: '[unused530]',\n",
       " 536: '[unused531]',\n",
       " 537: '[unused532]',\n",
       " 538: '[unused533]',\n",
       " 539: '[unused534]',\n",
       " 540: '[unused535]',\n",
       " 541: '[unused536]',\n",
       " 542: '[unused537]',\n",
       " 543: '[unused538]',\n",
       " 544: '[unused539]',\n",
       " 545: '[unused540]',\n",
       " 546: '[unused541]',\n",
       " 547: '[unused542]',\n",
       " 548: '[unused543]',\n",
       " 549: '[unused544]',\n",
       " 550: '[unused545]',\n",
       " 551: '[unused546]',\n",
       " 552: '[unused547]',\n",
       " 553: '[unused548]',\n",
       " 554: '[unused549]',\n",
       " 555: '[unused550]',\n",
       " 556: '[unused551]',\n",
       " 557: '[unused552]',\n",
       " 558: '[unused553]',\n",
       " 559: '[unused554]',\n",
       " 560: '[unused555]',\n",
       " 561: '[unused556]',\n",
       " 562: '[unused557]',\n",
       " 563: '[unused558]',\n",
       " 564: '[unused559]',\n",
       " 565: '[unused560]',\n",
       " 566: '[unused561]',\n",
       " 567: '[unused562]',\n",
       " 568: '[unused563]',\n",
       " 569: '[unused564]',\n",
       " 570: '[unused565]',\n",
       " 571: '[unused566]',\n",
       " 572: '[unused567]',\n",
       " 573: '[unused568]',\n",
       " 574: '[unused569]',\n",
       " 575: '[unused570]',\n",
       " 576: '[unused571]',\n",
       " 577: '[unused572]',\n",
       " 578: '[unused573]',\n",
       " 579: '[unused574]',\n",
       " 580: '[unused575]',\n",
       " 581: '[unused576]',\n",
       " 582: '[unused577]',\n",
       " 583: '[unused578]',\n",
       " 584: '[unused579]',\n",
       " 585: '[unused580]',\n",
       " 586: '[unused581]',\n",
       " 587: '[unused582]',\n",
       " 588: '[unused583]',\n",
       " 589: '[unused584]',\n",
       " 590: '[unused585]',\n",
       " 591: '[unused586]',\n",
       " 592: '[unused587]',\n",
       " 593: '[unused588]',\n",
       " 594: '[unused589]',\n",
       " 595: '[unused590]',\n",
       " 596: '[unused591]',\n",
       " 597: '[unused592]',\n",
       " 598: '[unused593]',\n",
       " 599: '[unused594]',\n",
       " 600: '[unused595]',\n",
       " 601: '[unused596]',\n",
       " 602: '[unused597]',\n",
       " 603: '[unused598]',\n",
       " 604: '[unused599]',\n",
       " 605: '[unused600]',\n",
       " 606: '[unused601]',\n",
       " 607: '[unused602]',\n",
       " 608: '[unused603]',\n",
       " 609: '[unused604]',\n",
       " 610: '[unused605]',\n",
       " 611: '[unused606]',\n",
       " 612: '[unused607]',\n",
       " 613: '[unused608]',\n",
       " 614: '[unused609]',\n",
       " 615: '[unused610]',\n",
       " 616: '[unused611]',\n",
       " 617: '[unused612]',\n",
       " 618: '[unused613]',\n",
       " 619: '[unused614]',\n",
       " 620: '[unused615]',\n",
       " 621: '[unused616]',\n",
       " 622: '[unused617]',\n",
       " 623: '[unused618]',\n",
       " 624: '[unused619]',\n",
       " 625: '[unused620]',\n",
       " 626: '[unused621]',\n",
       " 627: '[unused622]',\n",
       " 628: '[unused623]',\n",
       " 629: '[unused624]',\n",
       " 630: '[unused625]',\n",
       " 631: '[unused626]',\n",
       " 632: '[unused627]',\n",
       " 633: '[unused628]',\n",
       " 634: '[unused629]',\n",
       " 635: '[unused630]',\n",
       " 636: '[unused631]',\n",
       " 637: '[unused632]',\n",
       " 638: '[unused633]',\n",
       " 639: '[unused634]',\n",
       " 640: '[unused635]',\n",
       " 641: '[unused636]',\n",
       " 642: '[unused637]',\n",
       " 643: '[unused638]',\n",
       " 644: '[unused639]',\n",
       " 645: '[unused640]',\n",
       " 646: '[unused641]',\n",
       " 647: '[unused642]',\n",
       " 648: '[unused643]',\n",
       " 649: '[unused644]',\n",
       " 650: '[unused645]',\n",
       " 651: '[unused646]',\n",
       " 652: '[unused647]',\n",
       " 653: '[unused648]',\n",
       " 654: '[unused649]',\n",
       " 655: '[unused650]',\n",
       " 656: '[unused651]',\n",
       " 657: '[unused652]',\n",
       " 658: '[unused653]',\n",
       " 659: '[unused654]',\n",
       " 660: '[unused655]',\n",
       " 661: '[unused656]',\n",
       " 662: '[unused657]',\n",
       " 663: '[unused658]',\n",
       " 664: '[unused659]',\n",
       " 665: '[unused660]',\n",
       " 666: '[unused661]',\n",
       " 667: '[unused662]',\n",
       " 668: '[unused663]',\n",
       " 669: '[unused664]',\n",
       " 670: '[unused665]',\n",
       " 671: '[unused666]',\n",
       " 672: '[unused667]',\n",
       " 673: '[unused668]',\n",
       " 674: '[unused669]',\n",
       " 675: '[unused670]',\n",
       " 676: '[unused671]',\n",
       " 677: '[unused672]',\n",
       " 678: '[unused673]',\n",
       " 679: '[unused674]',\n",
       " 680: '[unused675]',\n",
       " 681: '[unused676]',\n",
       " 682: '[unused677]',\n",
       " 683: '[unused678]',\n",
       " 684: '[unused679]',\n",
       " 685: '[unused680]',\n",
       " 686: '[unused681]',\n",
       " 687: '[unused682]',\n",
       " 688: '[unused683]',\n",
       " 689: '[unused684]',\n",
       " 690: '[unused685]',\n",
       " 691: '[unused686]',\n",
       " 692: '[unused687]',\n",
       " 693: '[unused688]',\n",
       " 694: '[unused689]',\n",
       " 695: '[unused690]',\n",
       " 696: '[unused691]',\n",
       " 697: '[unused692]',\n",
       " 698: '[unused693]',\n",
       " 699: '[unused694]',\n",
       " 700: '[unused695]',\n",
       " 701: '[unused696]',\n",
       " 702: '[unused697]',\n",
       " 703: '[unused698]',\n",
       " 704: '[unused699]',\n",
       " 705: '[unused700]',\n",
       " 706: '[unused701]',\n",
       " 707: '[unused702]',\n",
       " 708: '[unused703]',\n",
       " 709: '[unused704]',\n",
       " 710: '[unused705]',\n",
       " 711: '[unused706]',\n",
       " 712: '[unused707]',\n",
       " 713: '[unused708]',\n",
       " 714: '[unused709]',\n",
       " 715: '[unused710]',\n",
       " 716: '[unused711]',\n",
       " 717: '[unused712]',\n",
       " 718: '[unused713]',\n",
       " 719: '[unused714]',\n",
       " 720: '[unused715]',\n",
       " 721: '[unused716]',\n",
       " 722: '[unused717]',\n",
       " 723: '[unused718]',\n",
       " 724: '[unused719]',\n",
       " 725: '[unused720]',\n",
       " 726: '[unused721]',\n",
       " 727: '[unused722]',\n",
       " 728: '[unused723]',\n",
       " 729: '[unused724]',\n",
       " 730: '[unused725]',\n",
       " 731: '[unused726]',\n",
       " 732: '[unused727]',\n",
       " 733: '[unused728]',\n",
       " 734: '[unused729]',\n",
       " 735: '[unused730]',\n",
       " 736: '[unused731]',\n",
       " 737: '[unused732]',\n",
       " 738: '[unused733]',\n",
       " 739: '[unused734]',\n",
       " 740: '[unused735]',\n",
       " 741: '[unused736]',\n",
       " 742: '[unused737]',\n",
       " 743: '[unused738]',\n",
       " 744: '[unused739]',\n",
       " 745: '[unused740]',\n",
       " 746: '[unused741]',\n",
       " 747: '[unused742]',\n",
       " 748: '[unused743]',\n",
       " 749: '[unused744]',\n",
       " 750: '[unused745]',\n",
       " 751: '[unused746]',\n",
       " 752: '[unused747]',\n",
       " 753: '[unused748]',\n",
       " 754: '[unused749]',\n",
       " 755: '[unused750]',\n",
       " 756: '[unused751]',\n",
       " 757: '[unused752]',\n",
       " 758: '[unused753]',\n",
       " 759: '[unused754]',\n",
       " 760: '[unused755]',\n",
       " 761: '[unused756]',\n",
       " 762: '[unused757]',\n",
       " 763: '[unused758]',\n",
       " 764: '[unused759]',\n",
       " 765: '[unused760]',\n",
       " 766: '[unused761]',\n",
       " 767: '[unused762]',\n",
       " 768: '[unused763]',\n",
       " 769: '[unused764]',\n",
       " 770: '[unused765]',\n",
       " 771: '[unused766]',\n",
       " 772: '[unused767]',\n",
       " 773: '[unused768]',\n",
       " 774: '[unused769]',\n",
       " 775: '[unused770]',\n",
       " 776: '[unused771]',\n",
       " 777: '[unused772]',\n",
       " 778: '[unused773]',\n",
       " 779: '[unused774]',\n",
       " 780: '[unused775]',\n",
       " 781: '[unused776]',\n",
       " 782: '[unused777]',\n",
       " 783: '[unused778]',\n",
       " 784: '[unused779]',\n",
       " 785: '[unused780]',\n",
       " 786: '[unused781]',\n",
       " 787: '[unused782]',\n",
       " 788: '[unused783]',\n",
       " 789: '[unused784]',\n",
       " 790: '[unused785]',\n",
       " 791: '[unused786]',\n",
       " 792: '[unused787]',\n",
       " 793: '[unused788]',\n",
       " 794: '[unused789]',\n",
       " 795: '[unused790]',\n",
       " 796: '[unused791]',\n",
       " 797: '[unused792]',\n",
       " 798: '[unused793]',\n",
       " 799: '[unused794]',\n",
       " 800: '[unused795]',\n",
       " 801: '[unused796]',\n",
       " 802: '[unused797]',\n",
       " 803: '[unused798]',\n",
       " 804: '[unused799]',\n",
       " 805: '[unused800]',\n",
       " 806: '[unused801]',\n",
       " 807: '[unused802]',\n",
       " 808: '[unused803]',\n",
       " 809: '[unused804]',\n",
       " 810: '[unused805]',\n",
       " 811: '[unused806]',\n",
       " 812: '[unused807]',\n",
       " 813: '[unused808]',\n",
       " 814: '[unused809]',\n",
       " 815: '[unused810]',\n",
       " 816: '[unused811]',\n",
       " 817: '[unused812]',\n",
       " 818: '[unused813]',\n",
       " 819: '[unused814]',\n",
       " 820: '[unused815]',\n",
       " 821: '[unused816]',\n",
       " 822: '[unused817]',\n",
       " 823: '[unused818]',\n",
       " 824: '[unused819]',\n",
       " 825: '[unused820]',\n",
       " 826: '[unused821]',\n",
       " 827: '[unused822]',\n",
       " 828: '[unused823]',\n",
       " 829: '[unused824]',\n",
       " 830: '[unused825]',\n",
       " 831: '[unused826]',\n",
       " 832: '[unused827]',\n",
       " 833: '[unused828]',\n",
       " 834: '[unused829]',\n",
       " 835: '[unused830]',\n",
       " 836: '[unused831]',\n",
       " 837: '[unused832]',\n",
       " 838: '[unused833]',\n",
       " 839: '[unused834]',\n",
       " 840: '[unused835]',\n",
       " 841: '[unused836]',\n",
       " 842: '[unused837]',\n",
       " 843: '[unused838]',\n",
       " 844: '[unused839]',\n",
       " 845: '[unused840]',\n",
       " 846: '[unused841]',\n",
       " 847: '[unused842]',\n",
       " 848: '[unused843]',\n",
       " 849: '[unused844]',\n",
       " 850: '[unused845]',\n",
       " 851: '[unused846]',\n",
       " 852: '[unused847]',\n",
       " 853: '[unused848]',\n",
       " 854: '[unused849]',\n",
       " 855: '[unused850]',\n",
       " 856: '[unused851]',\n",
       " 857: '[unused852]',\n",
       " 858: '[unused853]',\n",
       " 859: '[unused854]',\n",
       " 860: '[unused855]',\n",
       " 861: '[unused856]',\n",
       " 862: '[unused857]',\n",
       " 863: '[unused858]',\n",
       " 864: '[unused859]',\n",
       " 865: '[unused860]',\n",
       " 866: '[unused861]',\n",
       " 867: '[unused862]',\n",
       " 868: '[unused863]',\n",
       " 869: '[unused864]',\n",
       " 870: '[unused865]',\n",
       " 871: '[unused866]',\n",
       " 872: '[unused867]',\n",
       " 873: '[unused868]',\n",
       " 874: '[unused869]',\n",
       " 875: '[unused870]',\n",
       " 876: '[unused871]',\n",
       " 877: '[unused872]',\n",
       " 878: '[unused873]',\n",
       " 879: '[unused874]',\n",
       " 880: '[unused875]',\n",
       " 881: '[unused876]',\n",
       " 882: '[unused877]',\n",
       " 883: '[unused878]',\n",
       " 884: '[unused879]',\n",
       " 885: '[unused880]',\n",
       " 886: '[unused881]',\n",
       " 887: '[unused882]',\n",
       " 888: '[unused883]',\n",
       " 889: '[unused884]',\n",
       " 890: '[unused885]',\n",
       " 891: '[unused886]',\n",
       " 892: '[unused887]',\n",
       " 893: '[unused888]',\n",
       " 894: '[unused889]',\n",
       " 895: '[unused890]',\n",
       " 896: '[unused891]',\n",
       " 897: '[unused892]',\n",
       " 898: '[unused893]',\n",
       " 899: '[unused894]',\n",
       " 900: '[unused895]',\n",
       " 901: '[unused896]',\n",
       " 902: '[unused897]',\n",
       " 903: '[unused898]',\n",
       " 904: '[unused899]',\n",
       " 905: '[unused900]',\n",
       " 906: '[unused901]',\n",
       " 907: '[unused902]',\n",
       " 908: '[unused903]',\n",
       " 909: '[unused904]',\n",
       " 910: '[unused905]',\n",
       " 911: '[unused906]',\n",
       " 912: '[unused907]',\n",
       " 913: '[unused908]',\n",
       " 914: '[unused909]',\n",
       " 915: '[unused910]',\n",
       " 916: '[unused911]',\n",
       " 917: '[unused912]',\n",
       " 918: '[unused913]',\n",
       " 919: '[unused914]',\n",
       " 920: '[unused915]',\n",
       " 921: '[unused916]',\n",
       " 922: '[unused917]',\n",
       " 923: '[unused918]',\n",
       " 924: '[unused919]',\n",
       " 925: '[unused920]',\n",
       " 926: '[unused921]',\n",
       " 927: '[unused922]',\n",
       " 928: '[unused923]',\n",
       " 929: '[unused924]',\n",
       " 930: '[unused925]',\n",
       " 931: '[unused926]',\n",
       " 932: '[unused927]',\n",
       " 933: '[unused928]',\n",
       " 934: '[unused929]',\n",
       " 935: '[unused930]',\n",
       " 936: '[unused931]',\n",
       " 937: '[unused932]',\n",
       " 938: '[unused933]',\n",
       " 939: '[unused934]',\n",
       " 940: '[unused935]',\n",
       " 941: '[unused936]',\n",
       " 942: '[unused937]',\n",
       " 943: '[unused938]',\n",
       " 944: '[unused939]',\n",
       " 945: '[unused940]',\n",
       " 946: '[unused941]',\n",
       " 947: '[unused942]',\n",
       " 948: '[unused943]',\n",
       " 949: '[unused944]',\n",
       " 950: '[unused945]',\n",
       " 951: '[unused946]',\n",
       " 952: '[unused947]',\n",
       " 953: '[unused948]',\n",
       " 954: '[unused949]',\n",
       " 955: '[unused950]',\n",
       " 956: '[unused951]',\n",
       " 957: '[unused952]',\n",
       " 958: '[unused953]',\n",
       " 959: '[unused954]',\n",
       " 960: '[unused955]',\n",
       " 961: '[unused956]',\n",
       " 962: '[unused957]',\n",
       " 963: '[unused958]',\n",
       " 964: '[unused959]',\n",
       " 965: '[unused960]',\n",
       " 966: '[unused961]',\n",
       " 967: '[unused962]',\n",
       " 968: '[unused963]',\n",
       " 969: '[unused964]',\n",
       " 970: '[unused965]',\n",
       " 971: '[unused966]',\n",
       " 972: '[unused967]',\n",
       " 973: '[unused968]',\n",
       " 974: '[unused969]',\n",
       " 975: '[unused970]',\n",
       " 976: '[unused971]',\n",
       " 977: '[unused972]',\n",
       " 978: '[unused973]',\n",
       " 979: '[unused974]',\n",
       " 980: '[unused975]',\n",
       " 981: '[unused976]',\n",
       " 982: '[unused977]',\n",
       " 983: '[unused978]',\n",
       " 984: '[unused979]',\n",
       " 985: '[unused980]',\n",
       " 986: '[unused981]',\n",
       " 987: '[unused982]',\n",
       " 988: '[unused983]',\n",
       " 989: '[unused984]',\n",
       " 990: '[unused985]',\n",
       " 991: '[unused986]',\n",
       " 992: '[unused987]',\n",
       " 993: '[unused988]',\n",
       " 994: '[unused989]',\n",
       " 995: '[unused990]',\n",
       " 996: '[unused991]',\n",
       " 997: '[unused992]',\n",
       " 998: '[unused993]',\n",
       " 999: '!',\n",
       " ...}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "featured-texas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] this movie can be labeled as a study case . it s not just the fact that it denotes an un ##hea ##lth ##y and non artistic lust for anything that might be termed as ca ##co imagery . the author lives with the impression that his san ##ct ##imo ##nio ##us revolt against some generic and childish ##ly termed social ill ##s mold ##avia is the most pau ##per region of europe i don t believe one io ##ta in the birds flu romanian people steal because they are poor europeans steal because they are thieves are more or less close to a responsible moral and artistic attitude but he is sore ##ly off target br br what dane ##li ##uc doesn t know is that it s not enough to pose as a righteous person you also need a mod ##icum of professional ##ism talent and intelligence to trans ##pose this stance into an artistic product . fate ##fully the foreign legion shows as much ac ##umen as a family video with uncle go ##gu drunken ##ly wet ##ting himself in front of the guests . the script is chaotic and inc ##oh ##ere ##nt randomly [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(*[reverse_vocab[token_id] for token_id in txt_data[rnd_review]['input_ids'][0].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "patent-maintenance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this movie can be labeled as a study case . it  s not just the fact that it denotes an unhealthy and non  artistic lust for anything that might be termed as caco  imagery . the author lives with the impression that his sanctimonious revolt against some generic and childishly termed social ills   moldavia is the most pauper region of europe    i don  t believe one iota in the birds flu    romanian people steal because they are poor europeans steal because they are thieves   are more or less close to a responsible moral and artistic attitude  but he is sorely off  target   br    br   what daneliuc doesn  t know  is that it  s not enough to pose as a righteous person  you also need a modicum of professionalism  talent and intelligence to transpose this stance into an artistic product . fatefully   the foreign legion  shows as much acumen as a family video with uncle gogu drunkenly wetting himself in front of the guests . the script is chaotic and incoherent  randomly bustling together sundry half  subjects  in an illiterate attempt to suggest some kind of a story . the direction is pathetically dilettante  the so  called  director  is unable to build up at least a mediocre mise  en  scene  his shots are annoyingly awkward  and any sense of storytelling shines by total absence .  of course  any comment is forced to stop at this level it would be ridiculous to mention concepts as  cinematographic language    means of expression  or  style   . the acting is positively  cntarea romniei    romania  s chant   level  with the exception of . . . paradoxically  the soccer goal  keeper necula raducanu  who is very natural  and nicodim ungureanu . oana piecnita seems to have a genuine freshness  but she is compromised by the amateurish directions given by daneliuc .  br    br   the most serious side of this offense to decent cinema is the fact that the production received a hefty financing from the national budget  via c . n . c .  the national cinematography council  . the fact that long  time  dead old dinosaurs like daneliuc are still thirsty for the government udder is understandable  in a market  driven economy  they would be instantly eliminated through natural selection  . but the corruption of the so  called  jury  that squanders the country  s money on such ridiculously scabrous non  art  non  cinema and non  culture belongs to the criminal field .  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[rnd_review])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-wisdom",
   "metadata": {},
   "source": [
    "#### DataLoader and train/val/test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "cutting-scratch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20001, 22500)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train, test, validation split\n",
    "split_frac = 0.8\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(data)*split_frac) + 1\n",
    "test_val_idx = split_idx + int((len(data) - split_idx)//2)\n",
    "split_idx, test_val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "excessive-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextData(labels[:split_idx], data[:split_idx])\n",
    "val_dataset = TextData(labels[split_idx:test_val_idx], data[split_idx:test_val_idx])\n",
    "test_dataset = TextData(labels[test_val_idx:], data[test_val_idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "complimentary-facial",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "итого 708K\n",
      "-rw-r--r-- 1 testuser testuser   28 июн 15 16:03 8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "-rw-r--r-- 1 testuser testuser  148 июн 15 16:03 8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.json\n",
      "-rwxr-xr-x 1 testuser testuser    0 июн 15 16:03 8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "-rw-r--r-- 1 testuser testuser 456K июн 15 16:03 75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "-rw-r--r-- 1 testuser testuser  141 июн 15 16:03 75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.json\n",
      "-rwxr-xr-x 1 testuser testuser    0 июн 15 16:03 75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "-rw-r--r-- 1 testuser testuser 227K июн 15 16:03 0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "-rw-r--r-- 1 testuser testuser  136 июн 15 16:03 0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.json\n",
      "-rwxr-xr-x 1 testuser testuser    0 июн 15 16:03 0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "-rw-r--r-- 1 testuser testuser  442 июн 15 16:03 23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
      "-rw-r--r-- 1 testuser testuser  138 июн 15 16:03 23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.json\n",
      "-rwxr-xr-x 1 testuser testuser    0 июн 15 16:03 23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\n"
     ]
    }
   ],
   "source": [
    "!ls -lth /home/testuser/.cache/huggingface/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "manual-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = {'train': DataLoader(train_dataset,batch_size=BATCH_SIZE, shuffle=True), \n",
    "               'valid': DataLoader(val_dataset,batch_size=BATCH_SIZE, shuffle=True), \n",
    "               'test': DataLoader(test_dataset,batch_size=BATCH_SIZE, shuffle=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "decreased-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = next(iter(data_loader['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "higher-ozone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  2003,  1037,  2200,  3697,  3185,  1998,  2009,  1055,\n",
       "           2471,  5263,  2000,  2131,  1037,  5047,  2006,  2054,  1055,  2183,\n",
       "           2006,  1012,  2012,  2034,  2009,  3849,  2000,  2022,  1037,  2738,\n",
       "          14662,  3185,  2055,  1037,  3124, 29461, 13687, 21983,  2040,  3791,\n",
       "           2019,  4545,  1998,  2738, 13675, 12054,  2135, 18675,  2370,  2046,\n",
       "           2028,  2043,  1996,  2783, 16713,  1037,  2450, 27791,  5920,  1012,\n",
       "           2059,  1996, 21438,  1998,  4332,  2707,  1012,  2024,  1996, 10638,\n",
       "           2667,  2000,  3102,  2032,  1998,  2339,  2024,  1996,  2757, 16713,\n",
       "           1055,  4253,  3810,  2039,  1999,  1996,  4545,  2028, 16278,  2633,\n",
       "           2065, 29461, 13687,  4492,  5705,  2100,  2003,  1996,  3188, 16713,\n",
       "           1012,  7987,  7987, 27594,  2121, 27594,  2121, 27594,  2121, 27594,\n",
       "           2121, 27594,  2121, 27594,  2121,  7987,  7987,  2028,  1997,  1996,\n",
       "          12225, 14955,  6962,  3211,  8005,  2006,  2149,  2003,  2000,  4682,\n",
       "           2000,  2149,  1012,  2057,  7868,  2043,  2057,  2156,  2477,  2013,\n",
       "           1996,  2391,  1997,  3193,  1997,  1037,  2839,  2008,  2057,  2156,\n",
       "           2477,  2004,  1996,  2839,  2515,  1998,  2008,  2045,  2089,  2022,\n",
       "          20870,  2015,  1997,  4507,  1012,  2057,  7868,  2043,  1996,  4950,\n",
       "           2003,  4760,  2149,  2477,  2013,  2049, 18168,  8977, 23402,  3372,\n",
       "           2391,  1997,  3193,  2008,  2057,  2156,  5025,  3012,  2021, 14955,\n",
       "           6962,  3211,  2038,  1996,  4950,  4682,  2000,  2149,  1012,   102],\n",
       "         [  101, 19356,  7987,  7987,  7814,  6463,  1012,  7987,  7987,  2614,\n",
       "           4289, 12991,  7987,  7987,  7065,  7869,  3372,  2295,  8040, 21531,\n",
       "          21227,  4189,  4070,  1997,  1996,  2166,  1998,  2335,  1997,  3410,\n",
       "          10423,  3505, 19356,  1012,  2445,  2010, 10652,  2006,  1037,  9040,\n",
       "           3715,  1996,  2143,  2003,  6176,  2025,  2000, 17279,  2032,  2004,\n",
       "           5394,  2030, 12700,  2021, 23262,  1037,  2162,  3215,  1998,  2035,\n",
       "           6533,  1997,  2010,  4125,  2000,  4476,  1996, 15399,  1997,  3112,\n",
       "           1998,  1996,  2111,  2040,  5044,  2010, 10461,  2005,  2204,  2030,\n",
       "           2919,  1012, 27570,  2011,  2051,  6537,  1996,  5896, 25624,  4523,\n",
       "           2083,  1037,  8338,  1997,  7882,  4751,  4862, 13900,  2075,  4780,\n",
       "           2006,  3278,  2824, 16979,  2007, 19356,  1055,  5741, 22252,  1999,\n",
       "           1012, 23131,  2015,  2097,  2022,  4372,  7138,  6675,  2011,  1996,\n",
       "          17873,  2096,  8362,  4599,  2097,  2022, 21474,  2011,  2472, 17359,\n",
       "           2072,  3968,  2884,  1055,  3442,  8612,  3921,  2000,  1996,  3430,\n",
       "           1012,  2002, 17509, 19356,  1055,  2166,  2004,  1037,  9661,  1999,\n",
       "           2029,  2002,  2001,  4821,  2419,  2004, 28473,  2011,  1996,  6214,\n",
       "           1997,  2010,  2219,  3112,  1012,  1999,  2755,  1996,  5896,  8269,\n",
       "           2087,  1997,  2049, 15779,  2005, 19356,  1055,  4654,  2564,  5863,\n",
       "           2445,  2015,  2839,  9355,  2014,  2004,  2019,  4895, 22780,  3993,\n",
       "           2751, 28661,  2040,  2165,  5056,  1997,  2010, 15743,  3723,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'targets': tensor([1, 0])}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "mathematical-express",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20001, 2499, 2500)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-delaware",
   "metadata": {},
   "source": [
    "## Define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-sodium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-guidance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "balanced-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_dataset = TextData(labels[:split_idx-1], data[:split_idx-1])\n",
    "valid_dataset = TextData(labels[split_idx-1:test_val_idx], data[split_idx-1:test_val_idx])\n",
    "test_dataset = TextData(labels[test_val_idx:], data[test_val_idx:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "better-philadelphia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 2500, 20000)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset), len(valid_dataset), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "drawn-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader =  DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=tokenize_batch)\n",
    "valid_loader =  DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=tokenize_batch)\n",
    "test_loader =  DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=tokenize_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-drawing",
   "metadata": {},
   "source": [
    "## Make a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "forty-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#         Input shape     torch.Size([2, 200])\n",
    "#         Embedding shape torch.Size([2, 200, 32]) 50\n",
    "#         Conv1d shape    torch.Size([2, 32, 30]) 48\n",
    "#         MaxPool shape   torch.Size([2, 32, 15]) 24\n",
    "#         LSTM shape      torch.Size([2, 32, 64])\n",
    "#         Dense shape     torch.Size([2, 32, 1])\n",
    "#         Sigmoid shape   torch.Size([2])\n",
    "\n",
    "class SentimentConvNN(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size=1, embedding_dim=32, hidden_dim=64, out_channels=32, drop_prob=0.5, vocab_vectors=None):\n",
    "        super(SentimentConvNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_size = output_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.drop_prob = drop_prob\n",
    "        self.out_channels = out_channels\n",
    "        self.n_layers = 1\n",
    "        \n",
    "        # if we provide vocab_vectors then initialize weights\n",
    "        if vocab_vectors is not None:\n",
    "            self.embed = nn.Embedding.from_pretrained(vocab_vectors, freeze=True)\n",
    "        else:\n",
    "            self.embed = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "            \n",
    "        self.conv1d = nn.Conv1d(in_channels=200, out_channels=self.out_channels, kernel_size=3, bias=False, padding=False)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        lstm_input = int((self.embed.weight.shape[-1]-2)/2) # repoduce the logic of conv1d resulting dimention\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=lstm_input,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            dropout=0)\n",
    "        self.dense = nn.Linear(hidden_dim, 1)\n",
    "        self.drop = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        self.bn_embedding = nn.BatchNorm1d(num_features=200)\n",
    "        self.bn_conv1d = nn.BatchNorm1d(num_features=self.out_channels)\n",
    "        self.bn_lstm = nn.BatchNorm1d(num_features=self.out_channels)\n",
    "    \n",
    "    def num_parameters(self):\n",
    "        '''\n",
    "        get the number of parameters in a network\n",
    "        '''\n",
    "\n",
    "        # return sum((list(map(lambda x: torch.as_tensor(x.flatten().size()).sum().item(), self.parameters()))))\n",
    "        s=\"\"\n",
    "        for k, v in self.named_parameters():\n",
    "            s+=f'{k:20} {v.shape}\\n'\n",
    "        s+=f'Total number of parameters = {sum(list(map(lambda x: x.numel(), self.parameters()))):,}'\n",
    "        return s\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new_zeros(self.n_layers, batch_size, self.hidden_dim),\n",
    "                  weight.new_zeros(self.n_layers, batch_size, self.hidden_dim))\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        '''\n",
    "        Input shape \ttorch.Size([2, 200])\n",
    "        Embedding shape torch.Size([2, 200, 32])\n",
    "        Conv1d shape \ttorch.Size([2, 32, 30])\n",
    "        MaxPool shape \ttorch.Size([2, 32, 15])\n",
    "        LSTM shape \t\ttorch.Size([2, 32, 64])\n",
    "        Dense shape \ttorch.Size([2, 32, 1])\n",
    "        Sigmoid shape \ttorch.Size([2])\n",
    "        '''\n",
    "        #print(x.dtype)\n",
    "        embed_out = self.embed(x)\n",
    "        embed_out = self.bn_embedding(embed_out)\n",
    "        embed_out = self.drop(embed_out)\n",
    "        \n",
    "        conv_out = self.conv1d(embed_out)\n",
    "        conv_out = self.bn_conv1d(conv_out)\n",
    "        conv_out_relu = F.relu(conv_out)\n",
    "        maxpool_out = self.maxpool(conv_out_relu)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(maxpool_out, hidden)\n",
    "        lstm_out = self.bn_lstm(lstm_out)\n",
    "        lstm_out = self.drop(lstm_out)\n",
    "        \n",
    "        out_dense = self.dense(lstm_out)\n",
    "        out = nn.Sigmoid()(out_dense[:,-1,:]).view(out_dense.shape[0])\n",
    "        \n",
    "        return out, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "sealed-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparams\n",
    "vocab_size = len(vocab) \n",
    "output_size = 1 # not needed\n",
    "embedding_dim = 32\n",
    "hidden_dim = 64\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "civilian-understanding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.weight         torch.Size([67359, 32])\n",
      "conv1d.weight        torch.Size([32, 200, 3])\n",
      "lstm.weight_ih_l0    torch.Size([256, 15])\n",
      "lstm.weight_hh_l0    torch.Size([256, 64])\n",
      "lstm.bias_ih_l0      torch.Size([256])\n",
      "lstm.bias_hh_l0      torch.Size([256])\n",
      "dense.weight         torch.Size([1, 64])\n",
      "dense.bias           torch.Size([1])\n",
      "bn_embedding.weight  torch.Size([200])\n",
      "bn_embedding.bias    torch.Size([200])\n",
      "bn_conv1d.weight     torch.Size([32])\n",
      "bn_conv1d.bias       torch.Size([32])\n",
      "bn_lstm.weight       torch.Size([32])\n",
      "bn_lstm.bias         torch.Size([32])\n",
      "Total number of parameters = 2,196,017\n"
     ]
    }
   ],
   "source": [
    "convRNN = SentimentConvNN(vocab_size=vocab_size)\n",
    "print(convRNN.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fantastic-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = convRNN.init_hidden(batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "exclusive-worcester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1950, 0.5091]), torch.Size([2]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out, _ = convRNN.forward(txt, h0)\n",
    "out, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "detailed-kentucky",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentConvNN(\n",
       "  (embed): Embedding(67359, 32)\n",
       "  (conv1d): Conv1d(200, 32, kernel_size=(3,), stride=(1,), bias=False)\n",
       "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (lstm): LSTM(15, 64, batch_first=True)\n",
       "  (dense): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (bn_embedding): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_conv1d): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_lstm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "convRNN.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-missile",
   "metadata": {},
   "source": [
    "### Training conv RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "legitimate-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_score(net, val_loader, criterion):\n",
    "    '''\n",
    "    calculates validation loss\n",
    "    does not put a net into eval mode - have to do this manually before val_score call\n",
    "    \n",
    "    '''\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    loss = []\n",
    "    # that is the number of objects in loader, not batches\n",
    "    number_of_objects = len(val_loader.dataset)\n",
    "    \n",
    "    # make array of zeros with the shape of response\n",
    "    pred_y = np.zeros(number_of_objects)\n",
    "    true_y = np.zeros_like(pred_y)\n",
    "\n",
    "    # store a batch size \n",
    "    batch_size = val_loader.batch_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ii, (test_x, test_y) in enumerate(val_loader):\n",
    "            h = net.init_hidden(test_y.shape[0])\n",
    "            test_x, test_y = test_x.to(device), test_y.to(device)\n",
    "            out, _ = net.forward(test_x, h)\n",
    "            batch_loss = criterion(out, test_y)\n",
    "            \n",
    "            # store predictions and true labels\n",
    "            pred_y[ii*batch_size:ii*batch_size + len(test_y)] = out.to('cpu').numpy()\n",
    "            true_y[ii*batch_size:ii*batch_size + len(test_y)] = test_y.to('cpu').numpy()\n",
    "            \n",
    "            loss.append(batch_loss.item())\n",
    "    \n",
    "    precision = precision_score(true_y, np.round(pred_y))\n",
    "    recall = recall_score(true_y, np.round(pred_y))\n",
    "    accuracy = accuracy_score(true_y, np.round(pred_y))\n",
    "    fscore = f1_score(true_y, np.round(pred_y))\n",
    "    \n",
    "    metrics = {'precision':precision, 'recall':recall, 'accuracy':accuracy, 'fscore':fscore}\n",
    "    \n",
    "    return np.mean(loss), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "alert-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_SWA(net, criterion, optimizer, train_loader, valid_loader, clip_value=5, epochs=10, print_every=200, max_fscore=-np.inf):\n",
    "    '''\n",
    "    Train the network\n",
    "        net - network to trian\n",
    "        criterion - loss function \n",
    "        optimizer - your optimiser of choice \n",
    "        train_loader - loader for training data\n",
    "        vlid_loader - lodaer for validation/test data\n",
    "        clip_value - upper limit for gradient \n",
    "        epochs - number of epochs to train the net\n",
    "        print_every - prin stats every number of batches\n",
    "        max_fscore - best fscore on validation set - used in mutiple runs of training\n",
    "    '''\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "    \n",
    "    steps = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    # run over epochs of training\n",
    "    for e in trange(epochs):\n",
    "        \n",
    "        # array to keep value of losses over current epoch\n",
    "        train_loss = []\n",
    "\n",
    "        # run one pass through training samples = one epoch\n",
    "        for train_x, train_y in train_loader:\n",
    "            steps +=1\n",
    "\n",
    "            # zero out the grads \n",
    "            net.zero_grad()\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            # send data to device\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "\n",
    "            # initialize hidden state\n",
    "            h = net.init_hidden(len(train_x))\n",
    "\n",
    "            # calculate the output of the network\n",
    "            out, _ = net(train_x, h)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = criterion(out, train_y)\n",
    "            # backprop grads of the loss wrt to net parameters\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip_value)\n",
    "\n",
    "            # upadate parameters of network\n",
    "            optimizer.step()\n",
    "\n",
    "            # append current batch loss (loss per object in current batch)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            # test loss calc every \n",
    "            if steps%print_every == 0:\n",
    "                # calculate loss on test set\n",
    "                if max_fscore > 0.75:\n",
    "                    optimizer.update_swa()\n",
    "                    optimizer.swap_swa_sgd() # use SWA weights for the calc of validation loss\n",
    "                    for g in optimizer.param_groups:\n",
    "                        g['lr'] = 0.0005\n",
    "                net.eval()\n",
    "                test_loss, metrics = val_score(net, valid_loader, criterion)\n",
    "                if metrics['fscore'] > max_fscore:\n",
    "                    max_fscore = metrics['fscore']\n",
    "                    message = '=)'\n",
    "                    check_point = {'vocab_size':net.vocab_size, \n",
    "                                   'embedding_dim': net.embedding_dim, \n",
    "                                   'hidden_dim':net.hidden_dim, \n",
    "                                   'n_layers':net.n_layers, \n",
    "                                   'net_params':net.state_dict()}\n",
    "                    torch.save(check_point, f\"spam_model_fscore_{metrics['fscore']:.3f}.pt\")\n",
    "                else:\n",
    "                    message = ';('\n",
    "                if max_fscore > 0.75:\n",
    "                    optimizer.swap_swa_sgd() # swap back normal weights and continue training\n",
    "                net.train()\n",
    "                print(f\"Step {steps} epoch {e+1}. {message}\\nTest loss is {test_loss:.4f}. Train loss is {np.mean(train_loss):.4f}.\\\n",
    "                F1 Score={metrics['fscore']:.2%} Precision={metrics['precision']:.2%} Recall={metrics['recall']:.2%} Accuracy={metrics['accuracy']:.2%}\\n\")\n",
    "    return max_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "educational-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fscore = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "confirmed-marijuana",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SWA in module torchcontrib.optim.swa:\n",
      "\n",
      "class SWA(torch.optim.optimizer.Optimizer)\n",
      " |  Base class for all optimizers.\n",
      " |  \n",
      " |  .. warning::\n",
      " |      Parameters need to be specified as collections that have a deterministic\n",
      " |      ordering that is consistent between runs. Examples of objects that don't\n",
      " |      satisfy those properties are sets and iterators over values of dictionaries.\n",
      " |  \n",
      " |  Args:\n",
      " |      params (iterable): an iterable of :class:`torch.Tensor` s or\n",
      " |          :class:`dict` s. Specifies what Tensors should be optimized.\n",
      " |      defaults: (dict): a dict containing default values of optimization\n",
      " |          options (used when a parameter group doesn't specify them).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SWA\n",
      " |      torch.optim.optimizer.Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, optimizer, swa_start=None, swa_freq=None, swa_lr=None)\n",
      " |      Implements Stochastic Weight Averaging (SWA).\n",
      " |      \n",
      " |      Stochastic Weight Averaging was proposed in `Averaging Weights Leads to\n",
      " |      Wider Optima and Better Generalization`_ by Pavel Izmailov, Dmitrii\n",
      " |      Podoprikhin, Timur Garipov, Dmitry Vetrov and Andrew Gordon Wilson\n",
      " |      (UAI 2018).\n",
      " |      \n",
      " |      SWA is implemented as a wrapper class taking optimizer instance as input\n",
      " |      and applying SWA on top of that optimizer.\n",
      " |      \n",
      " |      SWA can be used in two modes: automatic and manual. In the automatic\n",
      " |      mode SWA running averages are automatically updated every\n",
      " |      :attr:`swa_freq` steps after :attr:`swa_start` steps of optimization. If\n",
      " |      :attr:`swa_lr` is provided, the learning rate of the optimizer is reset\n",
      " |      to :attr:`swa_lr` at every step starting from :attr:`swa_start`. To use\n",
      " |      SWA in automatic mode provide values for both :attr:`swa_start` and\n",
      " |      :attr:`swa_freq` arguments.\n",
      " |      \n",
      " |      Alternatively, in the manual mode, use :meth:`update_swa` or\n",
      " |      :meth:`update_swa_group` methods to update the SWA running averages.\n",
      " |      \n",
      " |      In the end of training use `swap_swa_sgd` method to set the optimized\n",
      " |      variables to the computed averages.\n",
      " |      \n",
      " |      Args:\n",
      " |          optimizer (torch.optim.Optimizer): optimizer to use with SWA\n",
      " |          swa_start (int): number of steps before starting to apply SWA in\n",
      " |              automatic mode; if None, manual mode is selected (default: None)\n",
      " |          swa_freq (int): number of steps between subsequent updates of\n",
      " |              SWA running averages in automatic mode; if None, manual mode is\n",
      " |              selected (default: None)\n",
      " |          swa_lr (float): learning rate to use starting from step swa_start\n",
      " |              in automatic mode; if None, learning rate is not changed\n",
      " |              (default: None)\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> # automatic mode\n",
      " |          >>> base_opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
      " |          >>> opt = torchcontrib.optim.SWA(\n",
      " |          >>>                 base_opt, swa_start=10, swa_freq=5, swa_lr=0.05)\n",
      " |          >>> for _ in range(100):\n",
      " |          >>>     opt.zero_grad()\n",
      " |          >>>     loss_fn(model(input), target).backward()\n",
      " |          >>>     opt.step()\n",
      " |          >>> opt.swap_swa_sgd()\n",
      " |          >>> # manual mode\n",
      " |          >>> opt = torchcontrib.optim.SWA(base_opt)\n",
      " |          >>> for i in range(100):\n",
      " |          >>>     opt.zero_grad()\n",
      " |          >>>     loss_fn(model(input), target).backward()\n",
      " |          >>>     opt.step()\n",
      " |          >>>     if i > 10 and i % 5 == 0:\n",
      " |          >>>         opt.update_swa()\n",
      " |          >>> opt.swap_swa_sgd()\n",
      " |      \n",
      " |      .. note::\n",
      " |          SWA does not support parameter-specific values of :attr:`swa_start`,\n",
      " |          :attr:`swa_freq` or :attr:`swa_lr`. In automatic mode SWA uses the\n",
      " |          same :attr:`swa_start`, :attr:`swa_freq` and :attr:`swa_lr` for all\n",
      " |          parameter groups. If needed, use manual mode with\n",
      " |          :meth:`update_swa_group` to use different update schedules for\n",
      " |          different parameter groups.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Call :meth:`swap_swa_sgd` in the end of training to use the computed\n",
      " |          running averages.\n",
      " |      \n",
      " |      .. note::\n",
      " |          If you are using SWA to optimize the parameters of a Neural Network\n",
      " |          containing Batch Normalization layers, you need to update the\n",
      " |          :attr:`running_mean` and :attr:`running_var` statistics of the\n",
      " |          Batch Normalization module. You can do so by using\n",
      " |          `torchcontrib.optim.swa.bn_update` utility.\n",
      " |      \n",
      " |      .. _Averaging Weights Leads to Wider Optima and Better Generalization:\n",
      " |          https://arxiv.org/abs/1803.05407\n",
      " |      .. _Improving Consistency-Based Semi-Supervised Learning with Weight\n",
      " |          Averaging:\n",
      " |          https://arxiv.org/abs/1806.05594\n",
      " |  \n",
      " |  add_param_group(self, param_group)\n",
      " |      Add a param group to the :class:`Optimizer` s `param_groups`.\n",
      " |      \n",
      " |      This can be useful when fine tuning a pre-trained network as frozen\n",
      " |      layers can be made trainable and added to the :class:`Optimizer` as\n",
      " |      training progresses.\n",
      " |      \n",
      " |      Args:\n",
      " |          param_group (dict): Specifies what Tensors should be optimized along\n",
      " |          with group specific optimization options.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Loads the optimizer state.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): SWA optimizer state. Should be an object returned\n",
      " |              from a call to `state_dict`.\n",
      " |  \n",
      " |  state_dict(self)\n",
      " |      Returns the state of SWA as a :class:`dict`.\n",
      " |      \n",
      " |      It contains three entries:\n",
      " |          * opt_state - a dict holding current optimization state of the base\n",
      " |              optimizer. Its content differs between optimizer classes.\n",
      " |          * swa_state - a dict containing current state of SWA. For each\n",
      " |              optimized variable it contains swa_buffer keeping the running\n",
      " |              average of the variable\n",
      " |          * param_groups - a dict containing all parameter groups\n",
      " |  \n",
      " |  step(self, closure=None)\n",
      " |      Performs a single optimization step.\n",
      " |      \n",
      " |      In automatic mode also updates SWA running averages.\n",
      " |  \n",
      " |  swap_swa_sgd(self)\n",
      " |      Swaps the values of the optimized variables and swa buffers.\n",
      " |      \n",
      " |      It's meant to be called in the end of training to use the collected\n",
      " |      swa running averages. It can also be used to evaluate the running\n",
      " |      averages during training; to continue training `swap_swa_sgd`\n",
      " |      should be called again.\n",
      " |  \n",
      " |  update_swa(self)\n",
      " |      Updates the SWA running averages of all optimized parameters.\n",
      " |  \n",
      " |  update_swa_group(self, group)\n",
      " |      Updates the SWA running averages for the given parameter group.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          param_group (dict): Specifies for what parameter group SWA running\n",
      " |              averages should be updated\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> # automatic mode\n",
      " |          >>> base_opt = torch.optim.SGD([{'params': [x]},\n",
      " |          >>>             {'params': [y], 'lr': 1e-3}], lr=1e-2, momentum=0.9)\n",
      " |          >>> opt = torchcontrib.optim.SWA(base_opt)\n",
      " |          >>> for i in range(100):\n",
      " |          >>>     opt.zero_grad()\n",
      " |          >>>     loss_fn(model(input), target).backward()\n",
      " |          >>>     opt.step()\n",
      " |          >>>     if i > 10 and i % 5 == 0:\n",
      " |          >>>         # Update SWA for the second parameter group\n",
      " |          >>>         opt.update_swa_group(opt.param_groups[1])\n",
      " |          >>> opt.swap_swa_sgd()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  bn_update(loader, model, device=None)\n",
      " |      Updates BatchNorm running_mean, running_var buffers in the model.\n",
      " |      \n",
      " |      It performs one pass over data in `loader` to estimate the activation\n",
      " |      statistics for BatchNorm layers in the model.\n",
      " |      \n",
      " |      Args:\n",
      " |          loader (torch.utils.data.DataLoader): dataset loader to compute the\n",
      " |              activation statistics on. Each data batch should be either a\n",
      " |              tensor, or a list/tuple whose first element is a tensor\n",
      " |              containing data.\n",
      " |      \n",
      " |          model (torch.nn.Module): model for which we seek to update BatchNorm\n",
      " |              statistics.\n",
      " |      \n",
      " |          device (torch.device, optional): If set, data will be trasferred to\n",
      " |              :attr:`device` before being passed into :attr:`model`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  zero_grad(self, set_to_none:bool=False)\n",
      " |      Sets the gradients of all optimized :class:`torch.Tensor` s to zero.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              This will in general have lower memory footprint, and can modestly improve performance.\n",
      " |              However, it changes certain behaviors. For example:\n",
      " |              1. When the user tries to access a gradient and perform manual ops on it,\n",
      " |              a None attribute or a Tensor full of 0s will behave differently.\n",
      " |              2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s\n",
      " |              are guaranteed to be None for params that did not receive a gradient.\n",
      " |              3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n",
      " |              (in one case it does the step with a gradient of 0 and in the other it skips\n",
      " |              the step altogether).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SWA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "minimal-least",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vocab_size', 'embedding_dim', 'hidden_dim', 'n_layers', 'net_params'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('spam_model_fscore_0.843.pt')\n",
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "capital-overhead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convRNN.load_state_dict(model['net_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "frequent-valuation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(convRNN.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "# opt = torchcontrib.optim.SWA(\n",
    "#                 base_opt, swa_start=10, swa_freq=5, swa_lr=0.05)\n",
    "# for _ in range(100):\n",
    "#     opt.zero_grad()\n",
    "#     loss_fn(model(input), target).backward()\n",
    "#     opt.step()\n",
    "# opt.swap_swa_sgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "elementary-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "criterion = nn.BCELoss(reduction='mean')\n",
    "base_opt = torch.optim.Adam(convRNN.parameters(), lr=lr, weight_decay=0.005)\n",
    "#optimizer = torch.optim.SGD(convRNN.parameters(), lr=lr, momentum=0.9, nesterov=True)\n",
    "#optimizer = torch.optim.RMSprop(convRNN.parameters(), lr=lr, momentum=0.9)\n",
    "lr = 0.001\n",
    "#base_opt = torch.optim.SGD(convRNN.parameters(), lr=lr)\n",
    "optimizer = SWA(base_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "broken-pleasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    print(g['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "pleased-detroit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e24a71fc1f9485890973e65b0843f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 epoch 1. ;(\n",
      "Test loss is 0.8857. Train loss is 0.1112.                F1 Score=84.88% Precision=86.99% Recall=82.88% Accuracy=85.24%\n",
      "\n",
      "Step 400 epoch 1. ;(\n",
      "Test loss is 0.9245. Train loss is 0.1272.                F1 Score=84.68% Precision=87.27% Recall=82.24% Accuracy=85.12%\n",
      "\n",
      "Step 600 epoch 2. ;(\n",
      "Test loss is 0.9929. Train loss is 0.1085.                F1 Score=84.89% Precision=86.73% Recall=83.12% Accuracy=85.20%\n",
      "\n",
      "Step 800 epoch 2. ;(\n",
      "Test loss is 1.0517. Train loss is 0.1230.                F1 Score=84.57% Precision=87.04% Recall=82.24% Accuracy=85.00%\n",
      "\n",
      "Step 1000 epoch 3. ;(\n",
      "Test loss is 1.1424. Train loss is 0.1166.                F1 Score=84.91% Precision=86.61% Recall=83.28% Accuracy=85.20%\n",
      "\n",
      "Step 1200 epoch 3. ;(\n",
      "Test loss is 1.3974. Train loss is 0.1272.                F1 Score=84.97% Precision=86.56% Recall=83.44% Accuracy=85.24%\n",
      "\n",
      "Step 1400 epoch 4. ;(\n",
      "Test loss is 2.1828. Train loss is 0.1135.                F1 Score=85.09% Precision=86.46% Recall=83.76% Accuracy=85.32%\n",
      "\n",
      "Step 1600 epoch 4. ;(\n",
      "Test loss is 2.4573. Train loss is 0.1258.                F1 Score=84.92% Precision=85.61% Recall=84.24% Accuracy=85.04%\n",
      "\n",
      "Step 1800 epoch 5. ;(\n",
      "Test loss is 3.0516. Train loss is 0.1142.                F1 Score=84.86% Precision=84.76% Recall=84.96% Accuracy=84.84%\n",
      "\n",
      "Step 2000 epoch 5. ;(\n",
      "Test loss is 3.2994. Train loss is 0.1295.                F1 Score=84.62% Precision=84.93% Recall=84.32% Accuracy=84.68%\n",
      "\n",
      "Step 2200 epoch 6. ;(\n",
      "Test loss is 3.2048. Train loss is 0.1132.                F1 Score=84.70% Precision=85.01% Recall=84.40% Accuracy=84.76%\n",
      "\n",
      "Step 2400 epoch 6. ;(\n",
      "Test loss is 3.1211. Train loss is 0.1275.                F1 Score=84.60% Precision=84.81% Recall=84.40% Accuracy=84.64%\n",
      "\n",
      "Step 2600 epoch 7. ;(\n",
      "Test loss is 3.1813. Train loss is 0.1014.                F1 Score=84.63% Precision=85.11% Recall=84.16% Accuracy=84.72%\n",
      "\n",
      "Step 2800 epoch 7. ;(\n",
      "Test loss is 3.2187. Train loss is 0.1223.                F1 Score=84.60% Precision=84.81% Recall=84.40% Accuracy=84.64%\n",
      "\n",
      "Step 3000 epoch 8. ;(\n",
      "Test loss is 3.1873. Train loss is 0.1097.                F1 Score=84.80% Precision=84.80% Recall=84.80% Accuracy=84.80%\n",
      "\n",
      "Step 3200 epoch 8. ;(\n",
      "Test loss is 3.0252. Train loss is 0.1241.                F1 Score=84.70% Precision=85.25% Recall=84.16% Accuracy=84.80%\n",
      "\n",
      "Step 3400 epoch 9. ;(\n",
      "Test loss is 3.2734. Train loss is 0.1109.                F1 Score=84.67% Precision=85.18% Recall=84.16% Accuracy=84.76%\n",
      "\n",
      "Step 3600 epoch 9. ;(\n",
      "Test loss is 2.8786. Train loss is 0.1249.                F1 Score=84.61% Precision=85.23% Recall=84.00% Accuracy=84.72%\n",
      "\n",
      "Step 3800 epoch 10. ;(\n",
      "Test loss is 2.8520. Train loss is 0.1098.                F1 Score=84.79% Precision=85.52% Recall=84.08% Accuracy=84.92%\n",
      "\n",
      "Step 4000 epoch 10. ;(\n",
      "Test loss is 2.8650. Train loss is 0.1252.                F1 Score=84.84% Precision=85.05% Recall=84.64% Accuracy=84.88%\n",
      "\n",
      "Step 4200 epoch 11. ;(\n",
      "Test loss is 2.7497. Train loss is 0.1082.                F1 Score=84.84% Precision=85.53% Recall=84.16% Accuracy=84.96%\n",
      "\n",
      "Step 4400 epoch 11. ;(\n",
      "Test loss is 2.6334. Train loss is 0.1233.                F1 Score=84.89% Precision=85.30% Recall=84.48% Accuracy=84.96%\n",
      "\n",
      "Step 4600 epoch 12. ;(\n",
      "Test loss is 2.5041. Train loss is 0.1120.                F1 Score=84.96% Precision=85.87% Recall=84.08% Accuracy=85.12%\n",
      "\n",
      "Step 4800 epoch 12. ;(\n",
      "Test loss is 2.4952. Train loss is 0.1220.                F1 Score=84.88% Precision=86.03% Recall=83.76% Accuracy=85.08%\n",
      "\n",
      "Step 5000 epoch 13. ;(\n",
      "Test loss is 2.6730. Train loss is 0.1155.                F1 Score=84.96% Precision=85.87% Recall=84.08% Accuracy=85.12%\n",
      "\n",
      "Step 5200 epoch 13. ;(\n",
      "Test loss is 2.5161. Train loss is 0.1313.                F1 Score=84.75% Precision=85.26% Recall=84.24% Accuracy=84.84%\n",
      "\n",
      "Step 5400 epoch 14. ;(\n",
      "Test loss is 2.4409. Train loss is 0.1077.                F1 Score=84.99% Precision=86.00% Recall=84.00% Accuracy=85.16%\n",
      "\n",
      "Step 5600 epoch 14. ;(\n",
      "Test loss is 2.3140. Train loss is 0.1249.                F1 Score=84.61% Precision=85.90% Recall=83.36% Accuracy=84.84%\n",
      "\n",
      "Step 5800 epoch 15. ;(\n",
      "Test loss is 2.4138. Train loss is 0.1028.                F1 Score=84.88% Precision=86.03% Recall=83.76% Accuracy=85.08%\n",
      "\n",
      "Step 6000 epoch 15. ;(\n",
      "Test loss is 2.3780. Train loss is 0.1235.                F1 Score=84.84% Precision=85.77% Recall=83.92% Accuracy=85.00%\n",
      "\n",
      "Step 6200 epoch 16. ;(\n",
      "Test loss is 2.3430. Train loss is 0.1104.                F1 Score=84.87% Precision=85.60% Recall=84.16% Accuracy=85.00%\n",
      "\n",
      "Step 6400 epoch 16. ;(\n",
      "Test loss is 2.3356. Train loss is 0.1257.                F1 Score=84.70% Precision=86.18% Recall=83.28% Accuracy=84.96%\n",
      "\n",
      "Step 6600 epoch 17. ;(\n",
      "Test loss is 2.2052. Train loss is 0.1118.                F1 Score=84.53% Precision=86.07% Recall=83.04% Accuracy=84.80%\n",
      "\n",
      "Step 6800 epoch 17. ;(\n",
      "Test loss is 2.2647. Train loss is 0.1230.                F1 Score=84.74% Precision=85.75% Recall=83.76% Accuracy=84.92%\n",
      "\n",
      "Step 7000 epoch 18. ;(\n",
      "Test loss is 2.2124. Train loss is 0.1140.                F1 Score=84.76% Precision=85.88% Recall=83.68% Accuracy=84.96%\n",
      "\n",
      "Step 7200 epoch 18. ;(\n",
      "Test loss is 2.2311. Train loss is 0.1255.                F1 Score=84.76% Precision=86.13% Recall=83.44% Accuracy=85.00%\n",
      "\n",
      "Step 7400 epoch 19. ;(\n",
      "Test loss is 1.9894. Train loss is 0.1123.                F1 Score=84.53% Precision=86.07% Recall=83.04% Accuracy=84.80%\n",
      "\n",
      "Step 7600 epoch 19. ;(\n",
      "Test loss is 1.9181. Train loss is 0.1304.                F1 Score=84.53% Precision=86.07% Recall=83.04% Accuracy=84.80%\n",
      "\n",
      "Step 7800 epoch 20. ;(\n",
      "Test loss is 1.7975. Train loss is 0.1101.                F1 Score=84.40% Precision=86.23% Recall=82.64% Accuracy=84.72%\n",
      "\n",
      "Step 8000 epoch 20. ;(\n",
      "Test loss is 1.9128. Train loss is 0.1267.                F1 Score=84.67% Precision=86.10% Recall=83.28% Accuracy=84.92%\n",
      "\n",
      "Step 8200 epoch 21. ;(\n",
      "Test loss is 1.7727. Train loss is 0.1069.                F1 Score=84.57% Precision=86.08% Recall=83.12% Accuracy=84.84%\n",
      "\n",
      "Step 8400 epoch 21. ;(\n",
      "Test loss is 1.5424. Train loss is 0.1259.                F1 Score=84.40% Precision=86.23% Recall=82.64% Accuracy=84.72%\n",
      "\n",
      "Step 8600 epoch 22. ;(\n",
      "Test loss is 1.8044. Train loss is 0.1139.                F1 Score=84.54% Precision=86.01% Recall=83.12% Accuracy=84.80%\n",
      "\n",
      "Step 8800 epoch 22. ;(\n",
      "Test loss is 1.6370. Train loss is 0.1276.                F1 Score=84.67% Precision=86.10% Recall=83.28% Accuracy=84.92%\n",
      "\n",
      "Step 9000 epoch 23. ;(\n",
      "Test loss is 1.6318. Train loss is 0.1075.                F1 Score=84.04% Precision=86.27% Recall=81.92% Accuracy=84.44%\n",
      "\n",
      "Step 9200 epoch 23. ;(\n",
      "Test loss is 1.5312. Train loss is 0.1223.                F1 Score=84.27% Precision=86.13% Recall=82.48% Accuracy=84.60%\n",
      "\n",
      "Step 9400 epoch 24. ;(\n",
      "Test loss is 1.6536. Train loss is 0.1074.                F1 Score=84.40% Precision=85.98% Recall=82.88% Accuracy=84.68%\n",
      "\n",
      "Step 9600 epoch 24. ;(\n",
      "Test loss is 1.6162. Train loss is 0.1295.                F1 Score=84.28% Precision=86.07% Recall=82.56% Accuracy=84.60%\n",
      "\n",
      "Step 9800 epoch 25. ;(\n",
      "Test loss is 1.4244. Train loss is 0.1086.                F1 Score=83.74% Precision=86.00% Recall=81.60% Accuracy=84.16%\n",
      "\n",
      "Step 10000 epoch 25. ;(\n",
      "Test loss is 1.4452. Train loss is 0.1274.                F1 Score=84.21% Precision=85.93% Recall=82.56% Accuracy=84.52%\n",
      "\n",
      "Step 10200 epoch 26. ;(\n",
      "Test loss is 1.3920. Train loss is 0.1090.                F1 Score=83.98% Precision=86.06% Recall=82.00% Accuracy=84.36%\n",
      "\n",
      "Step 10400 epoch 26. ;(\n",
      "Test loss is 1.4786. Train loss is 0.1231.                F1 Score=83.68% Precision=86.05% Recall=81.44% Accuracy=84.12%\n",
      "\n",
      "Step 10600 epoch 27. ;(\n",
      "Test loss is 1.3799. Train loss is 0.1089.                F1 Score=83.68% Precision=86.31% Recall=81.20% Accuracy=84.16%\n",
      "\n",
      "Step 10800 epoch 27. ;(\n",
      "Test loss is 1.2623. Train loss is 0.1269.                F1 Score=83.90% Precision=85.98% Recall=81.92% Accuracy=84.28%\n",
      "\n",
      "Step 11000 epoch 28. ;(\n",
      "Test loss is 0.7866. Train loss is 0.1048.                F1 Score=82.90% Precision=88.77% Recall=77.76% Accuracy=83.96%\n",
      "\n",
      "Step 11200 epoch 28. ;(\n",
      "Test loss is 1.2033. Train loss is 0.1172.                F1 Score=83.55% Precision=86.67% Recall=80.64% Accuracy=84.12%\n",
      "\n",
      "Step 11400 epoch 29. ;(\n",
      "Test loss is 1.2270. Train loss is 0.1089.                F1 Score=83.77% Precision=86.33% Recall=81.36% Accuracy=84.24%\n",
      "\n",
      "Step 11600 epoch 29. ;(\n",
      "Test loss is 1.2247. Train loss is 0.1259.                F1 Score=83.82% Precision=86.34% Recall=81.44% Accuracy=84.28%\n",
      "\n",
      "Step 11800 epoch 30. ;(\n",
      "Test loss is 1.3264. Train loss is 0.1079.                F1 Score=83.65% Precision=86.43% Recall=81.04% Accuracy=84.16%\n",
      "\n",
      "Step 12000 epoch 30. ;(\n",
      "Test loss is 1.2347. Train loss is 0.1230.                F1 Score=83.71% Precision=86.38% Recall=81.20% Accuracy=84.20%\n",
      "\n",
      "Step 12200 epoch 31. ;(\n",
      "Test loss is 1.1536. Train loss is 0.1122.                F1 Score=83.57% Precision=86.35% Recall=80.96% Accuracy=84.08%\n",
      "\n",
      "Step 12400 epoch 31. ;(\n",
      "Test loss is 1.2172. Train loss is 0.1300.                F1 Score=83.58% Precision=86.29% Recall=81.04% Accuracy=84.08%\n",
      "\n",
      "Step 12600 epoch 32. ;(\n",
      "Test loss is 1.2262. Train loss is 0.1184.                F1 Score=83.72% Precision=86.32% Recall=81.28% Accuracy=84.20%\n",
      "\n",
      "Step 12800 epoch 32. ;(\n",
      "Test loss is 1.1536. Train loss is 0.1325.                F1 Score=83.71% Precision=85.93% Recall=81.60% Accuracy=84.12%\n",
      "\n",
      "Step 13000 epoch 33. ;(\n",
      "Test loss is 1.1867. Train loss is 0.1045.                F1 Score=83.68% Precision=85.86% Recall=81.60% Accuracy=84.08%\n",
      "\n",
      "Step 13200 epoch 33. ;(\n",
      "Test loss is 1.1234. Train loss is 0.1266.                F1 Score=83.91% Precision=86.17% Recall=81.76% Accuracy=84.32%\n",
      "\n",
      "Step 13400 epoch 34. ;(\n",
      "Test loss is 1.1606. Train loss is 0.1063.                F1 Score=83.62% Precision=86.56% Recall=80.88% Accuracy=84.16%\n",
      "\n",
      "Step 13600 epoch 34. ;(\n",
      "Test loss is 1.2190. Train loss is 0.1194.                F1 Score=83.86% Precision=86.62% Recall=81.28% Accuracy=84.36%\n",
      "\n",
      "Step 13800 epoch 35. ;(\n",
      "Test loss is 1.1051. Train loss is 0.1069.                F1 Score=83.55% Precision=86.67% Recall=80.64% Accuracy=84.12%\n",
      "\n",
      "Step 14000 epoch 35. ;(\n",
      "Test loss is 0.9779. Train loss is 0.1241.                F1 Score=83.92% Precision=86.56% Recall=81.44% Accuracy=84.40%\n",
      "\n",
      "Step 14200 epoch 36. ;(\n",
      "Test loss is 1.1389. Train loss is 0.1011.                F1 Score=83.58% Precision=86.29% Recall=81.04% Accuracy=84.08%\n",
      "\n",
      "Step 14400 epoch 36. ;(\n",
      "Test loss is 1.1564. Train loss is 0.1273.                F1 Score=83.67% Precision=86.83% Recall=80.72% Accuracy=84.24%\n",
      "\n",
      "Step 14600 epoch 37. ;(\n",
      "Test loss is 1.1135. Train loss is 0.1102.                F1 Score=83.68% Precision=86.77% Recall=80.80% Accuracy=84.24%\n",
      "\n",
      "Step 14800 epoch 37. ;(\n",
      "Test loss is 1.1127. Train loss is 0.1238.                F1 Score=83.60% Precision=86.42% Recall=80.96% Accuracy=84.12%\n",
      "\n",
      "Step 15000 epoch 38. ;(\n",
      "Test loss is 1.1417. Train loss is 0.1008.                F1 Score=83.57% Precision=86.81% Recall=80.56% Accuracy=84.16%\n",
      "\n",
      "Step 15200 epoch 38. ;(\n",
      "Test loss is 1.0006. Train loss is 0.1163.                F1 Score=83.27% Precision=87.01% Recall=79.84% Accuracy=83.96%\n",
      "\n",
      "Step 15400 epoch 39. ;(\n",
      "Test loss is 1.0339. Train loss is 0.1136.                F1 Score=83.71% Precision=86.64% Recall=80.96% Accuracy=84.24%\n",
      "\n",
      "Step 15600 epoch 39. ;(\n",
      "Test loss is 1.0066. Train loss is 0.1259.                F1 Score=83.53% Precision=86.54% Recall=80.72% Accuracy=84.08%\n",
      "\n",
      "Step 15800 epoch 40. ;(\n",
      "Test loss is 0.9986. Train loss is 0.1110.                F1 Score=83.75% Precision=86.20% Recall=81.44% Accuracy=84.20%\n",
      "\n",
      "Step 16000 epoch 40. ;(\n",
      "Test loss is 0.9791. Train loss is 0.1293.                F1 Score=83.64% Precision=86.24% Recall=81.20% Accuracy=84.12%\n",
      "\n",
      "Step 16200 epoch 41. ;(\n",
      "Test loss is 1.0202. Train loss is 0.1075.                F1 Score=83.54% Precision=86.47% Recall=80.80% Accuracy=84.08%\n",
      "\n",
      "Step 16400 epoch 41. ;(\n",
      "Test loss is 0.9642. Train loss is 0.1206.                F1 Score=83.93% Precision=86.31% Recall=81.68% Accuracy=84.36%\n",
      "\n",
      "Step 16600 epoch 42. ;(\n",
      "Test loss is 1.0217. Train loss is 0.1005.                F1 Score=83.55% Precision=86.41% Recall=80.88% Accuracy=84.08%\n",
      "\n",
      "Step 16800 epoch 42. ;(\n",
      "Test loss is 0.9945. Train loss is 0.1172.                F1 Score=83.45% Precision=86.19% Recall=80.88% Accuracy=83.96%\n",
      "\n",
      "Step 17000 epoch 43. ;(\n",
      "Test loss is 0.9933. Train loss is 0.1151.                F1 Score=83.55% Precision=86.67% Recall=80.64% Accuracy=84.12%\n",
      "\n",
      "Step 17200 epoch 43. ;(\n",
      "Test loss is 0.9562. Train loss is 0.1265.                F1 Score=83.57% Precision=86.81% Recall=80.56% Accuracy=84.16%\n",
      "\n",
      "Step 17400 epoch 44. ;(\n",
      "Test loss is 0.9995. Train loss is 0.1025.                F1 Score=83.65% Precision=86.43% Recall=81.04% Accuracy=84.16%\n",
      "\n",
      "Step 17600 epoch 44. ;(\n",
      "Test loss is 0.9433. Train loss is 0.1203.                F1 Score=83.39% Precision=86.70% Recall=80.32% Accuracy=84.00%\n",
      "\n",
      "Step 17800 epoch 45. ;(\n",
      "Test loss is 1.0004. Train loss is 0.0925.                F1 Score=83.71% Precision=86.64% Recall=80.96% Accuracy=84.24%\n",
      "\n",
      "Step 18000 epoch 45. ;(\n",
      "Test loss is 0.9457. Train loss is 0.1210.                F1 Score=83.38% Precision=86.50% Recall=80.48% Accuracy=83.96%\n",
      "\n",
      "Step 18200 epoch 46. ;(\n",
      "Test loss is 0.9955. Train loss is 0.1051.                F1 Score=83.33% Precision=86.96% Recall=80.00% Accuracy=84.00%\n",
      "\n",
      "Step 18400 epoch 46. ;(\n",
      "Test loss is 0.9093. Train loss is 0.1254.                F1 Score=83.35% Precision=86.63% Recall=80.32% Accuracy=83.96%\n",
      "\n",
      "Step 18600 epoch 47. ;(\n",
      "Test loss is 0.9391. Train loss is 0.1068.                F1 Score=83.27% Precision=86.54% Recall=80.24% Accuracy=83.88%\n",
      "\n",
      "Step 18800 epoch 47. ;(\n",
      "Test loss is 0.9192. Train loss is 0.1232.                F1 Score=83.44% Precision=86.71% Recall=80.40% Accuracy=84.04%\n",
      "\n",
      "Step 19000 epoch 48. ;(\n",
      "Test loss is 0.9211. Train loss is 0.1068.                F1 Score=83.33% Precision=86.96% Recall=80.00% Accuracy=84.00%\n",
      "\n",
      "Step 19200 epoch 48. ;(\n",
      "Test loss is 0.8822. Train loss is 0.1254.                F1 Score=83.71% Precision=86.38% Recall=81.20% Accuracy=84.20%\n",
      "\n",
      "Step 19400 epoch 49. ;(\n",
      "Test loss is 0.8975. Train loss is 0.1092.                F1 Score=83.67% Precision=86.57% Recall=80.96% Accuracy=84.20%\n",
      "\n",
      "Step 19600 epoch 49. ;(\n",
      "Test loss is 0.9203. Train loss is 0.1203.                F1 Score=83.73% Precision=86.52% Recall=81.12% Accuracy=84.24%\n",
      "\n",
      "Step 19800 epoch 50. ;(\n",
      "Test loss is 0.8858. Train loss is 0.1139.                F1 Score=83.15% Precision=86.85% Recall=79.76% Accuracy=83.84%\n",
      "\n",
      "Step 20000 epoch 50. ;(\n",
      "Test loss is 0.8690. Train loss is 0.1242.                F1 Score=83.70% Precision=86.45% Recall=81.12% Accuracy=84.20%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_fscore = trainer_SWA(convRNN, criterion, optimizer, train_loader, valid_loader, clip_value=10, epochs=50, print_every=200, max_fscore=max_fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-papua",
   "metadata": {},
   "source": [
    "## SWA result - you can not train a ntework with SGD... SWA does not give the result here\n",
    "# BUT with Adam in our case it works almost on par with vanilla attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-cocktail",
   "metadata": {},
   "source": [
    "## Make convRNN with pretrained embedding from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cleared-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparams\n",
    "vocab_size = len(vocab) \n",
    "output_size = 1 # not needed\n",
    "embedding_dim = 50\n",
    "hidden_dim = 64\n",
    "n_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "acquired-edgar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.weight         torch.Size([67359, 50])\n",
      "conv1d.weight        torch.Size([32, 200, 3])\n",
      "lstm.weight_ih_l0    torch.Size([256, 24])\n",
      "lstm.weight_hh_l0    torch.Size([256, 64])\n",
      "lstm.bias_ih_l0      torch.Size([256])\n",
      "lstm.bias_hh_l0      torch.Size([256])\n",
      "dense.weight         torch.Size([1, 64])\n",
      "dense.bias           torch.Size([1])\n",
      "bn_embedding.weight  torch.Size([200])\n",
      "bn_embedding.bias    torch.Size([200])\n",
      "bn_conv1d.weight     torch.Size([32])\n",
      "bn_conv1d.bias       torch.Size([32])\n",
      "bn_lstm.weight       torch.Size([32])\n",
      "bn_lstm.bias         torch.Size([32])\n",
      "Total number of parameters = 3,410,783\n"
     ]
    }
   ],
   "source": [
    "# vocab_size, output_size=1, embedding_dim=32, hidden_dim=64, out_channels=32, drop_prob=0.5, vocab_vectors=None\n",
    "convRNN = SentimentConvNN(vocab_size=vocab_size, embedding_dim=embedding_dim, vocab_vectors=vocab.vectors)\n",
    "print(convRNN.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "focused-amateur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0460, -0.1792, -0.1163,  ..., -0.0013, -0.0254, -0.0351],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you want to make a grad\n",
    "convRNN.embed.weight.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "accompanied-tobago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convRNN.embed.weight.requires_grad, convRNN.dense.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "numerous-centre",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentConvNN(\n",
       "  (embed): Embedding(67359, 50)\n",
       "  (conv1d): Conv1d(200, 32, kernel_size=(3,), stride=(1,), bias=False)\n",
       "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (lstm): LSTM(24, 64, batch_first=True)\n",
       "  (dense): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (bn_embedding): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_conv1d): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_lstm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "convRNN.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "nearby-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(net, criterion, optimizer, train_loader, valid_loader, clip_value=5, epochs=10, print_every=200, max_fscore=-np.inf):\n",
    "    '''\n",
    "    Train the network\n",
    "        net - network to trian\n",
    "        criterion - loss function \n",
    "        optimizer - your optimiser of choice \n",
    "        train_loader - loader for training data\n",
    "        vlid_loader - lodaer for validation/test data\n",
    "        clip_value - upper limit for gradient \n",
    "        epochs - number of epochs to train the net\n",
    "        print_every - prin stats every number of batches\n",
    "        max_fscore - best fscore on validation set - used in mutiple runs of training\n",
    "    '''\n",
    "    from tqdm.notebook import tqdm, trange\n",
    "    \n",
    "    steps = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    # run over epochs of training\n",
    "    for e in trange(epochs):\n",
    "        \n",
    "        # array to keep value of losses over current epoch\n",
    "        train_loss = []\n",
    "\n",
    "        # run one pass through training samples = one epoch\n",
    "        for train_x, train_y in train_loader:\n",
    "            steps +=1\n",
    "\n",
    "            # zero out the grads \n",
    "            net.zero_grad()\n",
    "            # optimioptimizer.zero_grad()\n",
    "\n",
    "            # send data to device\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "\n",
    "            # initialize hidden state\n",
    "            h = net.init_hidden(len(train_x))\n",
    "\n",
    "            # calculate the output of the network\n",
    "            out, _ = net(train_x, h)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = criterion(out, train_y)\n",
    "            # backprop grads of the loss wrt to net parameters\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip_value)\n",
    "\n",
    "            # upadate parameters of network\n",
    "            optimizer.step()\n",
    "\n",
    "            # append current batch loss (loss per object in current batch)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            # test loss calc every \n",
    "            if steps%print_every == 0:\n",
    "                # calculate loss on test set\n",
    "                test_loss, metrics = val_score(net, valid_loader, criterion)\n",
    "                if metrics['fscore'] > max_fscore:\n",
    "                    max_fscore = metrics['fscore']\n",
    "                    message = '=)'\n",
    "                    check_point = {'vocab_size':net.vocab_size, \n",
    "                                   'embedding_dim': net.embedding_dim, \n",
    "                                   'hidden_dim':net.hidden_dim, \n",
    "                                   'n_layers':net.n_layers, \n",
    "                                   'net_params':net.state_dict()}\n",
    "                    torch.save(check_point, f\"spam_model_fscore_{metrics['fscore']:.3f}.pt\")\n",
    "                else:\n",
    "                    message = ';('\n",
    "                net.train()\n",
    "                print(f\"Step {steps} epoch {e+1}. {message}\\nTest loss is {test_loss:.4f}. Train loss is {np.mean(train_loss):.4f}.\\\n",
    "                F1 Score={metrics['fscore']:.2%} Precision={metrics['precision']:.2%} Recall={metrics['recall']:.2%} Accuracy={metrics['accuracy']:.2%}\\n\")\n",
    "    return max_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "headed-driver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "adequate-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr = 0.001\n",
    "criterion = nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(convRNN.parameters(), lr=lr, weight_decay=1e-2)\n",
    "#optimizer = torch.optim.SGD(convRNN.parameters(), lr=lr, momentum=0.9, nesterov=True, weight_decay=1e-2)\n",
    "#optimizer = torch.optim.RMSprop(convRNN.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "severe-infrared",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8638963019443385"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "flush-tender",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249415a414f24f32ae57f33f7b5409f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 epoch 1. ;(\n",
      "Test loss is 0.6891. Train loss is 0.6993.                F1 Score=64.30% Precision=51.78% Recall=84.80% Accuracy=52.92%\n",
      "\n",
      "Step 400 epoch 1. ;(\n",
      "Test loss is 0.6878. Train loss is 0.6941.                F1 Score=34.81% Precision=60.24% Recall=24.48% Accuracy=54.16%\n",
      "\n",
      "Step 600 epoch 2. ;(\n",
      "Test loss is 0.6638. Train loss is 0.6787.                F1 Score=64.02% Precision=58.37% Recall=70.88% Accuracy=60.16%\n",
      "\n",
      "Step 800 epoch 2. ;(\n",
      "Test loss is 0.6365. Train loss is 0.6688.                F1 Score=61.05% Precision=67.21% Recall=55.92% Accuracy=64.32%\n",
      "\n",
      "Step 1000 epoch 3. ;(\n",
      "Test loss is 0.5935. Train loss is 0.6214.                F1 Score=65.12% Precision=73.35% Recall=58.56% Accuracy=68.64%\n",
      "\n",
      "Step 1200 epoch 3. ;(\n",
      "Test loss is 0.5561. Train loss is 0.6003.                F1 Score=69.35% Precision=73.65% Recall=65.52% Accuracy=71.04%\n",
      "\n",
      "Step 1400 epoch 4. ;(\n",
      "Test loss is 0.5144. Train loss is 0.5418.                F1 Score=74.96% Precision=76.46% Recall=73.52% Accuracy=75.44%\n",
      "\n",
      "Step 1600 epoch 4. ;(\n",
      "Test loss is 0.4939. Train loss is 0.5216.                F1 Score=76.11% Precision=74.09% Recall=78.24% Accuracy=75.44%\n",
      "\n",
      "Step 1800 epoch 5. ;(\n",
      "Test loss is 0.4603. Train loss is 0.4682.                F1 Score=78.18% Precision=78.12% Recall=78.24% Accuracy=78.16%\n",
      "\n",
      "Step 2000 epoch 5. ;(\n",
      "Test loss is 0.4477. Train loss is 0.4612.                F1 Score=79.50% Precision=77.94% Recall=81.12% Accuracy=79.08%\n",
      "\n",
      "Step 2200 epoch 6. ;(\n",
      "Test loss is 0.4343. Train loss is 0.4150.                F1 Score=79.40% Precision=82.19% Recall=76.80% Accuracy=80.08%\n",
      "\n",
      "Step 2400 epoch 6. ;(\n",
      "Test loss is 0.3989. Train loss is 0.4197.                F1 Score=82.74% Precision=79.64% Recall=86.08% Accuracy=82.04%\n",
      "\n",
      "Step 2600 epoch 7. ;(\n",
      "Test loss is 0.4130. Train loss is 0.3841.                F1 Score=81.95% Precision=75.96% Recall=88.96% Accuracy=80.40%\n",
      "\n",
      "Step 2800 epoch 7. ;(\n",
      "Test loss is 0.4009. Train loss is 0.3875.                F1 Score=81.65% Precision=82.52% Recall=80.80% Accuracy=81.84%\n",
      "\n",
      "Step 3000 epoch 8. ;(\n",
      "Test loss is 0.4053. Train loss is 0.3608.                F1 Score=81.74% Precision=81.64% Recall=81.84% Accuracy=81.72%\n",
      "\n",
      "Step 3200 epoch 8. ;(\n",
      "Test loss is 0.3909. Train loss is 0.3655.                F1 Score=82.99% Precision=83.84% Recall=82.16% Accuracy=83.16%\n",
      "\n",
      "Step 3400 epoch 9. ;(\n",
      "Test loss is 0.4085. Train loss is 0.3245.                F1 Score=81.68% Precision=80.14% Recall=83.28% Accuracy=81.32%\n",
      "\n",
      "Step 3600 epoch 9. ;(\n",
      "Test loss is 0.3978. Train loss is 0.3441.                F1 Score=82.34% Precision=83.01% Recall=81.68% Accuracy=82.48%\n",
      "\n",
      "Step 3800 epoch 10. ;(\n",
      "Test loss is 0.4027. Train loss is 0.2994.                F1 Score=82.71% Precision=80.42% Recall=85.12% Accuracy=82.20%\n",
      "\n",
      "Step 4000 epoch 10. ;(\n",
      "Test loss is 0.3994. Train loss is 0.3278.                F1 Score=81.87% Precision=82.64% Recall=81.12% Accuracy=82.04%\n",
      "\n",
      "Step 4200 epoch 11. ;(\n",
      "Test loss is 0.4064. Train loss is 0.2946.                F1 Score=81.56% Precision=83.61% Recall=79.60% Accuracy=82.00%\n",
      "\n",
      "Step 4400 epoch 11. ;(\n",
      "Test loss is 0.4072. Train loss is 0.3154.                F1 Score=81.10% Precision=84.94% Recall=77.60% Accuracy=81.92%\n",
      "\n",
      "Step 4600 epoch 12. ;(\n",
      "Test loss is 0.4068. Train loss is 0.2713.                F1 Score=82.95% Precision=83.01% Recall=82.88% Accuracy=82.96%\n",
      "\n",
      "Step 4800 epoch 12. ;(\n",
      "Test loss is 0.4010. Train loss is 0.3028.                F1 Score=82.97% Precision=81.59% Recall=84.40% Accuracy=82.68%\n",
      "\n",
      "Step 5000 epoch 13. ;(\n",
      "Test loss is 0.4300. Train loss is 0.2788.                F1 Score=81.17% Precision=82.72% Recall=79.68% Accuracy=81.52%\n",
      "\n",
      "Step 5200 epoch 13. ;(\n",
      "Test loss is 0.4081. Train loss is 0.2942.                F1 Score=81.98% Precision=79.83% Recall=84.24% Accuracy=81.48%\n",
      "\n",
      "Step 5400 epoch 14. ;(\n",
      "Test loss is 0.4344. Train loss is 0.2583.                F1 Score=82.04% Precision=82.23% Recall=81.84% Accuracy=82.08%\n",
      "\n",
      "Step 5600 epoch 14. ;(\n",
      "Test loss is 0.3962. Train loss is 0.2896.                F1 Score=82.21% Precision=84.10% Recall=80.40% Accuracy=82.60%\n",
      "\n",
      "Step 5800 epoch 15. ;(\n",
      "Test loss is 0.4456. Train loss is 0.2529.                F1 Score=80.54% Precision=80.13% Recall=80.96% Accuracy=80.44%\n",
      "\n",
      "Step 6000 epoch 15. ;(\n",
      "Test loss is 0.4164. Train loss is 0.2851.                F1 Score=80.65% Precision=84.43% Recall=77.20% Accuracy=81.48%\n",
      "\n",
      "Step 6200 epoch 16. ;(\n",
      "Test loss is 0.4357. Train loss is 0.2472.                F1 Score=81.02% Precision=81.98% Recall=80.08% Accuracy=81.24%\n",
      "\n",
      "Step 6400 epoch 16. ;(\n",
      "Test loss is 0.4255. Train loss is 0.2798.                F1 Score=81.46% Precision=81.72% Recall=81.20% Accuracy=81.52%\n",
      "\n",
      "Step 6600 epoch 17. ;(\n",
      "Test loss is 0.4243. Train loss is 0.2516.                F1 Score=81.83% Precision=82.23% Recall=81.44% Accuracy=81.92%\n",
      "\n",
      "Step 6800 epoch 17. ;(\n",
      "Test loss is 0.4072. Train loss is 0.2759.                F1 Score=80.08% Precision=84.73% Recall=75.92% Accuracy=81.12%\n",
      "\n",
      "Step 7000 epoch 18. ;(\n",
      "Test loss is 0.4265. Train loss is 0.2403.                F1 Score=82.83% Precision=81.92% Recall=83.76% Accuracy=82.64%\n",
      "\n",
      "Step 7200 epoch 18. ;(\n",
      "Test loss is 0.4091. Train loss is 0.2706.                F1 Score=81.67% Precision=82.64% Recall=80.72% Accuracy=81.88%\n",
      "\n",
      "Step 7400 epoch 19. ;(\n",
      "Test loss is 0.4340. Train loss is 0.2463.                F1 Score=81.84% Precision=80.29% Recall=83.44% Accuracy=81.48%\n",
      "\n",
      "Step 7600 epoch 19. ;(\n",
      "Test loss is 0.4150. Train loss is 0.2714.                F1 Score=81.95% Precision=82.72% Recall=81.20% Accuracy=82.12%\n",
      "\n",
      "Step 7800 epoch 20. ;(\n",
      "Test loss is 0.4423. Train loss is 0.2350.                F1 Score=81.42% Precision=81.00% Recall=81.84% Accuracy=81.32%\n",
      "\n",
      "Step 8000 epoch 20. ;(\n",
      "Test loss is 0.3999. Train loss is 0.2656.                F1 Score=81.69% Precision=82.36% Recall=81.04% Accuracy=81.84%\n",
      "\n",
      "Step 8200 epoch 21. ;(\n",
      "Test loss is 0.4554. Train loss is 0.2404.                F1 Score=81.57% Precision=79.85% Recall=83.36% Accuracy=81.16%\n",
      "\n",
      "Step 8400 epoch 21. ;(\n",
      "Test loss is 0.4173. Train loss is 0.2683.                F1 Score=80.77% Precision=82.49% Recall=79.12% Accuracy=81.16%\n",
      "\n",
      "Step 8600 epoch 22. ;(\n",
      "Test loss is 0.4525. Train loss is 0.2343.                F1 Score=80.74% Precision=82.59% Recall=78.96% Accuracy=81.16%\n",
      "\n",
      "Step 8800 epoch 22. ;(\n",
      "Test loss is 0.4009. Train loss is 0.2673.                F1 Score=82.65% Precision=82.59% Recall=82.72% Accuracy=82.64%\n",
      "\n",
      "Step 9000 epoch 23. ;(\n",
      "Test loss is 0.4619. Train loss is 0.2287.                F1 Score=79.49% Precision=79.94% Recall=79.04% Accuracy=79.60%\n",
      "\n",
      "Step 9200 epoch 23. ;(\n",
      "Test loss is 0.4215. Train loss is 0.2610.                F1 Score=81.91% Precision=81.42% Recall=82.40% Accuracy=81.80%\n",
      "\n",
      "Step 9400 epoch 24. ;(\n",
      "Test loss is 0.4254. Train loss is 0.2410.                F1 Score=82.22% Precision=83.09% Recall=81.36% Accuracy=82.40%\n",
      "\n",
      "Step 9600 epoch 24. ;(\n",
      "Test loss is 0.4173. Train loss is 0.2658.                F1 Score=82.62% Precision=80.05% Recall=85.36% Accuracy=82.04%\n",
      "\n",
      "Step 9800 epoch 25. ;(\n",
      "Test loss is 0.4507. Train loss is 0.2403.                F1 Score=81.50% Precision=81.08% Recall=81.92% Accuracy=81.40%\n",
      "\n",
      "Step 10000 epoch 25. ;(\n",
      "Test loss is 0.4222. Train loss is 0.2640.                F1 Score=81.03% Precision=83.22% Recall=78.96% Accuracy=81.52%\n",
      "\n",
      "Step 10200 epoch 26. ;(\n",
      "Test loss is 0.4728. Train loss is 0.2302.                F1 Score=80.13% Precision=81.77% Recall=78.56% Accuracy=80.52%\n",
      "\n",
      "Step 10400 epoch 26. ;(\n",
      "Test loss is 0.4060. Train loss is 0.2594.                F1 Score=81.23% Precision=81.99% Recall=80.48% Accuracy=81.40%\n",
      "\n",
      "Step 10600 epoch 27. ;(\n",
      "Test loss is 0.4900. Train loss is 0.2308.                F1 Score=78.97% Precision=82.36% Recall=75.84% Accuracy=79.80%\n",
      "\n",
      "Step 10800 epoch 27. ;(\n",
      "Test loss is 0.4611. Train loss is 0.2602.                F1 Score=80.38% Precision=78.74% Recall=82.08% Accuracy=79.96%\n",
      "\n",
      "Step 11000 epoch 28. ;(\n",
      "Test loss is 0.4469. Train loss is 0.2258.                F1 Score=82.10% Precision=82.20% Recall=82.00% Accuracy=82.12%\n",
      "\n",
      "Step 11200 epoch 28. ;(\n",
      "Test loss is 0.4551. Train loss is 0.2566.                F1 Score=80.08% Precision=79.45% Recall=80.72% Accuracy=79.92%\n",
      "\n",
      "Step 11400 epoch 29. ;(\n",
      "Test loss is 0.4673. Train loss is 0.2214.                F1 Score=80.82% Precision=82.86% Recall=78.88% Accuracy=81.28%\n",
      "\n",
      "Step 11600 epoch 29. ;(\n",
      "Test loss is 0.4481. Train loss is 0.2539.                F1 Score=81.16% Precision=80.81% Recall=81.52% Accuracy=81.08%\n",
      "\n",
      "Step 11800 epoch 30. ;(\n",
      "Test loss is 0.4551. Train loss is 0.2301.                F1 Score=81.60% Precision=81.77% Recall=81.44% Accuracy=81.64%\n",
      "\n",
      "Step 12000 epoch 30. ;(\n",
      "Test loss is 0.4258. Train loss is 0.2559.                F1 Score=80.36% Precision=85.75% Recall=75.60% Accuracy=81.52%\n",
      "\n",
      "Step 12200 epoch 31. ;(\n",
      "Test loss is 0.4607. Train loss is 0.2272.                F1 Score=81.18% Precision=81.97% Recall=80.40% Accuracy=81.36%\n",
      "\n",
      "Step 12400 epoch 31. ;(\n",
      "Test loss is 0.4254. Train loss is 0.2541.                F1 Score=82.56% Precision=81.63% Recall=83.52% Accuracy=82.36%\n",
      "\n",
      "Step 12600 epoch 32. ;(\n",
      "Test loss is 0.4425. Train loss is 0.2282.                F1 Score=81.29% Precision=81.23% Recall=81.36% Accuracy=81.28%\n",
      "\n",
      "Step 12800 epoch 32. ;(\n",
      "Test loss is 0.4059. Train loss is 0.2564.                F1 Score=82.81% Precision=82.58% Recall=83.04% Accuracy=82.76%\n",
      "\n",
      "Step 13000 epoch 33. ;(\n",
      "Test loss is 0.4524. Train loss is 0.2310.                F1 Score=80.71% Precision=83.07% Recall=78.48% Accuracy=81.24%\n",
      "\n",
      "Step 13200 epoch 33. ;(\n",
      "Test loss is 0.4322. Train loss is 0.2543.                F1 Score=81.42% Precision=78.43% Recall=84.64% Accuracy=80.68%\n",
      "\n",
      "Step 13400 epoch 34. ;(\n",
      "Test loss is 0.4475. Train loss is 0.2217.                F1 Score=81.79% Precision=83.14% Recall=80.48% Accuracy=82.08%\n",
      "\n",
      "Step 13600 epoch 34. ;(\n",
      "Test loss is 0.4252. Train loss is 0.2566.                F1 Score=81.70% Precision=81.80% Recall=81.60% Accuracy=81.72%\n",
      "\n",
      "Step 13800 epoch 35. ;(\n",
      "Test loss is 0.4771. Train loss is 0.2258.                F1 Score=79.10% Precision=84.73% Recall=74.16% Accuracy=80.40%\n",
      "\n",
      "Step 14000 epoch 35. ;(\n",
      "Test loss is 0.4132. Train loss is 0.2575.                F1 Score=82.07% Precision=80.89% Recall=83.28% Accuracy=81.80%\n",
      "\n",
      "Step 14200 epoch 36. ;(\n",
      "Test loss is 0.4456. Train loss is 0.2258.                F1 Score=81.16% Precision=81.52% Recall=80.80% Accuracy=81.24%\n",
      "\n",
      "Step 14400 epoch 36. ;(\n",
      "Test loss is 0.4394. Train loss is 0.2559.                F1 Score=81.24% Precision=79.43% Recall=83.12% Accuracy=80.80%\n",
      "\n",
      "Step 14600 epoch 37. ;(\n",
      "Test loss is 0.4586. Train loss is 0.2287.                F1 Score=79.60% Precision=79.60% Recall=79.60% Accuracy=79.60%\n",
      "\n",
      "Step 14800 epoch 37. ;(\n",
      "Test loss is 0.4085. Train loss is 0.2584.                F1 Score=81.26% Precision=81.17% Recall=81.36% Accuracy=81.24%\n",
      "\n",
      "Step 15000 epoch 38. ;(\n",
      "Test loss is 0.4596. Train loss is 0.2265.                F1 Score=82.93% Precision=81.37% Recall=84.56% Accuracy=82.60%\n",
      "\n",
      "Step 15200 epoch 38. ;(\n",
      "Test loss is 0.4333. Train loss is 0.2527.                F1 Score=81.11% Precision=82.59% Recall=79.68% Accuracy=81.44%\n",
      "\n",
      "Step 15400 epoch 39. ;(\n",
      "Test loss is 0.4767. Train loss is 0.2208.                F1 Score=79.66% Precision=81.04% Recall=78.32% Accuracy=80.00%\n",
      "\n",
      "Step 15600 epoch 39. ;(\n",
      "Test loss is 0.4462. Train loss is 0.2492.                F1 Score=79.38% Precision=81.41% Recall=77.44% Accuracy=79.88%\n",
      "\n",
      "Step 15800 epoch 40. ;(\n",
      "Test loss is 0.4638. Train loss is 0.2337.                F1 Score=82.32% Precision=78.15% Recall=86.96% Accuracy=81.32%\n",
      "\n",
      "Step 16000 epoch 40. ;(\n",
      "Test loss is 0.4032. Train loss is 0.2557.                F1 Score=82.99% Precision=81.54% Recall=84.48% Accuracy=82.68%\n",
      "\n",
      "Step 16200 epoch 41. ;(\n",
      "Test loss is 0.4306. Train loss is 0.2304.                F1 Score=82.93% Precision=80.00% Recall=86.08% Accuracy=82.28%\n",
      "\n",
      "Step 16400 epoch 41. ;(\n",
      "Test loss is 0.4107. Train loss is 0.2608.                F1 Score=81.62% Precision=80.17% Recall=83.12% Accuracy=81.28%\n",
      "\n",
      "Step 16600 epoch 42. ;(\n",
      "Test loss is 0.4693. Train loss is 0.2247.                F1 Score=79.77% Precision=82.60% Recall=77.12% Accuracy=80.44%\n",
      "\n",
      "Step 16800 epoch 42. ;(\n",
      "Test loss is 0.4279. Train loss is 0.2504.                F1 Score=81.55% Precision=81.25% Recall=81.84% Accuracy=81.48%\n",
      "\n",
      "Step 17000 epoch 43. ;(\n",
      "Test loss is 0.4623. Train loss is 0.2354.                F1 Score=82.51% Precision=78.95% Recall=86.40% Accuracy=81.68%\n",
      "\n",
      "Step 17200 epoch 43. ;(\n",
      "Test loss is 0.4236. Train loss is 0.2579.                F1 Score=80.66% Precision=82.87% Recall=78.56% Accuracy=81.16%\n",
      "\n",
      "Step 17400 epoch 44. ;(\n",
      "Test loss is 0.4399. Train loss is 0.2222.                F1 Score=80.65% Precision=81.81% Recall=79.52% Accuracy=80.92%\n",
      "\n",
      "Step 17600 epoch 44. ;(\n",
      "Test loss is 0.4362. Train loss is 0.2544.                F1 Score=81.47% Precision=82.41% Recall=80.56% Accuracy=81.68%\n",
      "\n",
      "Step 17800 epoch 45. ;(\n",
      "Test loss is 0.4690. Train loss is 0.2199.                F1 Score=80.59% Precision=82.02% Recall=79.20% Accuracy=80.92%\n",
      "\n",
      "Step 18000 epoch 45. ;(\n",
      "Test loss is 0.4305. Train loss is 0.2515.                F1 Score=80.65% Precision=81.30% Recall=80.00% Accuracy=80.80%\n",
      "\n",
      "Step 18200 epoch 46. ;(\n",
      "Test loss is 0.4595. Train loss is 0.2233.                F1 Score=81.41% Precision=81.71% Recall=81.12% Accuracy=81.48%\n",
      "\n",
      "Step 18400 epoch 46. ;(\n",
      "Test loss is 0.4312. Train loss is 0.2523.                F1 Score=80.83% Precision=80.54% Recall=81.12% Accuracy=80.76%\n",
      "\n",
      "Step 18600 epoch 47. ;(\n",
      "Test loss is 0.4504. Train loss is 0.2351.                F1 Score=81.88% Precision=78.87% Recall=85.12% Accuracy=81.16%\n",
      "\n",
      "Step 18800 epoch 47. ;(\n",
      "Test loss is 0.4169. Train loss is 0.2592.                F1 Score=82.59% Precision=81.00% Recall=84.24% Accuracy=82.24%\n",
      "\n",
      "Step 19000 epoch 48. ;(\n",
      "Test loss is 0.4498. Train loss is 0.2246.                F1 Score=80.90% Precision=80.83% Recall=80.96% Accuracy=80.88%\n",
      "\n",
      "Step 19200 epoch 48. ;(\n",
      "Test loss is 0.4284. Train loss is 0.2558.                F1 Score=82.22% Precision=81.04% Recall=83.44% Accuracy=81.96%\n",
      "\n",
      "Step 19400 epoch 49. ;(\n",
      "Test loss is 0.4759. Train loss is 0.2200.                F1 Score=80.40% Precision=84.16% Recall=76.96% Accuracy=81.24%\n",
      "\n",
      "Step 19600 epoch 49. ;(\n",
      "Test loss is 0.4405. Train loss is 0.2499.                F1 Score=80.77% Precision=84.78% Recall=77.12% Accuracy=81.64%\n",
      "\n",
      "Step 19800 epoch 50. ;(\n",
      "Test loss is 0.4874. Train loss is 0.2303.                F1 Score=81.29% Precision=80.06% Recall=82.56% Accuracy=81.00%\n",
      "\n",
      "Step 20000 epoch 50. ;(\n",
      "Test loss is 0.4225. Train loss is 0.2586.                F1 Score=81.12% Precision=81.12% Recall=81.12% Accuracy=81.12%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_fscore = trainer(convRNN, criterion, optimizer, train_loader, valid_loader, clip_value=10, epochs=50, print_every=200, max_fscore=max_fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-yesterday",
   "metadata": {},
   "source": [
    "## CONV RNN with pretrained embedding gives no gain in metrics - you must make embedding trainable otherwise the results are horrible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-broadcasting",
   "metadata": {},
   "source": [
    "---\n",
    "### Manual conv network assembly\n",
    "---\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length,input_length=max_review_length)) \n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2)) \n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "stupid-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "conv1d_sentiment = nn.Conv1d(in_channels=200, out_channels=32, kernel_size=3, bias=False, padding=False)\n",
    "maxpool_sentiment = nn.MaxPool1d(kernel_size=2)\n",
    "lstm_sentiment = nn.LSTM(input_size=15,\n",
    "                         hidden_size=hidden_dim,\n",
    "                         num_layers=1,\n",
    "                         batch_first=True,\n",
    "                         dropout=0)\n",
    "lstm_h0 = (torch.zeros(1,2,hidden_dim), torch.zeros(1,2,hidden_dim))\n",
    "dense_sentiment = nn.Linear(hidden_dim, 1)\n",
    "drop_sentiment = nn.Dropout(p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "collectible-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 200, 3])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_sentiment.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "interested-saturday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape \ttorch.Size([2, 200])\n",
      "Embedding shape torch.Size([2, 200, 32])\n",
      "Conv1d shape \ttorch.Size([2, 32, 30])\n",
      "MaxPool shape \ttorch.Size([2, 32, 15])\n",
      "LSTM shape \ttorch.Size([2, 32, 64])\n",
      "Dense shape \ttorch.Size([2, 32, 1])\n",
      "Sigmoid shape \ttorch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    embed_out = embed(txt)\n",
    "    embed_out = drop_sentiment(embed_out)\n",
    "    conv_out = conv1d_sentiment(embed_out)\n",
    "    conv_out_relu = F.relu(conv_out)\n",
    "    maxpool_out = maxpool_sentiment(conv_out_relu)\n",
    "    lstm_out, _ = lstm_sentiment(maxpool_out, lstm_h0)\n",
    "    lstm_out = drop_sentiment(lstm_out)\n",
    "    out_dense = dense_sentiment(lstm_out)\n",
    "    out = nn.Sigmoid()(out_dense[:,-1,:]).view(out_dense.shape[0])\n",
    "# Input:  (N, Cin,  Lin)\n",
    "# Output: (N, Cout, Lout)\n",
    "print(f'Input shape \\t{txt.shape}\\nEmbedding shape {embed_out.shape}\\nConv1d shape \\t{conv_out.shape}\\\n",
    "\\nMaxPool shape \\t{maxpool_out.shape}\\nLSTM shape \\t{lstm_out.shape}\\nDense shape \\t{out_dense.shape}\\nSigmoid shape \\t{out.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-inquiry",
   "metadata": {},
   "source": [
    "---\n",
    "### END:Manual conv network assembly\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-moral",
   "metadata": {},
   "source": [
    "# Conv1d\n",
    "in the simplest case, the output value of the layer with input size (N,Cin,L) and output (N,Cout,Lout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "accredited-pioneer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  2.],\n",
       "          [ 3.,  4.,  5.]],\n",
       " \n",
       "         [[ 6.,  7.,  8.],\n",
       "          [ 9., 10., 11.]],\n",
       " \n",
       "         [[12., 13., 14.],\n",
       "          [15., 16., 17.]]]),\n",
       " torch.Size([3, 2, 3]))"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_1d_input_temp = torch.arange(18).to(torch.float).view(3,2,-1)\n",
    "conv_1d_input_temp, conv_1d_input_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "interior-gilbert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0],\n",
       "          [1]],\n",
       " \n",
       "         [[2],\n",
       "          [3]],\n",
       " \n",
       "         [[4],\n",
       "          [5]]]),\n",
       " torch.Size([3, 2, 1]))"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra = torch.arange(6).view((3,2,1))\n",
    "extra, extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "global-matrix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  2.,  0.],\n",
       "          [ 3.,  4.,  5.,  1.]],\n",
       " \n",
       "         [[ 6.,  7.,  8.,  2.],\n",
       "          [ 9., 10., 11.,  3.]],\n",
       " \n",
       "         [[12., 13., 14.,  4.],\n",
       "          [15., 16., 17.,  5.]]]),\n",
       " torch.Size([3, 2, 4]))"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_1d_input = torch.cat((conv_1d_input_temp, extra), dim = 2)\n",
    "conv_1d_input, conv_1d_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "widespread-circulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1.],\n",
       "          [1., 1.]],\n",
       " \n",
       "         [[2., 2.],\n",
       "          [2., 2.]],\n",
       " \n",
       "         [[1., 1.],\n",
       "          [1., 1.]],\n",
       " \n",
       "         [[1., 1.],\n",
       "          [1., 1.]]]),\n",
       " torch.Size([4, 2, 2]))"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weight(m):\n",
    "    from functools import reduce\n",
    "    l = reduce(lambda x,y: x*y, m.weight.data.shape)\n",
    "    if type(m) == nn.Conv1d:\n",
    "        m.weight.data = torch.ones(l).to(torch.float).reshape(m.weight.data.shape)\n",
    "        m.weight.data[1] += m.weight.data[1]\n",
    "\n",
    "layer_conv1d = nn.Conv1d(in_channels=2, out_channels=4, kernel_size=2, bias=False)\n",
    "layer_conv1d.apply(init_weight)\n",
    "\n",
    "layer_conv1d.weight.data, layer_conv1d.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "sorted-opportunity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  8.,  12.,   8.],\n",
       "         [ 16.,  24.,  16.],\n",
       "         [  8.,  12.,   8.],\n",
       "         [  8.,  12.,   8.]],\n",
       "\n",
       "        [[ 32.,  36.,  24.],\n",
       "         [ 64.,  72.,  48.],\n",
       "         [ 32.,  36.,  24.],\n",
       "         [ 32.,  36.,  24.]],\n",
       "\n",
       "        [[ 56.,  60.,  40.],\n",
       "         [112., 120.,  80.],\n",
       "         [ 56.,  60.,  40.],\n",
       "         [ 56.,  60.,  40.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_1d_out = layer_conv1d(conv_1d_input)\n",
    "\n",
    "conv_1d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-fighter",
   "metadata": {},
   "source": [
    "##  Convolution of kernel size 1\n",
    "it is equal to linear layer for point-wise transformation in transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "nearby-research",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.],\n",
       "          [1.],\n",
       "          [1.]],\n",
       " \n",
       "         [[2.],\n",
       "          [2.],\n",
       "          [2.]],\n",
       " \n",
       "         [[1.],\n",
       "          [1.],\n",
       "          [1.]],\n",
       " \n",
       "         [[1.],\n",
       "          [1.],\n",
       "          [1.]]]),\n",
       " torch.Size([4, 3, 1]))"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv1d = nn.Conv1d(in_channels=3, out_channels=4, kernel_size=1, bias=False)\n",
    "layer_conv1d.apply(init_weight)\n",
    "\n",
    "layer_conv1d.weight.data, layer_conv1d.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "posted-probe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.],\n",
       "          [1.],\n",
       "          [2.]]]),\n",
       " torch.Size([1, 3, 1]))"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel1input = torch.arange(3).to(torch.float).view(1,3,-1)\n",
    "kernel1input, kernel1input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "artistic-property",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.],\n",
       "         [6.],\n",
       "         [3.],\n",
       "         [3.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv1d(kernel1input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
